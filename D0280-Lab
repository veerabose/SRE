Summary
In this chapter, you learned:
• Red Hat OpenShift Container Platform is based on Red Hat Enterprise Linux CoreOS, the CRI-
O container engine, and Kubernetes.
• RHOCP 4 provides a number of services on top of Kubernetes, such as an internal container
image registry, storage, networking providers, and centralized logging and monitoring.
• Operators package applications that manage Kubernetes resources, and the Operator Lifecycle
Manager (OLM) handles installation and management of operators.
• OperatorHub.io is an online catalog for discovering operators.

oc get nodes
oc adm top nodes
oc describe node my-node-name
oc login -u kubeadmin -p MMTUc-TnXjo-NFyh3-aeWmC
oc get clusterversion
oc describe clusterversion
oc get clusteroperators

Displaying the Logs of OpenShift Nodes
oc adm node-logs -u crio my-node-name
oc adm node-logs -u kubelet my-node-name
oc adm node-logs my-node-name

[user@demo ~]$ oc debug node/my-node-name
...output omitted..................
sh-4.2# chroot /host
sh-4.2# systemctl is-active kubelet
active

Troubleshooting The Container Engine
user@demo ~]$ oc debug node/my-node-name
...output omitted...
sh-4.2# chroot /host
sh-4.2# crictl ps
...output omitted...

oc get events
oc describe pod

[user@demo ~]$ oc logs my-pod-name

If the pod contains multiple containers, then the oc logs command requires the -c option.
[user@demo ~]$ oc logs my-pod-name -c my-container-name


oc debug deployment/my-deployment-name --as-root

oc rsh my-pod-name
Opens a shell inside a pod to run shell commands interactively and non-interactively.

oc cp /local/path my-pod-name:/container/path
Copies local files to a location inside a pod. You can also reverse arguments and copy files
from inside a pod to your local file system. See also the oc rsync command for copying
multiple files at once.

oc port-forward my-pod-name local-port:remote-port
Creates a TCP tunnel from local-port on your workstation to local-port on the pod.
The tunnel is alive as long as you keep the oc port-forward running. This allows you to get

The --loglevel level option displays OpenShift API requests, starting with level 6. As you
increase the level, up to 10, more information about those requests is added, such as their HTTP
request headers and response bodies. Level 10 also includes a curl command to replicate each
request.
You can try these two commands, from any project, and compare their outputs.
[user@demo ~]$ oc get pod --loglevel 6
[user@demo ~]$ oc get pod --loglevel 10


[user@demo ~]$ oc whoami -t
network access to the pod without exposing it through a route. Because the tunnel starts at
your localhost, it cannot be accessed by other machines.

---------------------------------------------------------------------------------------------------
Chapter 2 | Verifying the Health of a Cluster
Guided Exercise
Troubleshooting OpenShift Clusters and Applications  Page 36
----------------------------------------------------------------------------------------------------
[student@workstation ~]$ lab execute-troubleshoot start

Checking prerequisites for Guided Exercise: Troubleshooting OpenShift Clusters and Applications

 Preparing the student's workstation:
 · Download exercise files.....................................  SUCCESS
 Verify the OpenShift cluster is running:
 · Waiting up to 5 minutes for router pods to be available.....  
SUCCESS
 · Waiting up to 5 minutes for OAuth to be available...........  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'execute-troubleshoot' project is absent................  SUCCESS

Setting up the classroom for Guided Exercise: Troubleshooting OpenShift Clusters and Applications

 Restoring authentication settings to installation defaults:
 · No need to perform any change...............................  SUCCESS
 Preparing the exercise:
 · Create project 'execute-troubleshoot'.......................  SUCCESS
 · Deploy PostgreSQL in 'execute-troubleshoot'.................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): y

Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
Welcome! See 'oc help' to get started.
[student@workstation ~]$ oc get nodes
NAME       STATUS                     ROLES           AGE    VERSION
master01   Ready,SchedulingDisabled   master,worker   366d   v1.18.3+012b3ec
master02   Ready                      master,worker   366d   v1.18.3+012b3ec
master03   Ready                      master,worker   366d   v1.18.3+012b3ec
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%     
master02   1584m        45%    5105Mi          34%         
master03   2671m        76%    6110Mi          40%         
master01   <unknown>                           <unknown>               <unknown>               <unknown>               
[student@workstation ~]$ oc adm top node
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master01   3742m        106%   2812Mi          18%       
master02   3102m        88%    3110Mi          20%       
master03   3176m        90%    6158Mi          41%       
[student@workstation ~]$ oc describe node master01
Name:               master01
Roles:              master,worker
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=master01
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
                    node-role.kubernetes.io/worker=
                    node.openshift.io/os_id=rhcos
Annotations:        machineconfiguration.openshift.io/currentConfig: rendered-master-ebfe876315cf6b098e7eb8750dfc72ed
                    machineconfiguration.openshift.io/desiredConfig: rendered-master-ebfe876315cf6b098e7eb8750dfc72ed
                    machineconfiguration.openshift.io/reason: 
                    machineconfiguration.openshift.io/state: Done
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 05 Aug 2020 14:23:06 -0400
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  master01
  AcquireTime:     <unset>
  RenewTime:       Sat, 07 Aug 2021 03:44:24 -0400
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 07 Aug 2021 03:43:34 -0400   Sat, 07 Aug 2021 03:43:34 -0400   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 07 Aug 2021 03:43:34 -0400   Sat, 07 Aug 2021 03:43:34 -0400   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 07 Aug 2021 03:43:34 -0400   Sat, 07 Aug 2021 03:43:34 -0400   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 07 Aug 2021 03:43:34 -0400   Sat, 07 Aug 2021 03:43:34 -0400   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.50.10
  Hostname:    master01
Capacity:
  cpu:                4
  ephemeral-storage:  41391084Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16419132Ki
  pods:               250
Allocatable:
  cpu:                3500m
  ephemeral-storage:  37072281128
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15268156Ki
  pods:               250
System Info:
  Machine ID:                                 8647fbfd86c5432aa6c9da08d596d4cc
  System UUID:                                54b4ab2d-135d-4ad9-b903-6fa97a2c5aaf
  Boot ID:                                    3affb095-7851-4b07-b451-24d2620bf124
  Kernel Version:                             4.18.0-193.13.2.el8_2.x86_64
  OS Image:                                   Red Hat Enterprise Linux CoreOS 45.82.202007240629-0 (Ootpa)
  Operating System:                           linux
  Architecture:                               amd64
  Container Runtime Version:                  cri-o://1.18.3-5.rhaos4.5.git1c13d1d.el8
  Kubelet Version:                            v1.18.3+012b3ec
  Kube-Proxy Version:                         v1.18.3+012b3ec
PodCIDR:                                      10.8.1.0/24
PodCIDRs:                                     10.8.1.0/24
Non-terminated Pods:                          (43 in total)
  Namespace                                   Name                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                                   ----                                                 ------------  ----------  ---------------  -------------  ---
  execute-troubleshoot                        psql-69676884f8-fpl98                                0 (0%)        0 (0%)      512Mi (3%)       512Mi (3%)     34s
  openshift-apiserver                         apiserver-59b6b7968c-zzv6g                           100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         7m21s
  openshift-authentication                    oauth-openshift-68799984ff-hmnqr                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         31s
  openshift-cloud-credential-operator         cloud-credential-operator-5bf8fb79db-jgd6l           10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         32s
  openshift-cluster-node-tuning-operator      tuned-qjbhv                                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         366d
  openshift-cluster-storage-operator          csi-snapshot-controller-operator-6f9d5b4bfc-w6987    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         33s
  openshift-console                           console-5df4fcbb47-r99g8                             10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         30s
  openshift-console                           downloads-5565cdb86b-9fccj                           10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         32s
  openshift-controller-manager                controller-manager-fkl2v                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         8m55s
  openshift-dns                               dns-default-tfhlc                                    65m (1%)      0 (0%)      110Mi (0%)       512Mi (3%)     366d
  openshift-etcd-operator                     etcd-operator-fcd9d5cb7-llglj                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         32s
  openshift-etcd                              etcd-master01                                        460m (13%)    0 (0%)      920Mi (6%)       0 (0%)         366d
  openshift-image-registry                    node-ca-qrrpd                                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         366d
  openshift-ingress                           router-default-57575dc57c-kcnvz                      100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         34s
  openshift-kube-apiserver                    kube-apiserver-master01                              330m (9%)     0 (0%)      1174Mi (7%)      0 (0%)         4m54s
  openshift-kube-controller-manager-operator  kube-controller-manager-operator-5658d865f7-bw9zx    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         33s
  openshift-kube-controller-manager           kube-controller-manager-master01                     100m (2%)     0 (0%)      500Mi (3%)       0 (0%)         366d
  openshift-kube-scheduler                    openshift-kube-scheduler-master01                    20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         366d
  openshift-kube-storage-version-migrator     migrator-84d87c7d77-jhmmw                            100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         32s
  openshift-machine-api                       cluster-autoscaler-operator-d5698d448-skhk2          30m (0%)      0 (0%)      70Mi (0%)        0 (0%)         32s
  openshift-machine-config-operator           etcd-quorum-guard-5fc4989788-5d62h                   10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         7m20s
  openshift-machine-config-operator           machine-config-daemon-l4ftv                          40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         366d
  openshift-machine-config-operator           machine-config-operator-5b87d8f4c5-vd7kp             20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         34s
  openshift-machine-config-operator           machine-config-server-cn4cx                          20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         366d
  openshift-marketplace                       certified-operators-85ccbb74bb-fb9p5                 10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         34s
  openshift-marketplace                       community-operators-7998c846c7-kk45k                 10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         34s
  openshift-marketplace                       redhat-marketplace-5bc4744b59-l8qpc                  10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         34s
  openshift-monitoring                        alertmanager-main-1                                  8m (0%)       0 (0%)      260Mi (1%)       0 (0%)         17s
  openshift-monitoring                        alertmanager-main-2                                  8m (0%)       0 (0%)      260Mi (1%)       0 (0%)         17s
  openshift-monitoring                        grafana-68bbfbdf97-mffvw                             5m (0%)       0 (0%)      120Mi (0%)       0 (0%)         31s
  openshift-monitoring                        node-exporter-7z66r                                  9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         366d
  openshift-monitoring                        prometheus-adapter-c868b4f4d-pfm9t                   1m (0%)       0 (0%)      25Mi (0%)        0 (0%)         31s
  openshift-monitoring                        prometheus-k8s-1                                     76m (2%)      0 (0%)      1184Mi (7%)      0 (0%)         17s
  openshift-monitoring                        thanos-querier-6c89f5cff9-v8fp4                      8m (0%)       0 (0%)      72Mi (0%)        0 (0%)         31s
  openshift-multus                            multus-admission-controller-plrpp                    20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         366d
  openshift-multus                            multus-xpkf5                                         10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         366d
  openshift-network-operator                  network-operator-6bd8b9b698-xbg4q                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         30s
  openshift-operator-lifecycle-manager        packageserver-d56b544dd-zmrsk                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         14s
  openshift-sdn                               ovs-7bzt6                                            100m (2%)     0 (0%)      400Mi (2%)       0 (0%)         366d
  openshift-sdn                               sdn-controller-czjdg                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         366d
  openshift-sdn                               sdn-xgl8z                                            100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         366d
  openshift-service-ca-operator               service-ca-operator-5f596775f8-szx8b                 10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         30s
  openshift-service-ca                        service-ca-8b8db557b-zlwl4                           10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         35s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                2 (57%)       0 (0%)
  memory             8408Mi (56%)  1Gi (6%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
Events:
  Type     Reason                   Age                  From               Message
  ----     ------                   ----                 ----               -------
  Normal   NodeNotSchedulable       366d                 kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   Starting                 366d                 kubelet, master01  Starting kubelet.
  Normal   NodeHasSufficientMemory  366d (x2 over 366d)  kubelet, master01  Node master01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    366d (x2 over 366d)  kubelet, master01  Node master01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     366d (x2 over 366d)  kubelet, master01  Node master01 status is now: NodeHasSufficientPID
  Warning  Rebooted                 366d                 kubelet, master01  Node master01 has been rebooted, boot id: 087a4978-52e9-4981-a3e4-9a640017b061
  Normal   NodeNotReady             366d                 kubelet, master01  Node master01 status is now: NodeNotReady
  Normal   NodeNotSchedulable       366d                 kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   NodeAllocatableEnforced  366d                 kubelet, master01  Updated Node Allocatable limit across pods
  Normal   NodeReady                366d                 kubelet, master01  Node master01 status is now: NodeReady
  Normal   NodeSchedulable          366d                 kubelet, master01  Node master01 status is now: NodeSchedulable
  Normal   NodeNotSchedulable       7m20s                kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   NodeReady                2m2s                 kubelet, master01  Node master01 status is now: NodeReady
  Normal   NodeHasSufficientMemory  2m2s (x2 over 2m2s)  kubelet, master01  Node master01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    2m2s (x2 over 2m2s)  kubelet, master01  Node master01 status is now: NodeHasNoDiskPressure
  Normal   Starting                 2m2s                 kubelet, master01  Starting kubelet.
  Warning  Rebooted                 2m2s                 kubelet, master01  Node master01 has been rebooted, boot id: 4167e399-5543-4339-9d6e-cfec9a429282
  Normal   NodeNotReady             2m2s                 kubelet, master01  Node master01 status is now: NodeNotReady
  Normal   NodeNotSchedulable       2m2s                 kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   NodeAllocatableEnforced  2m2s                 kubelet, master01  Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientPID     2m2s (x2 over 2m2s)  kubelet, master01  Node master01 status is now: NodeHasSufficientPID
  Normal   Starting                 53s                  kubelet, master01  Starting kubelet.
  Normal   NodeHasSufficientMemory  53s (x2 over 53s)    kubelet, master01  Node master01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    53s (x2 over 53s)    kubelet, master01  Node master01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     53s (x2 over 53s)    kubelet, master01  Node master01 status is now: NodeHasSufficientPID
  Warning  Rebooted                 53s                  kubelet, master01  Node master01 has been rebooted, boot id: 3affb095-7851-4b07-b451-24d2620bf124
  Normal   NodeNotReady             53s                  kubelet, master01  Node master01 status is now: NodeNotReady
  Normal   NodeNotSchedulable       53s                  kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   NodeAllocatableEnforced  53s                  kubelet, master01  Updated Node Allocatable limit across pods
  Normal   NodeReady                53s                  kubelet, master01  Node master01 status is now: NodeReady
  Normal   NodeSchedulable          32s                  kubelet, master01  Node master01 status is now: NodeSchedulable

[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get pod -n openshift-image-registry
NAME                                               READY   STATUS    RESTARTS   AGE
cluster-image-registry-operator-5f47f6fcf7-vd2x5   2/2     Running   0          5m47s
image-registry-8c54885cd-6hd5v                     1/1     Running   0          5m47s
node-ca-24k9d                                      1/1     Running   0          366d
node-ca-lfh4z                                      1/1     Running   0          366d
node-ca-qrrpd                                      1/1     Running   0          366d
[student@workstation ~]$ oc logs --tail 3 -n openshift-image-registry -c cluster-image-registry-operator cluster-image-registry-operator-5f47f6fcf7-vd2x5
I0807 07:54:01.359030      12 clusteroperator.go:99] event from workqueue successfully processed
I0807 07:54:02.397528      12 clusteroperator.go:99] event from workqueue successfully processed
I0807 07:54:03.564534      12 clusteroperator.go:99] event from workqueue successfully processed
[student@workstation ~]$ oc logs --tail 1 -n openshift-image-registry image-registry-8c54885cd-6hd5v
time="2021-08-07T07:55:31.344300341Z" level=info msg=response go.version=go1.13.4 http.request.host="10.8.0.20:5000" http.request.id=61793a83-bd26-44db-a60d-a761c2d72665 http.request.method=GET http.request.remoteaddr="10.8.0.1:41478" http.request.uri=/healthz http.request.useragent=kube-probe/1.18+ http.response.duration="49.423µs" http.response.status=200 http.response.written=0
[student@workstation ~]$ oc adm node-logs --tail 1 -u kubelet master01
-- Logs begin at Wed 2020-08-05 18:20:28 UTC, end at Sat 2021-08-07 07:56:12 UTC. --
Aug 07 07:42:44.421312 master01 systemd[1]: kubelet.service: Consumed 3.401s CPU time
-- Logs begin at Wed 2020-08-05 18:20:28 UTC, end at Sat 2021-08-07 07:56:12 UTC. --
Aug 07 07:56:12.231869 master01 hyperkube[1501]: I0807 07:56:12.231863    1501 prober.go:133] Readiness probe for "packageserver-d56b544dd-zmrsk_openshift-operator-lifecycle-manager(c5600d4d-a93d-47c1-8fc9-ea9d26d47b66):packageserver" succeeded

[student@workstation ~]$ oc debug node/master01
Starting pod/master01-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.50.10
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host 
sh-4.4# systemctl status kubelet
● kubelet.service - MCO environment configuration
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-mco-default-env.conf
   Active: active (running) since Sat 2021-08-07 07:43:34 UTC; 14min ago
  Process: 1499 ExecStartPre=/bin/rm -f /var/lib/kubelet/cpu_manager_state (code=exited, status=0/SUCCESS)
  Process: 1497 ExecStartPre=/bin/mkdir --parents /etc/kubernetes/manifests (code=exited, status=0/SUCCESS)
 Main PID: 1501 (kubelet)
    Tasks: 32 (limit: 102120)
   Memory: 398.1M
      CPU: 1min 58.425s
   CGroup: /system.slice/kubelet.service
           └─1501 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfi>

Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.736141    1501 prober.go:184] HTTP-Probe Headers: map[]
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.743564    1501 http.go:128] Probe succeeded for https:/>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.743637    1501 prober.go:133] Liveness probe for "kube->
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.803319    1501 prober.go:181] HTTP-Probe Host: http://l>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.803356    1501 prober.go:184] HTTP-Probe Headers: map[]
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.805316    1501 http.go:128] Probe succeeded for http://>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.805371    1501 prober.go:133] Liveness probe for "route>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.846469    1501 kubelet.go:1989] SyncLoop (SYNC): 1 pods>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.846569    1501 kubelet.go:2034] Pod "recyler-pod-master>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.846603    1501 kubelet.go:2012] SyncLoop (housekeeping)
sh-4.4# systemctl status cri-o
● crio.service - MCO environment configuration
   Loaded: loaded (/usr/lib/systemd/system/crio.service; disabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/crio.service.d
           └─10-mco-default-env.conf
   Active: active (running) since Sat 2021-08-07 07:43:30 UTC; 14min ago
     Docs: https://github.com/cri-o/cri-o
 Main PID: 1452 (crio)
    Tasks: 36
   Memory: 458.5M
      CPU: 2min 16.593s
   CGroup: /system.slice/crio.service
           ├─ 1452 /usr/bin/crio --enable-metrics=true --metrics-port=9537
           ├─73296 /usr/libexec/crio/conmon -c 50ee88b6365fa08d27fb087ffa31281611b2dbe1c196db6ad9c44dbbf663df13 >
           └─73297 /usr/bin/runc --root=/run/runc exec --pid-file /tmp/pidfile818200403 --process /tmp/exec-proc>

Aug 07 07:57:11 master01 crio[1452]: time="2021-08-07 07:57:11.380192551Z" level=info msg="Starting container: 0>
Aug 07 07:57:11 master01 crio[1452]: time="2021-08-07 07:57:11.403389087Z" level=info msg="Started container 096>
Aug 07 07:57:14 master01 crio[1452]: time="2021-08-07 07:57:14.862350110Z" level=info msg="Checking image status>
Aug 07 07:57:14 master01 crio[1452]: time="2021-08-07 07:57:14.862637971Z" level=info msg="Image registry.access>
Aug 07 07:57:27 master01 crio[1452]: time="2021-08-07 07:57:27.848476912Z" level=info msg="Checking image status>
Aug 07 07:57:27 master01 crio[1452]: time="2021-08-07 07:57:27.848702947Z" level=info msg="Image registry.access>
Aug 07 07:57:39 master01 crio[1452]: time="2021-08-07 07:57:39.847893007Z" level=info msg="Checking image status>
Aug 07 07:57:39 master01 crio[1452]: time="2021-08-07 07:57:39.848203581Z" level=info msg="Image registry.access>
Aug 07 07:57:54 master01 crio[1452]: time="2021-08-07 07:57:54.850299018Z" level=info msg="Checking image status>
Aug 07 07:57:54 master01 crio[1452]: time="2021-08-07 07:57:54.850699985Z" level=info msg="Image registry.access>
sh-4.4# crictl ps --name openvswitch
CONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID
e5db87f51af56       81ac7e4df0c960f19146662f538bcc2b72709040390ef6f94bfa880f5c0ebc3a   14 minutes ago      Running             openvswitch         0                   f78c3cd101d9b
sh-4.4# exit
exit
sh-4.2# exit
exit

Removing debug pod ...
[student@workstation ~]$ oc project execute-troubleshoot
Now using project "execute-troubleshoot" on server "https://api.ocp4.example.com:6443".
[student@workstation ~]$ oc get pod
NAME                    READY   STATUS             RESTARTS   AGE
psql-69676884f8-fpl98   0/1     ImagePullBackOff   0          14m
[student@workstation ~]$ oc status
In project execute-troubleshoot on server https://api.ocp4.example.com:6443

svc/psql - 172.30.150.124:5432

deployment/psql deploys registry.access.redhat.com/rhscl/postgresq-96-rhel7:1
  deployment #1 running for 19 minutes - 0/1 pods

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.
[student@workstation ~]$ oc get events
LAST SEEN   TYPE      REASON              OBJECT                       MESSAGE
19m         Normal    Scheduled           pod/psql-69676884f8-czk8l    Successfully assigned execute-troubleshoot/psql-69676884f8-czk8l to master02
19m         Normal    AddedInterface      pod/psql-69676884f8-czk8l    Add eth0 [10.8.0.30/23]
18m         Normal    Pulling             pod/psql-69676884f8-czk8l    Pulling image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1"
18m         Warning   Failed              pod/psql-69676884f8-czk8l    Failed to pull image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1": rpc error: code = Unknown desc = Error reading manifest 1 in registry.access.redhat.com/rhscl/postgresq-96-rhel7: name unknown: Repo not found
18m         Warning   Failed              pod/psql-69676884f8-czk8l    Error: ErrImagePull
17m         Normal    BackOff             pod/psql-69676884f8-czk8l    Back-off pulling image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1"
17m         Warning   Failed              pod/psql-69676884f8-czk8l    Error: ImagePullBackOff
15m         Normal    Scheduled           pod/psql-69676884f8-fpl98    Successfully assigned execute-troubleshoot/psql-69676884f8-fpl98 to master01
15m         Normal    AddedInterface      pod/psql-69676884f8-fpl98    Add eth0 [10.10.0.12/23]
14m         Normal    Pulling             pod/psql-69676884f8-fpl98    Pulling image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1"
14m         Warning   Failed              pod/psql-69676884f8-fpl98    Failed to pull image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1": rpc error: code = Unknown desc = Error reading manifest 1 in registry.access.redhat.com/rhscl/postgresq-96-rhel7: name unknown: Repo not found
14m         Warning   Failed              pod/psql-69676884f8-fpl98    Error: ErrImagePull
5m20s       Normal    BackOff             pod/psql-69676884f8-fpl98    Back-off pulling image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1"
10s         Warning   Failed              pod/psql-69676884f8-fpl98    Error: ImagePullBackOff
19m         Normal    SuccessfulCreate    replicaset/psql-69676884f8   Created pod: psql-69676884f8-czk8l
15m         Normal    SuccessfulCreate    replicaset/psql-69676884f8   Created pod: psql-69676884f8-fpl98
19m         Normal    ScalingReplicaSet   deployment/psql              Scaled up replica set psql-69676884f8 to 1
[student@workstation ~]$ skopeo inspect docker://registry.access.redhat.com/rhscl/postgresq-96-rhel7:1
FATA[0000] Error parsing image name "docker://registry.access.redhat.com/rhscl/postgresq-96-rhel7:1": Error reading manifest 1 in registry.access.redhat.com/rhscl/postgresq-96-rhel7: name unknown: Repo not found 
[student@workstation ~]$ skopeo inspect docker://registry.access.redhat.com/rhscl/postgresql-96-rhel7:1
{
    "Name": "registry.access.redhat.com/rhscl/postgresql-96-rhel7",
    "Digest": "sha256:6c3090779c0553d773380ff4cb0532086075376964de398417d494313f9a933b",
    "RepoTags": [
        "1-42.1561731092",
        "1-59",
        "1-58",
        "1-17",
        "1-10",
        "1-13",
        "1-12",
        "1-51",
        "1-53",
        "1-52",
        "1-57",
        "1-38",
        "1-37",
        "1-36",
        "1-32",
        "1",
        "1-25.1535384678",
        "1-8",
        "1-5",
        "1-4",
        "1-7",
        "1-6",
        "1-61",
        "1-63",
        "1-48",
        "1-49",
        "1-46",
        "1-47",
        "1-44",
        "1-45",
        "1-42",
        "1-40",
        "1-51.1575996559",
        "1-28",
        "1-14",
        "1-25",
        "1-27",
        "latest"
    ],
    "Created": "2020-09-21T15:39:59.766636Z",
    "DockerVersion": "1.13.1",
    "Labels": {
        "architecture": "x86_64",
        "build-date": "2020-09-21T15:38:28.666467",
        "com.redhat.build-host": "cpt-1002.osbs.prod.upshift.rdu2.redhat.com",
        "com.redhat.component": "rh-postgresql96-container",
        "com.redhat.license_terms": "https://www.redhat.com/en/about/red-hat-end-user-license-agreements#rhel",
        "description": "PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.",
        "distribution-scope": "public",
        "io.k8s.description": "PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.",
        "io.k8s.display-name": "PostgreSQL 9.6",
        "io.openshift.expose-services": "5432:postgresql",
        "io.openshift.s2i.assemble-user": "26",
        "io.openshift.s2i.scripts-url": "image:///usr/libexec/s2i",
        "io.openshift.tags": "database,postgresql,postgresql96,rh-postgresql96",
        "io.s2i.scripts-url": "image:///usr/libexec/s2i",
        "maintainer": "SoftwareCollections.org \u003csclorg@redhat.com\u003e",
        "name": "rhscl/postgresql-96-rhel7",
        "release": "63",
        "summary": "PostgreSQL is an advanced Object-Relational database management system",
        "url": "https://access.redhat.com/containers/#/registry.access.redhat.com/rhscl/postgresql-96-rhel7/images/1-63",
        "usage": "podman run -d --name postgresql_database -e POSTGRESQL_USER=user -e POSTGRESQL_PASSWORD=pass -e POSTGRESQL_DATABASE=db -p 5432:5432 rhscl/postgresql-96-rhel7",
        "vcs-ref": "ae9489bc8bf8f1c543d15dbe9ed5b9d1b6adcacd",
        "vcs-type": "git",
        "vendor": "Red Hat, Inc.",
        "version": "1"
    },
    "Architecture": "amd64",
    "Os": "linux",
    "Layers": [
        "sha256:1323a241cc068f2816dd88f00168be73339471d6dc6eb2e6c761b63b734501b6",
        "sha256:2bd25ca124579d6fce8668ff5d4ed83866d7e7438cb561a51ddde8cc40272822",
        "sha256:5d011ac93e7456d4c646b0fbb53712598bda9a6b0d027b2be788016e078ded77",
        "sha256:16634824fe3f2f2bbbf87f9a3082ec4b3d1b3b084ce14b3e63419a4dc5b3150d"
    ],
    "Env": [
        "PATH=/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
        "container=oci",
        "SUMMARY=PostgreSQL is an advanced Object-Relational database management system",
        "DESCRIPTION=PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.",
        "STI_SCRIPTS_URL=image:///usr/libexec/s2i",
        "STI_SCRIPTS_PATH=/usr/libexec/s2i",
        "APP_ROOT=/opt/app-root",
        "HOME=/var/lib/pgsql",
        "PLATFORM=el7",
        "BASH_ENV=/usr/share/container-scripts/postgresql/scl_enable",
        "ENV=/usr/share/container-scripts/postgresql/scl_enable",
        "PROMPT_COMMAND=. /usr/share/container-scripts/postgresql/scl_enable",
        "POSTGRESQL_VERSION=9.6",
        "POSTGRESQL_PREV_VERSION=9.5",
        "PGUSER=postgres",
        "APP_DATA=/opt/app-root",
        "CONTAINER_SCRIPTS_PATH=/usr/share/container-scripts/postgresql",
        "ENABLED_COLLECTIONS=rh-postgresql96"
    ]
}
[student@workstation ~]$ oc edit deployment/psql
deployment.apps/psql edited
[student@workstation ~]$ oc status
In project execute-troubleshoot on server https://api.ocp4.example.com:6443

svc/psql - 172.30.150.124:5432

deployment/psql deploys registry.access.redhat.com/rhscl/postgresql-96-rhel7:1
  deployment #2 running for 8 seconds - 0/1 pods
  deployment #1 deployed 21 minutes ago - 0/1 pods

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.
[student@workstation ~]$ oc get pods
NAME                    READY   STATUS             RESTARTS   AGE
psql-69676884f8-fpl98   0/1     ImagePullBackOff   0          17m
psql-7b586df5d-w2cm4    0/1     Running            0          17s
[student@workstation ~]$ oc get pods
NAME                   READY   STATUS    RESTARTS   AGE
psql-7b586df5d-w2cm4   1/1     Running   0          33s
[student@workstation ~]$ lab execute-troubleshoot finish

Completing Guided Exercise: Troubleshooting OpenShift Clusters and Applications

 · Delete OpenShift project 'execute-troubleshoot'.............  SUCCESS
 · Wait for project 'execute-troubleshoot' to be gone..........  SUCCESS
 · Remove exercise files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

-------------------------------------------------------------------------------------------------------------
Guided Exercise:
Introducing OpenShift Dynamic Storage Page: 47
--------------------------------------------------------------------------------------------------------------
In this exercise, you will deploy a PostgreSQL database using a persistent volume claim and
identify its dynamically allocated volume.
Outcomes
You should be able to:
• Identify the default storage settings of an OpenShift cluster.
• Create persistent volume claims.
• Manage persistent volumes.

[student@workstation ~]$ lab install-storage start

Checking prerequisites for Guided Exercise: Introducing OpenShift Dynamic Storage

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'install-storage' project is absent.....................  SUCCESS

Setting up the classroom for Guided Exercise: Introducing OpenShift Dynamic Storage

 · Download exercise files.....................................  SUCCESS
 · Persistent volume claim 'postgresql-storage' is not present.  SUCCESS
 · Application 'postgres-persistent' is not present............  SUCCESS
 · Application 'postgres-persistent2' is not present...........  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc new-project install-storage
Now using project "install-storage" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc get storageclass
NAME                    PROVISIONER               RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage (default)   nfs-storage-provisioner   Delete          Immediate           false                  366d
[student@workstation ~]$ oc new-app --name postgresql-persistent --docker-image registry.redhat.io/rhel8/postgresql-12:1-43 -e POSTGRESQL_USER=redhat -e POSTGRESQL_PASSWORD=redhat123 -e POSTGRESQL_DATABASE=persistentdb
--> Found container image 667898a (11 months old) from registry.redhat.io for "registry.redhat.io/rhel8/postgresql-12:1-43"

    PostgreSQL 12 
    ------------- 
    PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.

    Tags: database, postgresql, postgresql12, postgresql-12

    * An image stream tag will be created as "postgresql-persistent:1-43" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "postgresql-persistent" created
    deployment.apps "postgresql-persistent" created
    service "postgresql-persistent" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/postgresql-persistent' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc set volumes deployment/postgresql-persistent --add --name postgresql-storage --type pvc --claim-class nfs-storage --claim-mode rwo --claim-size 10Gi --mount-path /var/lib/pgsql --claim-name postgresql-storage
deployment.apps/postgresql-persistent volume updated
[student@workstation ~]$ oc status
In project install-storage on server https://api.ocp4.example.com:6443

svc/postgresql-persistent - 172.30.91.12:5432
  deployment/postgresql-persistent deploys istag/postgresql-persistent:1-43 
    deployment #3 running for 6 seconds - 0/1 pods
    deployment #2 deployed 2 minutes ago - 1 pod
    deployment #1 deployed 2 minutes ago


1 info identified, use 'oc status --suggest' to see details.
[student@workstation ~]$ oc get pvc
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
postgresql-storage   Bound    pvc-5142b9b6-cb25-4af3-b0f1-52c6dd5270c1   10Gi       RWO            nfs-storage    38s
[student@workstation ~]$ oc get pv -o custom-columns=NAME:.metadata.name,CLAIM:.spec.claimRef.name
NAME                                       CLAIM
pvc-26cc804a-4ec2-4f52-b6e5-84404b4b9def   image-registry-storage
pvc-5142b9b6-cb25-4af3-b0f1-52c6dd5270c1   postgresql-storage
[student@workstation ~]$ cd ~/DO280/labs/install-storage
[student@workstation install-storage]$ ./init_data.sh
Populating characters table
CREATE TABLE
INSERT 0 5
[student@workstation install-storage]$ ./check_data.sh
Checking characters table
 id |          name           |           nationality            
----+-------------------------+----------------------------------
  1 | Wolfgang Amadeus Mozart | Prince-Archbishopric of Salzburg
  2 | Ludwig van Beethoven    | Bonn, Germany
  3 | Johann Sebastian Bach   | Eisenach, Germany
  4 | José Pablo Moncayo     | Guadalajara, México
  5 | Niccolò Paganini       | Genoa, Italy
(5 rows)

[student@workstation install-storage]$ oc delete all -l app=postgresql-persistent
service "postgresql-persistent" deleted
deployment.apps "postgresql-persistent" deleted
imagestream.image.openshift.io "postgresql-persistent" deleted
[student@workstation install-storage]$ oc new-app --name postgresql-persistent2 --docker-image registry.redhat.io/rhel8/postgresql-12:1-43 -e POSTGRESQL_USER=redhat -e POSTGRESQL_PASSWORD=redhat123 -e POSTGRESQL_DATABASE=persistentdb
--> Found container image 667898a (11 months old) from registry.redhat.io for "registry.redhat.io/rhel8/postgresql-12:1-43"

    PostgreSQL 12 
    ------------- 
    PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.

    Tags: database, postgresql, postgresql12, postgresql-12

    * An image stream tag will be created as "postgresql-persistent2:1-43" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "postgresql-persistent2" created
    deployment.apps "postgresql-persistent2" created
    service "postgresql-persistent2" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/postgresql-persistent2' 
    Run 'oc status' to view your app.
[student@workstation install-storage]$ ./check_data.sh
Checking characters table
ERROR: 'characters' table does not exist
[student@workstation install-storage]$ oc set volumes deployment/postgresql-persistent2 --add --name postgresql-storage --type pvc --claim-name postgresql-storage --mount-path /var/lib/pgsql
deployment.apps/postgresql-persistent2 volume updated
[student@workstation install-storage]$ ./check_data.sh
Checking characters table
 id |          name           |           nationality            
----+-------------------------+----------------------------------
  1 | Wolfgang Amadeus Mozart | Prince-Archbishopric of Salzburg
  2 | Ludwig van Beethoven    | Bonn, Germany
  3 | Johann Sebastian Bach   | Eisenach, Germany
  4 | José Pablo Moncayo     | Guadalajara, México
  5 | Niccolò Paganini       | Genoa, Italy
(5 rows)

[student@workstation install-storage]$ oc delete all -l app=postgresql-persistent2
service "postgresql-persistent2" deleted
deployment.apps "postgresql-persistent2" deleted
imagestream.image.openshift.io "postgresql-persistent2" deleted
[student@workstation install-storage]$ oc delete pvc/postgresql-storage
persistentvolumeclaim "postgresql-storage" deleted
[student@workstation install-storage]$ cd
[student@workstation ~]$ lab install-storage finish

Completing Guided Exercise: Introducing OpenShift Dynamic Storage

 · Delete OpenShift project 'install-storage'..................  SUCCESS
 · Wait for project 'install-storage' to be gone...............  SUCCESS
 · Remove exercise files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

Summary
In this chapter, you learned:
• Red Hat OpenShift Container Platform provides two main installation methods: full-stack
automation, and pre-existing infrastructure.
• Future releases are expected to add more cloud and virtualization providers, such as VMware,
Red Hat Virtualization, and IBM System Z.
• An OpenShift node based on Red Hat Enterprise Linux CoreOS runs very few local services that
would require direct access to a node to inspect their status. Most of the system services run as
containers, the main exceptions are the CRI-O container engine and the Kubelet.
• The oc get node, oc adm top, oc adm node-logs, and oc adm debug commands
provide troubleshooting information about OpenShift nodes.

------------------------------------------------------------------------------------------------------------
Chapter 3 | Configuring Authentication and Authorization
Guided Exercise
Configuring Identity Providers Page: 61
------------------------------------------------------------------------------------------------------------
In this exercise, you will configure the HTPasswd identity provider and create users for cluster
administrators.
Outcomes
You should be able to:
• Create users and passwords for HTPasswd authentication.
• Configure the Identity Provider for HTPasswd authentication.
• Assign cluster administration rights to users.


[student@workstation ~]$ lab auth-provider start

Checking prerequisites for Guided Exercise: Configuring Identity Providers

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'auth-provider' project is absent.......................  SUCCESS

Setting up the classroom for Guided Exercise: Configuring Identity Providers

 Preparing the student's workstation:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS
 Restoring authentication settings to installation defaults:
 · No need to perform any change...............................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ htpasswd -c -B -b ~/DO280/labs/auth-provider/htpasswd admin redhat
Adding password for user admin
[student@workstation ~]$ htpasswd -b ~/DO280/labs/auth-provider/htpasswd developer developer
Adding password for user developer
[student@workstation ~]$ cat ~/DO280/labs/auth-provider/htpasswd
admin:$2y$05$a2aKYuv8R96uoQ2xxxTEIeGnEI8xZasIQ5/FGYHO7MXpxzU4MT3He
developer:$apr1$nPaPLoFJ$GYsV0obfH5cznZj0erWtW0
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc create secret generic localusers --from-file htpasswd=/home/student/DO280/labs/auth-provider/htpasswd -n openshift-config
secret/localusers created
[student@workstation ~]$ oc adm policy add-cluster-role-to-user cluster-admin admin
Warning: User 'admin' not found
clusterrole.rbac.authorization.k8s.io/cluster-admin added: "admin"
[student@workstation ~]$ oc get oauth cluster -o yaml > ~/DO280/labs/auth-provider/oauth.yaml
[student@workstation ~]$ vi !$
vi ~/DO280/labs/auth-provider/oauth.yaml
[student@workstation ~]$ vi ~/DO280/labs/auth-provider/oauth.yaml
[student@workstation ~]$ cat !$
cat ~/DO280/labs/auth-provider/oauth.yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  annotations:
    release.openshift.io/create-only: "true"
  creationTimestamp: "2020-08-05T18:23:41Z"
  generation: 1
  managedFields:
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:release.openshift.io/create-only: {}
      f:spec: {}
    manager: cluster-version-operator
    operation: Update
    time: "2020-08-05T18:23:41Z"
  name: cluster
  resourceVersion: "2121"
  selfLink: /apis/config.openshift.io/v1/oauths/cluster
  uid: c96a18f6-b8de-4722-b5bf-ca6c39cdb78b
spec:
  identityProviders:
  - htpasswd:
      fileData:    
        name: localusers
    mappingMethod: claim
    name: myusers
    type: HTPasswd
[student@workstation ~]$ oc replace -f ~/DO280/labs/auth-provider/oauth.yaml
oauth.config.openshift.io/cluster replaced
[student@workstation ~]$ oc login -u admin -p redhat
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get nodes
NAME       STATUS   ROLES           AGE    VERSION
master01   Ready    master,worker   366d   v1.18.3+012b3ec
master02   Ready    master,worker   366d   v1.18.3+012b3ec
master03   Ready    master,worker   366d   v1.18.3+012b3ec
[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc get nodes
Error from server (Forbidden): nodes is forbidden: User "developer" cannot list resource "nodes" in API group "" at the cluster scope
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get users
NAME        UID                                    FULL NAME   IDENTITIES
admin       17914b9a-4a74-4e68-989d-2a988c289136               myusers:admin
developer   ebb1a8e6-3dba-41b2-b0fc-f8a65b0846d7               myusers:developer
[student@workstation ~]$ oc get identity
NAME                IDP NAME   IDP USER NAME   USER NAME   USER UID
myusers:admin       myusers    admin           admin       17914b9a-4a74-4e68-989d-2a988c289136
myusers:developer   myusers    developer       developer   ebb1a8e6-3dba-41b2-b0fc-f8a65b0846d7
[student@workstation ~]$ oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-provider/ --confirm
/home/student/DO280/labs/auth-provider/htpasswd
[student@workstation ~]$ htpasswd -b ~/DO280/labs/auth-provider/htpasswd manager redhat
Adding password for user manager
[student@workstation ~]$ cat ~/DO280/labs/auth-provider/htpasswd
admin:$2y$05$a2aKYuv8R96uoQ2xxxTEIeGnEI8xZasIQ5/FGYHO7MXpxzU4MT3He
developer:$apr1$nPaPLoFJ$GYsV0obfH5cznZj0erWtW0
manager:$apr1$VCHd.PG9$QUM3QFOAmk6b.CQGhc/NX1
[student@workstation ~]$ oc set data secret/localusers --from-file htpasswd=/home/student/DO280/labs/auth-provider/htpasswd -n openshift-config
secret/localusers data updated
[student@workstation ~]$ oc login -u manager -p redhat
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ oc new-project auth-provider
Now using project "auth-provider" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc delete project auth-provider
Error from server (Forbidden): projects.project.openshift.io "auth-provider" is forbidden: User "developer" cannot delete resource "projects" in API group "project.openshift.io" in the namespace "auth-provider"
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-provider/ --confirm
/home/student/DO280/labs/auth-provider/htpasswd
[student@workstation ~]$ MANAGER_PASSWD="$(openssl rand -hex 15)"
[student@workstation ~]$ htpasswd -b ~/DO280/labs/auth-provider/htpasswd manager ${MANAGER_PASSWD}
Updating password for user manager
[student@workstation ~]$ oc set data secret/localusers --from-file htpasswd=/home/student/DO280/labs/auth-provider/htpasswd -n openshift-config
secret/localusers data updated
[student@workstation ~]$ oc login -u manager -p ${MANAGER_PASSWD}
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ oc login -u manager -p ${MANAGER_PASSWD}
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-provider/ --confirm
/home/student/DO280/labs/auth-provider/htpasswd
[student@workstation ~]$ htpasswd -D ~/DO280/labs/auth-provider/htpasswd manager
Deleting password for user manager
[student@workstation ~]$ oc set data secret/localusers --from-file htpasswd=/home/student/DO280/labs/auth-provider/htpasswd -n openshift-config
secret/localusers data updated
[student@workstation ~]$ oc delete identity "myusers:manager"
identity.user.openshift.io "myusers:manager" deleted
[student@workstation ~]$ oc delete user manager
user.user.openshift.io "manager" deleted
[student@workstation ~]$ oc login -u manager -p ${MANAGER_PASSWD}
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ oc get users
NAME        UID                                    FULL NAME   IDENTITIES
admin       17914b9a-4a74-4e68-989d-2a988c289136               myusers:admin
developer   ebb1a8e6-3dba-41b2-b0fc-f8a65b0846d7               myusers:developer
[student@workstation ~]$ oc get identity
NAME                IDP NAME   IDP USER NAME   USER NAME   USER UID
myusers:admin       myusers    admin           admin       17914b9a-4a74-4e68-989d-2a988c289136
myusers:developer   myusers    developer       developer   ebb1a8e6-3dba-41b2-b0fc-f8a65b0846d7
[student@workstation ~]$ oc extract secret/localusers -n openshift-config --to -
# htpasswd
admin:$2y$05$a2aKYuv8R96uoQ2xxxTEIeGnEI8xZasIQ5/FGYHO7MXpxzU4MT3He
developer:$apr1$nPaPLoFJ$GYsV0obfH5cznZj0erWtW0
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD}
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc delete project auth-provider
project.project.openshift.io "auth-provider" deleted
[student@workstation ~]$ oc edit oauth
oauth.config.openshift.io/cluster edited
[student@workstation ~]$ oc delete secret localusers -n openshift-config
secret "localusers" deleted
[student@workstation ~]$ oc delete user --all
user.user.openshift.io "admin" deleted
user.user.openshift.io "developer" deleted
[student@workstation ~]$ oc delete identity --all
identity.user.openshift.io "myusers:admin" deleted
identity.user.openshift.io "myusers:developer" deleted
[student@workstation ~]$ lab auth-provider finish

Completing Guided Exercise: Configuring Identity Providers

 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

--------------------------------------------------------------------------------------------------------------------
Guided Exercise
Defining and Applying Permissions using RBAC page: 74

-------------------------------------------------------------------------------------------------------------------
In this exercise, you will define role-based access controls and apply permissions to users.
Outcomes
You should be able to:
• Remove project creation privileges from users who are not OpenShift cluster
administrators.
• Create OpenShift groups and add members to these groups.
• Create a project and assign project administration privileges to the project.
• As a project administrator, assign read and write privileges to different groups of users.

[student@workstation ~]$ lab auth-rbac start

Checking prerequisites for Guided Exercise: Defining and Applying permissions using RBAC

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'auth-rbac' project is absent...........................  SUCCESS

Setting up the classroom for Guided Exercise: Defining and Applying permissions using RBAC

 · Create HTPasswd entry for 'admin'...........................  SUCCESS
 · Create HTPasswd entry for 'leader'..........................  SUCCESS
 · Create HTPasswd entry for 'developer'.......................  SUCCESS
 · Create HTPasswd entry for 'qa-engineer'.....................  SUCCESS
 · Create HTPasswd secret: 'localusers'........................  SUCCESS
 · Add HTPasswd IdP............................................  SUCCESS
 · Pause for creation of first oauth pod.......................  SUCCESS
 · Pause for creation of second oauth pod......................  SUCCESS
 · Wait up to 1 minute for 'pod/oauth-openshift-869cff5b95-srnd
   k'..........................................................  SUCCESS
 · Wait up to 1 minute for oauth pod containers to be ready....  SUCCESS
 · Delete all previous users...................................  SUCCESS
 · Delete all previous identities..............................  SUCCESS
 · Pause 5 seconds before validating authentication............  SUCCESS
 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS
 · Validate 'qa-engineer' can log in with password 'redhat'....  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get clusterrolebinding -o wide | grep -E 'NAME|self-provisioner'
NAME                                                                             ROLE                                                                               AGE     USERS                                   GROUPS                                         SERVICEACCOUNTS
self-provisioners                                                                ClusterRole/self-provisioner                                                       366d                                            system:authenticated:oauth                     
[student@workstation ~]$ oc describe clusterrolebindings self-provisioners
Name:         self-provisioners
Labels:       <none>
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  self-provisioner
Subjects:
  Kind   Name                        Namespace
  ----   ----                        ---------
  Group  system:authenticated:oauth  
[student@workstation ~]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
Warning: Your changes may get lost whenever a master is restarted, unless you prevent reconciliation of this rolebinding using the following command: oc annotate clusterrolebinding.rbac self-provisioners 'rbac.authorization.kubernetes.io/autoupdate=false' --overwrite
clusterrole.rbac.authorization.k8s.io/self-provisioner removed: "system:authenticated:oauth"
[student@workstation ~]$ oc describe clusterrolebindings self-provisioners
Error from server (NotFound): clusterrolebindings.rbac.authorization.k8s.io "self-provisioners" not found
[student@workstation ~]$ oc get clusterrolebinding -o wide | grep -E 'NAME|self-provisioner'
NAME                                                                             ROLE                                                                               AGE    USERS                                   GROUPS                                         SERVICEACCOUNTS
[student@workstation ~]$ oc login -u leader -p redhat
Login successful.

You don't have any projects. Contact your system administrator to request a project.
[student@workstation ~]$ oc new-project test
Error from server (Forbidden): You may not request a new project via this API.
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc new-project auth-rbac
Now using project "auth-rbac" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc policy add-role-to-user admin leader
clusterrole.rbac.authorization.k8s.io/admin added: "leader"
[student@workstation ~]$ oc adm groups new dev-group
group.user.openshift.io/dev-group created
[student@workstation ~]$ oc adm groups new dev-group
Error from server (AlreadyExists): groups.user.openshift.io "dev-group" already exists
[student@workstation ~]$ oc adm groups add-users dev-group developer
group.user.openshift.io/dev-group added: "developer"
[student@workstation ~]$ oc adm groups new qa-group
group.user.openshift.io/qa-group created
[student@workstation ~]$ oc adm groups add-users qa-group qa-engineer
group.user.openshift.io/qa-group added: "qa-engineer"
[student@workstation ~]$ oc get groups
NAME        USERS
dev-group   developer
qa-group    qa-engineer
[student@workstation ~]$ oc login -u leader -p redhat
Login successful.

You have one project on this server: "auth-rbac"

Using project "auth-rbac".
[student@workstation ~]$ oc policy add-role-to-group edit dev-group
clusterrole.rbac.authorization.k8s.io/edit added: "dev-group"
[student@workstation ~]$ oc policy add-role-to-group view qa-group
clusterrole.rbac.authorization.k8s.io/view added: "qa-group"
[student@workstation ~]$ oc get rolebindings -o wide
NAME                    ROLE                               AGE     USERS    GROUPS                             SERVICEACCOUNTS
admin                   ClusterRole/admin                  4m9s    admin                                       
admin-0                 ClusterRole/admin                  3m55s   leader                                      
edit                    ClusterRole/edit                   54s              dev-group                          
system:deployers        ClusterRole/system:deployer        4m9s                                                auth-rbac/deployer
system:image-builders   ClusterRole/system:image-builder   4m9s                                                auth-rbac/builder
system:image-pullers    ClusterRole/system:image-puller    4m9s             system:serviceaccounts:auth-rbac   
view                    ClusterRole/view                   41s              qa-group                           
[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You have one project on this server: "auth-rbac"

Using project "auth-rbac".
[student@workstation ~]$ oc new-app --name httpd httpd:2.4
--> Found image d9163e7 (12 months old) in image stream "openshift/httpd" under tag "2.4" for "httpd:2.4"

    Apache httpd 2.4 
    ---------------- 
    Apache httpd 2.4 available as container, is a powerful, efficient, and extensible web server. Apache supports a variety of features, many implemented as compiled modules which extend the core functionality. These can range from server-side programming language support to authentication schemes. Virtual hosting allows one Apache installation to serve many different Web sites.

    Tags: builder, httpd, httpd24


--> Creating resources ...
    imagestreamtag.image.openshift.io "httpd:2.4" created
    deployment.apps "httpd" created
    service "httpd" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/httpd' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc policy add-role-to-user edit qa-engineer
Error from server (Forbidden): rolebindings.rbac.authorization.k8s.io is forbidden: User "developer" cannot list resource "rolebindings" in API group "rbac.authorization.k8s.io" in the namespace "auth-rbac"
[student@workstation ~]$ oc login -u qa-engineer -p redhat
Login successful.

You have one project on this server: "auth-rbac"

Using project "auth-rbac".
[student@workstation ~]$ oc scale deployment httpd --replicas 3
Error from server (Forbidden): deployments.apps "httpd" is forbidden: User "qa-engineer" cannot patch resource "deployments/scale" in API group "apps" in the namespace "auth-rbac"
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "auth-rbac".
[student@workstation ~]$ oc adm policy add-cluster-role-to-group --rolebinding-name self-provisioners self-provisioner system:authenticated:oauth
Warning: Group 'system:authenticated:oauth' not found
clusterrole.rbac.authorization.k8s.io/self-provisioner added: "system:authenticated:oauth"
[student@workstation ~]$ lab auth-rbac finish

Completing Guided Exercise: Defining and Applying permissions using RBAC

 · Delete OpenShift project 'auth-rbac'........................  SUCCESS
 · Wait for project 'auth-rbac' to be gone.....................  SUCCESS
 · Remove group 'dev-group'....................................  SUCCESS
 · Remove group 'qa-group'.....................................  SUCCESS
 · Delete HTPasswd entry for 'qa-engineer'.....................  SUCCESS
 · Update the 'localusers' secret data.........................  SUCCESS
 · Remove user 'qa-engineer'...................................  SUCCESS
 · Remove identity 'localusers:qa-engineer'....................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

---------------------------------------------------------------------------------------------------------------------
LAB
Configuring Authentication and Authorization Page 81
-----------------------------------------------------------------------------------------------------------------------
Outcomes
You should be able to:
• Create users and passwords for HTPasswd authentication.
• Configure the Identity Provider for HTPasswd authentication.
• Assign cluster administration rights to users.
• Remove the ability to create projects at the cluster level.
• Create groups and add users to groups.
• Manage user privileges in projects by granting privileges to groups.
[student@workstation ~]$ lab auth-review start

Checking prerequisites for Lab: Configuring Authentication and Authorization

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'auth-review' project is absent.........................  SUCCESS

Setting up the classroom for Lab: Configuring Authentication and Authorization

 Preparing the student's workstation:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS
 Restoring authentication settings to installation defaults:
 · No need to perform any change...............................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ htpasswd -D ~/DO280/labs/auth-review/tmp_users analyst
Deleting password for user analyst
[student@workstation ~]$ for NAME in tester leader admin developer; do htpasswd -b ~/DO280/labs/auth-review/tmp_users ${NAME} 'L@bR3v!ew';done
Updating password for user tester
Updating password for user leader
Adding password for user admin
Adding password for user developer
[student@workstation ~]$ cat ~/DO280/labs/auth-review/tmp_users
tester:$apr1$Q6ws3BHj$Wwy76CGsevo.DRzo0qvDX0
leader:$apr1$r355xPDT$X3LledBcIheIIcp4PWr21/
admin:$apr1$D/68bwT1$aFhSdvDUWqsU9kXBXMDZE/
developer:$apr1$uBVFBpcF$J9S.lWUs6l0Q0aAclGTq0/
[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc create secret generic auth-review --from-file htpasswd=/home/student/DO280/labs/auth-review/tmp_users -n openshift-config
secret/auth-review created
[student@workstation ~]$ oc get oauth cluster -o yaml > ~/DO280/labs/auth-review/oauth.yaml
[student@workstation ~]$ vi !$
vi ~/DO280/labs/auth-review/oauth.yaml
[student@workstation ~]$ cat !$
cat ~/DO280/labs/auth-review/oauth.yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  creationTimestamp: "2020-08-05T18:23:41Z"
  generation: 8
  managedFields:
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec: {}
    manager: cluster-version-operator
    operation: Update
    time: "2020-08-05T18:23:41Z"
  name: cluster
  resourceVersion: "106407"
  selfLink: /apis/config.openshift.io/v1/oauths/cluster
  uid: c96a18f6-b8de-4722-b5bf-ca6c39cdb78b
spec: 
  identityProviders:
  - htpasswd:
      fileData:
        name: auth-review
    mappingMethod: claim
    name: htpasswd
    type: HTPasswd
[student@workstation ~]$ oc replace -f ~/DO280/labs/auth-review/oauth.yaml
oauth.config.openshift.io/cluster replaced
[student@workstation ~]$ oc get pods -n openshift-authentication
NAME                               READY   STATUS    RESTARTS   AGE
oauth-openshift-57d5b9656b-qjqkb   1/1     Running   0          8m17s
oauth-openshift-57d5b9656b-tr7tx   1/1     Running   0          8m27s
oauth-openshift-5db697b59f-wcsds   0/1     Running   0          9s
[student@workstation ~]$ oc get pods -n openshift-authentication
NAME                               READY   STATUS        RESTARTS   AGE
oauth-openshift-57d5b9656b-qjqkb   1/1     Terminating   0          8m26s
oauth-openshift-57d5b9656b-tr7tx   1/1     Running       0          8m36s
oauth-openshift-5db697b59f-psd8c   0/1     Running       0          7s
oauth-openshift-5db697b59f-wcsds   1/1     Running       0          18s
[student@workstation ~]$ oc adm policy add-cluster-role-to-user cluster-admin admin
Warning: User 'admin' not found
clusterrole.rbac.authorization.k8s.io/cluster-admin added: "admin"
[student@workstation ~]$ oc login -u admin -p 'L@bR3v!ew'
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get nodes
NAME       STATUS   ROLES           AGE    VERSION
master01   Ready    master,worker   367d   v1.18.3+012b3ec
master02   Ready    master,worker   367d   v1.18.3+012b3ec
master03   Ready    master,worker   367d   v1.18.3+012b3ec
[student@workstation ~]$ oc login -u developer -p 'L@bR3v!ew'
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc get nodes
Error from server (Forbidden): nodes is forbidden: User "developer" cannot list resource "nodes" in API group "" at the cluster scope
[student@workstation ~]$ oc login -u admin -p 'L@bR3v!ew'
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
Warning: Your changes may get lost whenever a master is restarted, unless you prevent reconciliation of this rolebinding using the following command: oc annotate clusterrolebinding.rbac self-provisioners 'rbac.authorization.kubernetes.io/autoupdate=false' --overwrite
clusterrole.rbac.authorization.k8s.io/self-provisioner removed: "system:authenticated:oauth"
[student@workstation ~]$ oc adm groups new managers
group.user.openshift.io/managers created
[student@workstation ~]$ oc adm groups add-users managers leader
group.user.openshift.io/managers added: "leader"
[student@workstation ~]$ oc adm policy add-cluster-role-to-group self-provisioner managers
clusterrole.rbac.authorization.k8s.io/self-provisioner added: "managers"
[student@workstation ~]$ oc login -u leader -p 'L@bR3v!ew'
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc new-project auth-review
Now using project "auth-review" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc login -u admin -p 'L@bR3v!ew'
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "auth-review".
[student@workstation ~]$ oc adm groups new developers
group.user.openshift.io/developers created
[student@workstation ~]$ oc adm groups add-users developers developer
group.user.openshift.io/developers added: "developer"
[student@workstation ~]$ oc policy add-role-to-group edit developers
clusterrole.rbac.authorization.k8s.io/edit added: "developers"
[student@workstation ~]$ oc adm groups new qa
group.user.openshift.io/qa created
[student@workstation ~]$ oc adm groups add-users qa tester
group.user.openshift.io/qa added: "tester"
[student@workstation ~]$ oc policy add-role-to-group view qa
clusterrole.rbac.authorization.k8s.io/view added: "qa"
[student@workstation ~]$ lab auth-review grade

Grading the student's work for Lab: Configuring Authentication and Authorization

 · Cluster uses the HTPasswd identity provider.................  PASS
 · User 'analyst' does not exist in the HTPasswd secret........  PASS
 · The 'admin' user can log in with a password of 'L@bR3v!ew'..  PASS
 · The 'admin' user has the 'cluster-admin' cluster role.......  PASS
 · The 'self-provisioner' cluster role has been removed from th
   e 'system:authenticated:oauth' group........................  PASS
 · The 'managers' group exists.................................  PASS
 · The 'managers' group has the 'self-provisioner' cluster role
   ............................................................  PASS
 · The 'managers' group contains the 'leader' user.............  PASS
 · The 'leader' user can log in with a password of 'L@bR3v!ew'.  PASS
 · The 'auth-review' project exists............................  PASS
 · The 'leader' user has the 'admin' role on the 'auth-review' 
   project.....................................................  PASS
 · The 'developers' group exists...............................  PASS
 · The 'developers' group has the 'edit' role on the 'auth-revi
   ew' project.................................................  PASS
 · The 'dvelopers' group contains the 'developer' user.........  PASS
 · The 'developer' user can log in with a password of 'L@bR3v!e
   w'..........................................................  PASS
 · The 'qa' group exists.......................................  PASS
 · The 'qa' group has the 'view' role on the 'auth-review' proj
   ect.........................................................  PASS
 · The 'qa' group contains the 'tester' user...................  PASS
 · The 'tester' user can log in with a password of 'L@bR3v!ew'.  PASS

Overall exercise grade.........................................  PASS

[student@workstation ~]$ lab auth-review finish

Completing Lab: Configuring Authentication and Authorization

 · Delete OpenShift project 'auth-review'......................  SUCCESS
 · Wait for project 'auth-review' to be gone...................  SUCCESS
 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS
 · Restore project creation privileges.........................  SUCCESS
 · Removing the cluster role binding 'clusterrolebinding.rbac.a
   uthorization.k8s.io/self-provisioner'.......................  SUCCESS
 Restoring authentication settings to installation defaults:
 · Removing 'cluster-admin' role from the 'admin' user.........  SUCCESS
 · Remove HTPasswd secret: 'auth-review'.......................  SUCCESS
 · Remove all configured Identity Providers....................  SUCCESS
 · Remove all existing users...................................  SUCCESS
 · Remove all existing groups..................................  SUCCESS
 · Remove all existing identities..............................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

Summary
-------
In this chapter, you learned:
• A newly-installed OpenShift cluster provides two authentication methods that grant
administrative access: the kubeconfig file and the kubeadmin virtual user.
• The HTPasswd identity provider authenticates users against credentials stored in a secret. The
name of the secret, and other settings for the identity provider, are stored inside the OAuth
custom resource.
• To manage user credentials using the HTPasswd identity provider, you must extract data from
the secret, change that data using the htpasswd command, and then apply the data back to
the secret.
• Creating OpenShift users requires valid credentials, managed by an identity provider, plus user
and identity resources.
• Deleting OpenShift users requires deleting their credentials from the identity provider, and also
deleting their user and identity resources.
• OpenShift uses role-based access control (RBAC) to control user actions. A role is a collection
of rules that govern interaction with OpenShift resources. Default roles exist for cluster
administrators, developers, and auditors.
• To control user interaction, assign a user to one or more roles. A role binding contains all of the
associations of a role to users and groups.
• To grant a user cluster administrator privileges, assign the cluster-admin role to that user.

-------------------------------------------------------------------------------------------------------






