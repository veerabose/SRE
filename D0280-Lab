Summary
In this chapter, you learned:
• Red Hat OpenShift Container Platform is based on Red Hat Enterprise Linux CoreOS, the CRI-
O container engine, and Kubernetes.
• RHOCP 4 provides a number of services on top of Kubernetes, such as an internal container
image registry, storage, networking providers, and centralized logging and monitoring.
• Operators package applications that manage Kubernetes resources, and the Operator Lifecycle
Manager (OLM) handles installation and management of operators.
• OperatorHub.io is an online catalog for discovering operators.

oc get nodes
oc adm top nodes
oc describe node my-node-name
oc login -u kubeadmin -p MMTUc-TnXjo-NFyh3-aeWmC
oc get clusterversion
oc describe clusterversion
oc get clusteroperators

Displaying the Logs of OpenShift Nodes
oc adm node-logs -u crio my-node-name
oc adm node-logs -u kubelet my-node-name
oc adm node-logs my-node-name

[user@demo ~]$ oc debug node/my-node-name
...output omitted..................
sh-4.2# chroot /host
sh-4.2# systemctl is-active kubelet
active

Troubleshooting The Container Engine
user@demo ~]$ oc debug node/my-node-name
...output omitted...
sh-4.2# chroot /host
sh-4.2# crictl ps
...output omitted...

oc get events
oc describe pod

[user@demo ~]$ oc logs my-pod-name

If the pod contains multiple containers, then the oc logs command requires the -c option.
[user@demo ~]$ oc logs my-pod-name -c my-container-name


oc debug deployment/my-deployment-name --as-root

oc rsh my-pod-name
Opens a shell inside a pod to run shell commands interactively and non-interactively.

oc cp /local/path my-pod-name:/container/path
Copies local files to a location inside a pod. You can also reverse arguments and copy files
from inside a pod to your local file system. See also the oc rsync command for copying
multiple files at once.

oc port-forward my-pod-name local-port:remote-port
Creates a TCP tunnel from local-port on your workstation to local-port on the pod.
The tunnel is alive as long as you keep the oc port-forward running. This allows you to get

The --loglevel level option displays OpenShift API requests, starting with level 6. As you
increase the level, up to 10, more information about those requests is added, such as their HTTP
request headers and response bodies. Level 10 also includes a curl command to replicate each
request.
You can try these two commands, from any project, and compare their outputs.
[user@demo ~]$ oc get pod --loglevel 6
[user@demo ~]$ oc get pod --loglevel 10


[user@demo ~]$ oc whoami -t
network access to the pod without exposing it through a route. Because the tunnel starts at
your localhost, it cannot be accessed by other machines.

---------------------------------------------------------------------------------------------------
Chapter 2 | Verifying the Health of a Cluster
Guided Exercise
Troubleshooting OpenShift Clusters and Applications  Page 36
----------------------------------------------------------------------------------------------------
[student@workstation ~]$ lab execute-troubleshoot start

Checking prerequisites for Guided Exercise: Troubleshooting OpenShift Clusters and Applications

 Preparing the student's workstation:
 · Download exercise files.....................................  SUCCESS
 Verify the OpenShift cluster is running:
 · Waiting up to 5 minutes for router pods to be available.....  
SUCCESS
 · Waiting up to 5 minutes for OAuth to be available...........  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'execute-troubleshoot' project is absent................  SUCCESS

Setting up the classroom for Guided Exercise: Troubleshooting OpenShift Clusters and Applications

 Restoring authentication settings to installation defaults:
 · No need to perform any change...............................  SUCCESS
 Preparing the exercise:
 · Create project 'execute-troubleshoot'.......................  SUCCESS
 · Deploy PostgreSQL in 'execute-troubleshoot'.................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): y

Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
Welcome! See 'oc help' to get started.
[student@workstation ~]$ oc get nodes
NAME       STATUS                     ROLES           AGE    VERSION
master01   Ready,SchedulingDisabled   master,worker   366d   v1.18.3+012b3ec
master02   Ready                      master,worker   366d   v1.18.3+012b3ec
master03   Ready                      master,worker   366d   v1.18.3+012b3ec
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
error: metrics not available yet
[student@workstation ~]$ oc adm top node
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%     
master02   1584m        45%    5105Mi          34%         
master03   2671m        76%    6110Mi          40%         
master01   <unknown>                           <unknown>               <unknown>               <unknown>               
[student@workstation ~]$ oc adm top node
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master01   3742m        106%   2812Mi          18%       
master02   3102m        88%    3110Mi          20%       
master03   3176m        90%    6158Mi          41%       
[student@workstation ~]$ oc describe node master01
Name:               master01
Roles:              master,worker
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=master01
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
                    node-role.kubernetes.io/worker=
                    node.openshift.io/os_id=rhcos
Annotations:        machineconfiguration.openshift.io/currentConfig: rendered-master-ebfe876315cf6b098e7eb8750dfc72ed
                    machineconfiguration.openshift.io/desiredConfig: rendered-master-ebfe876315cf6b098e7eb8750dfc72ed
                    machineconfiguration.openshift.io/reason: 
                    machineconfiguration.openshift.io/state: Done
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 05 Aug 2020 14:23:06 -0400
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  master01
  AcquireTime:     <unset>
  RenewTime:       Sat, 07 Aug 2021 03:44:24 -0400
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 07 Aug 2021 03:43:34 -0400   Sat, 07 Aug 2021 03:43:34 -0400   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 07 Aug 2021 03:43:34 -0400   Sat, 07 Aug 2021 03:43:34 -0400   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 07 Aug 2021 03:43:34 -0400   Sat, 07 Aug 2021 03:43:34 -0400   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 07 Aug 2021 03:43:34 -0400   Sat, 07 Aug 2021 03:43:34 -0400   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.50.10
  Hostname:    master01
Capacity:
  cpu:                4
  ephemeral-storage:  41391084Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16419132Ki
  pods:               250
Allocatable:
  cpu:                3500m
  ephemeral-storage:  37072281128
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15268156Ki
  pods:               250
System Info:
  Machine ID:                                 8647fbfd86c5432aa6c9da08d596d4cc
  System UUID:                                54b4ab2d-135d-4ad9-b903-6fa97a2c5aaf
  Boot ID:                                    3affb095-7851-4b07-b451-24d2620bf124
  Kernel Version:                             4.18.0-193.13.2.el8_2.x86_64
  OS Image:                                   Red Hat Enterprise Linux CoreOS 45.82.202007240629-0 (Ootpa)
  Operating System:                           linux
  Architecture:                               amd64
  Container Runtime Version:                  cri-o://1.18.3-5.rhaos4.5.git1c13d1d.el8
  Kubelet Version:                            v1.18.3+012b3ec
  Kube-Proxy Version:                         v1.18.3+012b3ec
PodCIDR:                                      10.8.1.0/24
PodCIDRs:                                     10.8.1.0/24
Non-terminated Pods:                          (43 in total)
  Namespace                                   Name                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                                   ----                                                 ------------  ----------  ---------------  -------------  ---
  execute-troubleshoot                        psql-69676884f8-fpl98                                0 (0%)        0 (0%)      512Mi (3%)       512Mi (3%)     34s
  openshift-apiserver                         apiserver-59b6b7968c-zzv6g                           100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         7m21s
  openshift-authentication                    oauth-openshift-68799984ff-hmnqr                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         31s
  openshift-cloud-credential-operator         cloud-credential-operator-5bf8fb79db-jgd6l           10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         32s
  openshift-cluster-node-tuning-operator      tuned-qjbhv                                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         366d
  openshift-cluster-storage-operator          csi-snapshot-controller-operator-6f9d5b4bfc-w6987    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         33s
  openshift-console                           console-5df4fcbb47-r99g8                             10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         30s
  openshift-console                           downloads-5565cdb86b-9fccj                           10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         32s
  openshift-controller-manager                controller-manager-fkl2v                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         8m55s
  openshift-dns                               dns-default-tfhlc                                    65m (1%)      0 (0%)      110Mi (0%)       512Mi (3%)     366d
  openshift-etcd-operator                     etcd-operator-fcd9d5cb7-llglj                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         32s
  openshift-etcd                              etcd-master01                                        460m (13%)    0 (0%)      920Mi (6%)       0 (0%)         366d
  openshift-image-registry                    node-ca-qrrpd                                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         366d
  openshift-ingress                           router-default-57575dc57c-kcnvz                      100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         34s
  openshift-kube-apiserver                    kube-apiserver-master01                              330m (9%)     0 (0%)      1174Mi (7%)      0 (0%)         4m54s
  openshift-kube-controller-manager-operator  kube-controller-manager-operator-5658d865f7-bw9zx    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         33s
  openshift-kube-controller-manager           kube-controller-manager-master01                     100m (2%)     0 (0%)      500Mi (3%)       0 (0%)         366d
  openshift-kube-scheduler                    openshift-kube-scheduler-master01                    20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         366d
  openshift-kube-storage-version-migrator     migrator-84d87c7d77-jhmmw                            100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         32s
  openshift-machine-api                       cluster-autoscaler-operator-d5698d448-skhk2          30m (0%)      0 (0%)      70Mi (0%)        0 (0%)         32s
  openshift-machine-config-operator           etcd-quorum-guard-5fc4989788-5d62h                   10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         7m20s
  openshift-machine-config-operator           machine-config-daemon-l4ftv                          40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         366d
  openshift-machine-config-operator           machine-config-operator-5b87d8f4c5-vd7kp             20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         34s
  openshift-machine-config-operator           machine-config-server-cn4cx                          20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         366d
  openshift-marketplace                       certified-operators-85ccbb74bb-fb9p5                 10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         34s
  openshift-marketplace                       community-operators-7998c846c7-kk45k                 10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         34s
  openshift-marketplace                       redhat-marketplace-5bc4744b59-l8qpc                  10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         34s
  openshift-monitoring                        alertmanager-main-1                                  8m (0%)       0 (0%)      260Mi (1%)       0 (0%)         17s
  openshift-monitoring                        alertmanager-main-2                                  8m (0%)       0 (0%)      260Mi (1%)       0 (0%)         17s
  openshift-monitoring                        grafana-68bbfbdf97-mffvw                             5m (0%)       0 (0%)      120Mi (0%)       0 (0%)         31s
  openshift-monitoring                        node-exporter-7z66r                                  9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         366d
  openshift-monitoring                        prometheus-adapter-c868b4f4d-pfm9t                   1m (0%)       0 (0%)      25Mi (0%)        0 (0%)         31s
  openshift-monitoring                        prometheus-k8s-1                                     76m (2%)      0 (0%)      1184Mi (7%)      0 (0%)         17s
  openshift-monitoring                        thanos-querier-6c89f5cff9-v8fp4                      8m (0%)       0 (0%)      72Mi (0%)        0 (0%)         31s
  openshift-multus                            multus-admission-controller-plrpp                    20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         366d
  openshift-multus                            multus-xpkf5                                         10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         366d
  openshift-network-operator                  network-operator-6bd8b9b698-xbg4q                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         30s
  openshift-operator-lifecycle-manager        packageserver-d56b544dd-zmrsk                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         14s
  openshift-sdn                               ovs-7bzt6                                            100m (2%)     0 (0%)      400Mi (2%)       0 (0%)         366d
  openshift-sdn                               sdn-controller-czjdg                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         366d
  openshift-sdn                               sdn-xgl8z                                            100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         366d
  openshift-service-ca-operator               service-ca-operator-5f596775f8-szx8b                 10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         30s
  openshift-service-ca                        service-ca-8b8db557b-zlwl4                           10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         35s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                2 (57%)       0 (0%)
  memory             8408Mi (56%)  1Gi (6%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
Events:
  Type     Reason                   Age                  From               Message
  ----     ------                   ----                 ----               -------
  Normal   NodeNotSchedulable       366d                 kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   Starting                 366d                 kubelet, master01  Starting kubelet.
  Normal   NodeHasSufficientMemory  366d (x2 over 366d)  kubelet, master01  Node master01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    366d (x2 over 366d)  kubelet, master01  Node master01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     366d (x2 over 366d)  kubelet, master01  Node master01 status is now: NodeHasSufficientPID
  Warning  Rebooted                 366d                 kubelet, master01  Node master01 has been rebooted, boot id: 087a4978-52e9-4981-a3e4-9a640017b061
  Normal   NodeNotReady             366d                 kubelet, master01  Node master01 status is now: NodeNotReady
  Normal   NodeNotSchedulable       366d                 kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   NodeAllocatableEnforced  366d                 kubelet, master01  Updated Node Allocatable limit across pods
  Normal   NodeReady                366d                 kubelet, master01  Node master01 status is now: NodeReady
  Normal   NodeSchedulable          366d                 kubelet, master01  Node master01 status is now: NodeSchedulable
  Normal   NodeNotSchedulable       7m20s                kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   NodeReady                2m2s                 kubelet, master01  Node master01 status is now: NodeReady
  Normal   NodeHasSufficientMemory  2m2s (x2 over 2m2s)  kubelet, master01  Node master01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    2m2s (x2 over 2m2s)  kubelet, master01  Node master01 status is now: NodeHasNoDiskPressure
  Normal   Starting                 2m2s                 kubelet, master01  Starting kubelet.
  Warning  Rebooted                 2m2s                 kubelet, master01  Node master01 has been rebooted, boot id: 4167e399-5543-4339-9d6e-cfec9a429282
  Normal   NodeNotReady             2m2s                 kubelet, master01  Node master01 status is now: NodeNotReady
  Normal   NodeNotSchedulable       2m2s                 kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   NodeAllocatableEnforced  2m2s                 kubelet, master01  Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientPID     2m2s (x2 over 2m2s)  kubelet, master01  Node master01 status is now: NodeHasSufficientPID
  Normal   Starting                 53s                  kubelet, master01  Starting kubelet.
  Normal   NodeHasSufficientMemory  53s (x2 over 53s)    kubelet, master01  Node master01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    53s (x2 over 53s)    kubelet, master01  Node master01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     53s (x2 over 53s)    kubelet, master01  Node master01 status is now: NodeHasSufficientPID
  Warning  Rebooted                 53s                  kubelet, master01  Node master01 has been rebooted, boot id: 3affb095-7851-4b07-b451-24d2620bf124
  Normal   NodeNotReady             53s                  kubelet, master01  Node master01 status is now: NodeNotReady
  Normal   NodeNotSchedulable       53s                  kubelet, master01  Node master01 status is now: NodeNotSchedulable
  Normal   NodeAllocatableEnforced  53s                  kubelet, master01  Updated Node Allocatable limit across pods
  Normal   NodeReady                53s                  kubelet, master01  Node master01 status is now: NodeReady
  Normal   NodeSchedulable          32s                  kubelet, master01  Node master01 status is now: NodeSchedulable

[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get pod -n openshift-image-registry
NAME                                               READY   STATUS    RESTARTS   AGE
cluster-image-registry-operator-5f47f6fcf7-vd2x5   2/2     Running   0          5m47s
image-registry-8c54885cd-6hd5v                     1/1     Running   0          5m47s
node-ca-24k9d                                      1/1     Running   0          366d
node-ca-lfh4z                                      1/1     Running   0          366d
node-ca-qrrpd                                      1/1     Running   0          366d
[student@workstation ~]$ oc logs --tail 3 -n openshift-image-registry -c cluster-image-registry-operator cluster-image-registry-operator-5f47f6fcf7-vd2x5
I0807 07:54:01.359030      12 clusteroperator.go:99] event from workqueue successfully processed
I0807 07:54:02.397528      12 clusteroperator.go:99] event from workqueue successfully processed
I0807 07:54:03.564534      12 clusteroperator.go:99] event from workqueue successfully processed
[student@workstation ~]$ oc logs --tail 1 -n openshift-image-registry image-registry-8c54885cd-6hd5v
time="2021-08-07T07:55:31.344300341Z" level=info msg=response go.version=go1.13.4 http.request.host="10.8.0.20:5000" http.request.id=61793a83-bd26-44db-a60d-a761c2d72665 http.request.method=GET http.request.remoteaddr="10.8.0.1:41478" http.request.uri=/healthz http.request.useragent=kube-probe/1.18+ http.response.duration="49.423µs" http.response.status=200 http.response.written=0
[student@workstation ~]$ oc adm node-logs --tail 1 -u kubelet master01
-- Logs begin at Wed 2020-08-05 18:20:28 UTC, end at Sat 2021-08-07 07:56:12 UTC. --
Aug 07 07:42:44.421312 master01 systemd[1]: kubelet.service: Consumed 3.401s CPU time
-- Logs begin at Wed 2020-08-05 18:20:28 UTC, end at Sat 2021-08-07 07:56:12 UTC. --
Aug 07 07:56:12.231869 master01 hyperkube[1501]: I0807 07:56:12.231863    1501 prober.go:133] Readiness probe for "packageserver-d56b544dd-zmrsk_openshift-operator-lifecycle-manager(c5600d4d-a93d-47c1-8fc9-ea9d26d47b66):packageserver" succeeded

[student@workstation ~]$ oc debug node/master01
Starting pod/master01-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.50.10
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host 
sh-4.4# systemctl status kubelet
● kubelet.service - MCO environment configuration
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-mco-default-env.conf
   Active: active (running) since Sat 2021-08-07 07:43:34 UTC; 14min ago
  Process: 1499 ExecStartPre=/bin/rm -f /var/lib/kubelet/cpu_manager_state (code=exited, status=0/SUCCESS)
  Process: 1497 ExecStartPre=/bin/mkdir --parents /etc/kubernetes/manifests (code=exited, status=0/SUCCESS)
 Main PID: 1501 (kubelet)
    Tasks: 32 (limit: 102120)
   Memory: 398.1M
      CPU: 1min 58.425s
   CGroup: /system.slice/kubelet.service
           └─1501 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfi>

Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.736141    1501 prober.go:184] HTTP-Probe Headers: map[]
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.743564    1501 http.go:128] Probe succeeded for https:/>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.743637    1501 prober.go:133] Liveness probe for "kube->
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.803319    1501 prober.go:181] HTTP-Probe Host: http://l>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.803356    1501 prober.go:184] HTTP-Probe Headers: map[]
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.805316    1501 http.go:128] Probe succeeded for http://>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.805371    1501 prober.go:133] Liveness probe for "route>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.846469    1501 kubelet.go:1989] SyncLoop (SYNC): 1 pods>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.846569    1501 kubelet.go:2034] Pod "recyler-pod-master>
Aug 07 07:57:38 master01 hyperkube[1501]: I0807 07:57:38.846603    1501 kubelet.go:2012] SyncLoop (housekeeping)
sh-4.4# systemctl status cri-o
● crio.service - MCO environment configuration
   Loaded: loaded (/usr/lib/systemd/system/crio.service; disabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/crio.service.d
           └─10-mco-default-env.conf
   Active: active (running) since Sat 2021-08-07 07:43:30 UTC; 14min ago
     Docs: https://github.com/cri-o/cri-o
 Main PID: 1452 (crio)
    Tasks: 36
   Memory: 458.5M
      CPU: 2min 16.593s
   CGroup: /system.slice/crio.service
           ├─ 1452 /usr/bin/crio --enable-metrics=true --metrics-port=9537
           ├─73296 /usr/libexec/crio/conmon -c 50ee88b6365fa08d27fb087ffa31281611b2dbe1c196db6ad9c44dbbf663df13 >
           └─73297 /usr/bin/runc --root=/run/runc exec --pid-file /tmp/pidfile818200403 --process /tmp/exec-proc>

Aug 07 07:57:11 master01 crio[1452]: time="2021-08-07 07:57:11.380192551Z" level=info msg="Starting container: 0>
Aug 07 07:57:11 master01 crio[1452]: time="2021-08-07 07:57:11.403389087Z" level=info msg="Started container 096>
Aug 07 07:57:14 master01 crio[1452]: time="2021-08-07 07:57:14.862350110Z" level=info msg="Checking image status>
Aug 07 07:57:14 master01 crio[1452]: time="2021-08-07 07:57:14.862637971Z" level=info msg="Image registry.access>
Aug 07 07:57:27 master01 crio[1452]: time="2021-08-07 07:57:27.848476912Z" level=info msg="Checking image status>
Aug 07 07:57:27 master01 crio[1452]: time="2021-08-07 07:57:27.848702947Z" level=info msg="Image registry.access>
Aug 07 07:57:39 master01 crio[1452]: time="2021-08-07 07:57:39.847893007Z" level=info msg="Checking image status>
Aug 07 07:57:39 master01 crio[1452]: time="2021-08-07 07:57:39.848203581Z" level=info msg="Image registry.access>
Aug 07 07:57:54 master01 crio[1452]: time="2021-08-07 07:57:54.850299018Z" level=info msg="Checking image status>
Aug 07 07:57:54 master01 crio[1452]: time="2021-08-07 07:57:54.850699985Z" level=info msg="Image registry.access>
sh-4.4# crictl ps --name openvswitch
CONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID
e5db87f51af56       81ac7e4df0c960f19146662f538bcc2b72709040390ef6f94bfa880f5c0ebc3a   14 minutes ago      Running             openvswitch         0                   f78c3cd101d9b
sh-4.4# exit
exit
sh-4.2# exit
exit

Removing debug pod ...
[student@workstation ~]$ oc project execute-troubleshoot
Now using project "execute-troubleshoot" on server "https://api.ocp4.example.com:6443".
[student@workstation ~]$ oc get pod
NAME                    READY   STATUS             RESTARTS   AGE
psql-69676884f8-fpl98   0/1     ImagePullBackOff   0          14m
[student@workstation ~]$ oc status
In project execute-troubleshoot on server https://api.ocp4.example.com:6443

svc/psql - 172.30.150.124:5432

deployment/psql deploys registry.access.redhat.com/rhscl/postgresq-96-rhel7:1
  deployment #1 running for 19 minutes - 0/1 pods

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.
[student@workstation ~]$ oc get events
LAST SEEN   TYPE      REASON              OBJECT                       MESSAGE
19m         Normal    Scheduled           pod/psql-69676884f8-czk8l    Successfully assigned execute-troubleshoot/psql-69676884f8-czk8l to master02
19m         Normal    AddedInterface      pod/psql-69676884f8-czk8l    Add eth0 [10.8.0.30/23]
18m         Normal    Pulling             pod/psql-69676884f8-czk8l    Pulling image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1"
18m         Warning   Failed              pod/psql-69676884f8-czk8l    Failed to pull image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1": rpc error: code = Unknown desc = Error reading manifest 1 in registry.access.redhat.com/rhscl/postgresq-96-rhel7: name unknown: Repo not found
18m         Warning   Failed              pod/psql-69676884f8-czk8l    Error: ErrImagePull
17m         Normal    BackOff             pod/psql-69676884f8-czk8l    Back-off pulling image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1"
17m         Warning   Failed              pod/psql-69676884f8-czk8l    Error: ImagePullBackOff
15m         Normal    Scheduled           pod/psql-69676884f8-fpl98    Successfully assigned execute-troubleshoot/psql-69676884f8-fpl98 to master01
15m         Normal    AddedInterface      pod/psql-69676884f8-fpl98    Add eth0 [10.10.0.12/23]
14m         Normal    Pulling             pod/psql-69676884f8-fpl98    Pulling image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1"
14m         Warning   Failed              pod/psql-69676884f8-fpl98    Failed to pull image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1": rpc error: code = Unknown desc = Error reading manifest 1 in registry.access.redhat.com/rhscl/postgresq-96-rhel7: name unknown: Repo not found
14m         Warning   Failed              pod/psql-69676884f8-fpl98    Error: ErrImagePull
5m20s       Normal    BackOff             pod/psql-69676884f8-fpl98    Back-off pulling image "registry.access.redhat.com/rhscl/postgresq-96-rhel7:1"
10s         Warning   Failed              pod/psql-69676884f8-fpl98    Error: ImagePullBackOff
19m         Normal    SuccessfulCreate    replicaset/psql-69676884f8   Created pod: psql-69676884f8-czk8l
15m         Normal    SuccessfulCreate    replicaset/psql-69676884f8   Created pod: psql-69676884f8-fpl98
19m         Normal    ScalingReplicaSet   deployment/psql              Scaled up replica set psql-69676884f8 to 1
[student@workstation ~]$ skopeo inspect docker://registry.access.redhat.com/rhscl/postgresq-96-rhel7:1
FATA[0000] Error parsing image name "docker://registry.access.redhat.com/rhscl/postgresq-96-rhel7:1": Error reading manifest 1 in registry.access.redhat.com/rhscl/postgresq-96-rhel7: name unknown: Repo not found 
[student@workstation ~]$ skopeo inspect docker://registry.access.redhat.com/rhscl/postgresql-96-rhel7:1
{
    "Name": "registry.access.redhat.com/rhscl/postgresql-96-rhel7",
    "Digest": "sha256:6c3090779c0553d773380ff4cb0532086075376964de398417d494313f9a933b",
    "RepoTags": [
        "1-42.1561731092",
        "1-59",
        "1-58",
        "1-17",
        "1-10",
        "1-13",
        "1-12",
        "1-51",
        "1-53",
        "1-52",
        "1-57",
        "1-38",
        "1-37",
        "1-36",
        "1-32",
        "1",
        "1-25.1535384678",
        "1-8",
        "1-5",
        "1-4",
        "1-7",
        "1-6",
        "1-61",
        "1-63",
        "1-48",
        "1-49",
        "1-46",
        "1-47",
        "1-44",
        "1-45",
        "1-42",
        "1-40",
        "1-51.1575996559",
        "1-28",
        "1-14",
        "1-25",
        "1-27",
        "latest"
    ],
    "Created": "2020-09-21T15:39:59.766636Z",
    "DockerVersion": "1.13.1",
    "Labels": {
        "architecture": "x86_64",
        "build-date": "2020-09-21T15:38:28.666467",
        "com.redhat.build-host": "cpt-1002.osbs.prod.upshift.rdu2.redhat.com",
        "com.redhat.component": "rh-postgresql96-container",
        "com.redhat.license_terms": "https://www.redhat.com/en/about/red-hat-end-user-license-agreements#rhel",
        "description": "PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.",
        "distribution-scope": "public",
        "io.k8s.description": "PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.",
        "io.k8s.display-name": "PostgreSQL 9.6",
        "io.openshift.expose-services": "5432:postgresql",
        "io.openshift.s2i.assemble-user": "26",
        "io.openshift.s2i.scripts-url": "image:///usr/libexec/s2i",
        "io.openshift.tags": "database,postgresql,postgresql96,rh-postgresql96",
        "io.s2i.scripts-url": "image:///usr/libexec/s2i",
        "maintainer": "SoftwareCollections.org \u003csclorg@redhat.com\u003e",
        "name": "rhscl/postgresql-96-rhel7",
        "release": "63",
        "summary": "PostgreSQL is an advanced Object-Relational database management system",
        "url": "https://access.redhat.com/containers/#/registry.access.redhat.com/rhscl/postgresql-96-rhel7/images/1-63",
        "usage": "podman run -d --name postgresql_database -e POSTGRESQL_USER=user -e POSTGRESQL_PASSWORD=pass -e POSTGRESQL_DATABASE=db -p 5432:5432 rhscl/postgresql-96-rhel7",
        "vcs-ref": "ae9489bc8bf8f1c543d15dbe9ed5b9d1b6adcacd",
        "vcs-type": "git",
        "vendor": "Red Hat, Inc.",
        "version": "1"
    },
    "Architecture": "amd64",
    "Os": "linux",
    "Layers": [
        "sha256:1323a241cc068f2816dd88f00168be73339471d6dc6eb2e6c761b63b734501b6",
        "sha256:2bd25ca124579d6fce8668ff5d4ed83866d7e7438cb561a51ddde8cc40272822",
        "sha256:5d011ac93e7456d4c646b0fbb53712598bda9a6b0d027b2be788016e078ded77",
        "sha256:16634824fe3f2f2bbbf87f9a3082ec4b3d1b3b084ce14b3e63419a4dc5b3150d"
    ],
    "Env": [
        "PATH=/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
        "container=oci",
        "SUMMARY=PostgreSQL is an advanced Object-Relational database management system",
        "DESCRIPTION=PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.",
        "STI_SCRIPTS_URL=image:///usr/libexec/s2i",
        "STI_SCRIPTS_PATH=/usr/libexec/s2i",
        "APP_ROOT=/opt/app-root",
        "HOME=/var/lib/pgsql",
        "PLATFORM=el7",
        "BASH_ENV=/usr/share/container-scripts/postgresql/scl_enable",
        "ENV=/usr/share/container-scripts/postgresql/scl_enable",
        "PROMPT_COMMAND=. /usr/share/container-scripts/postgresql/scl_enable",
        "POSTGRESQL_VERSION=9.6",
        "POSTGRESQL_PREV_VERSION=9.5",
        "PGUSER=postgres",
        "APP_DATA=/opt/app-root",
        "CONTAINER_SCRIPTS_PATH=/usr/share/container-scripts/postgresql",
        "ENABLED_COLLECTIONS=rh-postgresql96"
    ]
}
[student@workstation ~]$ oc edit deployment/psql
deployment.apps/psql edited
[student@workstation ~]$ oc status
In project execute-troubleshoot on server https://api.ocp4.example.com:6443

svc/psql - 172.30.150.124:5432

deployment/psql deploys registry.access.redhat.com/rhscl/postgresql-96-rhel7:1
  deployment #2 running for 8 seconds - 0/1 pods
  deployment #1 deployed 21 minutes ago - 0/1 pods

View details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.
[student@workstation ~]$ oc get pods
NAME                    READY   STATUS             RESTARTS   AGE
psql-69676884f8-fpl98   0/1     ImagePullBackOff   0          17m
psql-7b586df5d-w2cm4    0/1     Running            0          17s
[student@workstation ~]$ oc get pods
NAME                   READY   STATUS    RESTARTS   AGE
psql-7b586df5d-w2cm4   1/1     Running   0          33s
[student@workstation ~]$ lab execute-troubleshoot finish

Completing Guided Exercise: Troubleshooting OpenShift Clusters and Applications

 · Delete OpenShift project 'execute-troubleshoot'.............  SUCCESS
 · Wait for project 'execute-troubleshoot' to be gone..........  SUCCESS
 · Remove exercise files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

-------------------------------------------------------------------------------------------------------------
Guided Exercise:
Introducing OpenShift Dynamic Storage Page: 47
--------------------------------------------------------------------------------------------------------------
In this exercise, you will deploy a PostgreSQL database using a persistent volume claim and
identify its dynamically allocated volume.
Outcomes
You should be able to:
• Identify the default storage settings of an OpenShift cluster.
• Create persistent volume claims.
• Manage persistent volumes.

[student@workstation ~]$ lab install-storage start

Checking prerequisites for Guided Exercise: Introducing OpenShift Dynamic Storage

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'install-storage' project is absent.....................  SUCCESS

Setting up the classroom for Guided Exercise: Introducing OpenShift Dynamic Storage

 · Download exercise files.....................................  SUCCESS
 · Persistent volume claim 'postgresql-storage' is not present.  SUCCESS
 · Application 'postgres-persistent' is not present............  SUCCESS
 · Application 'postgres-persistent2' is not present...........  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc new-project install-storage
Now using project "install-storage" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc get storageclass
NAME                    PROVISIONER               RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage (default)   nfs-storage-provisioner   Delete          Immediate           false                  366d
[student@workstation ~]$ oc new-app --name postgresql-persistent --docker-image registry.redhat.io/rhel8/postgresql-12:1-43 -e POSTGRESQL_USER=redhat -e POSTGRESQL_PASSWORD=redhat123 -e POSTGRESQL_DATABASE=persistentdb
--> Found container image 667898a (11 months old) from registry.redhat.io for "registry.redhat.io/rhel8/postgresql-12:1-43"

    PostgreSQL 12 
    ------------- 
    PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.

    Tags: database, postgresql, postgresql12, postgresql-12

    * An image stream tag will be created as "postgresql-persistent:1-43" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "postgresql-persistent" created
    deployment.apps "postgresql-persistent" created
    service "postgresql-persistent" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/postgresql-persistent' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc set volumes deployment/postgresql-persistent --add --name postgresql-storage --type pvc --claim-class nfs-storage --claim-mode rwo --claim-size 10Gi --mount-path /var/lib/pgsql --claim-name postgresql-storage
deployment.apps/postgresql-persistent volume updated
[student@workstation ~]$ oc status
In project install-storage on server https://api.ocp4.example.com:6443

svc/postgresql-persistent - 172.30.91.12:5432
  deployment/postgresql-persistent deploys istag/postgresql-persistent:1-43 
    deployment #3 running for 6 seconds - 0/1 pods
    deployment #2 deployed 2 minutes ago - 1 pod
    deployment #1 deployed 2 minutes ago


1 info identified, use 'oc status --suggest' to see details.
[student@workstation ~]$ oc get pvc
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
postgresql-storage   Bound    pvc-5142b9b6-cb25-4af3-b0f1-52c6dd5270c1   10Gi       RWO            nfs-storage    38s
[student@workstation ~]$ oc get pv -o custom-columns=NAME:.metadata.name,CLAIM:.spec.claimRef.name
NAME                                       CLAIM
pvc-26cc804a-4ec2-4f52-b6e5-84404b4b9def   image-registry-storage
pvc-5142b9b6-cb25-4af3-b0f1-52c6dd5270c1   postgresql-storage
[student@workstation ~]$ cd ~/DO280/labs/install-storage
[student@workstation install-storage]$ ./init_data.sh
Populating characters table
CREATE TABLE
INSERT 0 5
[student@workstation install-storage]$ ./check_data.sh
Checking characters table
 id |          name           |           nationality            
----+-------------------------+----------------------------------
  1 | Wolfgang Amadeus Mozart | Prince-Archbishopric of Salzburg
  2 | Ludwig van Beethoven    | Bonn, Germany
  3 | Johann Sebastian Bach   | Eisenach, Germany
  4 | José Pablo Moncayo     | Guadalajara, México
  5 | Niccolò Paganini       | Genoa, Italy
(5 rows)

[student@workstation install-storage]$ oc delete all -l app=postgresql-persistent
service "postgresql-persistent" deleted
deployment.apps "postgresql-persistent" deleted
imagestream.image.openshift.io "postgresql-persistent" deleted
[student@workstation install-storage]$ oc new-app --name postgresql-persistent2 --docker-image registry.redhat.io/rhel8/postgresql-12:1-43 -e POSTGRESQL_USER=redhat -e POSTGRESQL_PASSWORD=redhat123 -e POSTGRESQL_DATABASE=persistentdb
--> Found container image 667898a (11 months old) from registry.redhat.io for "registry.redhat.io/rhel8/postgresql-12:1-43"

    PostgreSQL 12 
    ------------- 
    PostgreSQL is an advanced Object-Relational database management system (DBMS). The image contains the client and server programs that you'll need to create, run, maintain and access a PostgreSQL DBMS server.

    Tags: database, postgresql, postgresql12, postgresql-12

    * An image stream tag will be created as "postgresql-persistent2:1-43" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "postgresql-persistent2" created
    deployment.apps "postgresql-persistent2" created
    service "postgresql-persistent2" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/postgresql-persistent2' 
    Run 'oc status' to view your app.
[student@workstation install-storage]$ ./check_data.sh
Checking characters table
ERROR: 'characters' table does not exist
[student@workstation install-storage]$ oc set volumes deployment/postgresql-persistent2 --add --name postgresql-storage --type pvc --claim-name postgresql-storage --mount-path /var/lib/pgsql
deployment.apps/postgresql-persistent2 volume updated
[student@workstation install-storage]$ ./check_data.sh
Checking characters table
 id |          name           |           nationality            
----+-------------------------+----------------------------------
  1 | Wolfgang Amadeus Mozart | Prince-Archbishopric of Salzburg
  2 | Ludwig van Beethoven    | Bonn, Germany
  3 | Johann Sebastian Bach   | Eisenach, Germany
  4 | José Pablo Moncayo     | Guadalajara, México
  5 | Niccolò Paganini       | Genoa, Italy
(5 rows)

[student@workstation install-storage]$ oc delete all -l app=postgresql-persistent2
service "postgresql-persistent2" deleted
deployment.apps "postgresql-persistent2" deleted
imagestream.image.openshift.io "postgresql-persistent2" deleted
[student@workstation install-storage]$ oc delete pvc/postgresql-storage
persistentvolumeclaim "postgresql-storage" deleted
[student@workstation install-storage]$ cd
[student@workstation ~]$ lab install-storage finish

Completing Guided Exercise: Introducing OpenShift Dynamic Storage

 · Delete OpenShift project 'install-storage'..................  SUCCESS
 · Wait for project 'install-storage' to be gone...............  SUCCESS
 · Remove exercise files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

Summary
In this chapter, you learned:
• Red Hat OpenShift Container Platform provides two main installation methods: full-stack
automation, and pre-existing infrastructure.
• Future releases are expected to add more cloud and virtualization providers, such as VMware,
Red Hat Virtualization, and IBM System Z.
• An OpenShift node based on Red Hat Enterprise Linux CoreOS runs very few local services that
would require direct access to a node to inspect their status. Most of the system services run as
containers, the main exceptions are the CRI-O container engine and the Kubelet.
• The oc get node, oc adm top, oc adm node-logs, and oc adm debug commands
provide troubleshooting information about OpenShift nodes.


Therory
---------
Managing RBAC Using the CLI
Cluster administrators can use the oc adm policy command to both add and remove cluster
roles and namespace roles.

To add a cluster role to a user, use the add-cluster-role-to-user subcommand:
[user@demo ~]$ oc adm policy add-cluster-role-to-user cluster-role username

For example, to change a regular user to a cluster administrator, use the following command:
[user@demo ~]$ oc adm policy add-cluster-role-to-user cluster-admin username

To remove a cluster role from a user, use the remove-cluster-role-from-user
subcommand:
[user@demo ~]$ oc adm policy remove-cluster-role-from-user cluster-role username

For example, to change a cluster administrator to a regular user, use the following command:
[user@demo ~]$ oc adm policy remove-cluster-role-from-user cluster-admin username

Rules are defined by an action and a resource. For example, the create user rule is part of the
cluster-admin role.
You can use the oc adm policy who-can command to determine if a user can execute an
action on a resource. For example:
[user@demo ~]$ oc adm policy who-can delete user

Default roles                                       Description
admin 				       Users with this role can manage all project resources, including granting
				             access to other users to access the project.
                 
basic-user			    Users with this role have read access to the project.

cluster-admin 			Users with this role have superuser access to the cluster resources. These
				             users can perform any action on the cluster, and have full control of all
				              projects.
cluster-status 			Users with this role can get cluster status information.

edit Users			 with this role can create, change, and delete common application
				          resources from the project, such as services and deployment
				          configurations. These users cannot act on management resources such
				          as limit ranges and quotas, and cannot manage access permissions to the
			          	project.
self-provisioner 		Users with this role can create new projects. This is a cluster role, not a
				              project role.
view 				Users with this role can view project resources, but cannot modify project
				resources.

User Types
Interaction with OpenShift Container Platform is associated with a user. An OpenShift Container
Platform user object represents a user who can be granted permissions in the system by adding
roles to that user or to a user's group via rolebindings.

Regular users
This is the way most interactive OpenShift Container Platform users are represented. Regular
users are represented with the User object. This type of user represents a person that has
been allowed access to the platform. Examples of regular users include user1 and user2.

System users
Many of these are created automatically when the infrastructure is defined, mainly for
the purpose of enabling the infrastructure to securely interact with the API. System users
include a cluster administrator (with access to everything), a per-node user, users for
routers and registries to use, and various others. An anonymous system user is used by
default for unauthenticated requests. Examples of system users include: system:admin,
system:openshift-registry, and system:node:node1.example.com.

Service accounts
These are special system users associated with projects; some are created automatically
when the project is first created, and project administrators can create more for the
purpose of defining access to the contents of each project. Service accounts are
often used to give extra privileges to pods or deployment configurations. Service
accounts are represented with the ServiceAccount object. Examples of service
account users include system:serviceaccount:default:deployer and
system:serviceaccount:foo:builder.

------------------------------------------------------------------------------------------------------------
Chapter 3 | Configuring Authentication and Authorization
Guided Exercise
Configuring Identity Providers Page: 61
------------------------------------------------------------------------------------------------------------
In this exercise, you will configure the HTPasswd identity provider and create users for cluster
administrators.
Outcomes
You should be able to:
• Create users and passwords for HTPasswd authentication.
• Configure the Identity Provider for HTPasswd authentication.
• Assign cluster administration rights to users.


[student@workstation ~]$ lab auth-provider start

Checking prerequisites for Guided Exercise: Configuring Identity Providers

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'auth-provider' project is absent.......................  SUCCESS

Setting up the classroom for Guided Exercise: Configuring Identity Providers

 Preparing the student's workstation:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS
 Restoring authentication settings to installation defaults:
 · No need to perform any change...............................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ htpasswd -c -B -b ~/DO280/labs/auth-provider/htpasswd admin redhat
Adding password for user admin
[student@workstation ~]$ htpasswd -b ~/DO280/labs/auth-provider/htpasswd developer developer
Adding password for user developer
[student@workstation ~]$ cat ~/DO280/labs/auth-provider/htpasswd
admin:$2y$05$a2aKYuv8R96uoQ2xxxTEIeGnEI8xZasIQ5/FGYHO7MXpxzU4MT3He
developer:$apr1$nPaPLoFJ$GYsV0obfH5cznZj0erWtW0
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc create secret generic localusers --from-file htpasswd=/home/student/DO280/labs/auth-provider/htpasswd -n openshift-config
secret/localusers created
[student@workstation ~]$ oc adm policy add-cluster-role-to-user cluster-admin admin
Warning: User 'admin' not found
clusterrole.rbac.authorization.k8s.io/cluster-admin added: "admin"
[student@workstation ~]$ oc get oauth cluster -o yaml > ~/DO280/labs/auth-provider/oauth.yaml
[student@workstation ~]$ vi !$
vi ~/DO280/labs/auth-provider/oauth.yaml
[student@workstation ~]$ vi ~/DO280/labs/auth-provider/oauth.yaml
[student@workstation ~]$ cat !$
cat ~/DO280/labs/auth-provider/oauth.yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  annotations:
    release.openshift.io/create-only: "true"
  creationTimestamp: "2020-08-05T18:23:41Z"
  generation: 1
  managedFields:
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:release.openshift.io/create-only: {}
      f:spec: {}
    manager: cluster-version-operator
    operation: Update
    time: "2020-08-05T18:23:41Z"
  name: cluster
  resourceVersion: "2121"
  selfLink: /apis/config.openshift.io/v1/oauths/cluster
  uid: c96a18f6-b8de-4722-b5bf-ca6c39cdb78b
spec:
  identityProviders:
  - htpasswd:
      fileData:    
        name: localusers
    mappingMethod: claim
    name: myusers
    type: HTPasswd
[student@workstation ~]$ oc replace -f ~/DO280/labs/auth-provider/oauth.yaml
oauth.config.openshift.io/cluster replaced
[student@workstation ~]$ oc login -u admin -p redhat
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get nodes
NAME       STATUS   ROLES           AGE    VERSION
master01   Ready    master,worker   366d   v1.18.3+012b3ec
master02   Ready    master,worker   366d   v1.18.3+012b3ec
master03   Ready    master,worker   366d   v1.18.3+012b3ec
[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc get nodes
Error from server (Forbidden): nodes is forbidden: User "developer" cannot list resource "nodes" in API group "" at the cluster scope
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get users
NAME        UID                                    FULL NAME   IDENTITIES
admin       17914b9a-4a74-4e68-989d-2a988c289136               myusers:admin
developer   ebb1a8e6-3dba-41b2-b0fc-f8a65b0846d7               myusers:developer
[student@workstation ~]$ oc get identity
NAME                IDP NAME   IDP USER NAME   USER NAME   USER UID
myusers:admin       myusers    admin           admin       17914b9a-4a74-4e68-989d-2a988c289136
myusers:developer   myusers    developer       developer   ebb1a8e6-3dba-41b2-b0fc-f8a65b0846d7
[student@workstation ~]$ oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-provider/ --confirm
/home/student/DO280/labs/auth-provider/htpasswd
[student@workstation ~]$ htpasswd -b ~/DO280/labs/auth-provider/htpasswd manager redhat
Adding password for user manager
[student@workstation ~]$ cat ~/DO280/labs/auth-provider/htpasswd
admin:$2y$05$a2aKYuv8R96uoQ2xxxTEIeGnEI8xZasIQ5/FGYHO7MXpxzU4MT3He
developer:$apr1$nPaPLoFJ$GYsV0obfH5cznZj0erWtW0
manager:$apr1$VCHd.PG9$QUM3QFOAmk6b.CQGhc/NX1
[student@workstation ~]$ oc set data secret/localusers --from-file htpasswd=/home/student/DO280/labs/auth-provider/htpasswd -n openshift-config
secret/localusers data updated
[student@workstation ~]$ oc login -u manager -p redhat
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ oc new-project auth-provider
Now using project "auth-provider" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc delete project auth-provider
Error from server (Forbidden): projects.project.openshift.io "auth-provider" is forbidden: User "developer" cannot delete resource "projects" in API group "project.openshift.io" in the namespace "auth-provider"
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-provider/ --confirm
/home/student/DO280/labs/auth-provider/htpasswd
[student@workstation ~]$ MANAGER_PASSWD="$(openssl rand -hex 15)"
[student@workstation ~]$ htpasswd -b ~/DO280/labs/auth-provider/htpasswd manager ${MANAGER_PASSWD}
Updating password for user manager
[student@workstation ~]$ oc set data secret/localusers --from-file htpasswd=/home/student/DO280/labs/auth-provider/htpasswd -n openshift-config
secret/localusers data updated
[student@workstation ~]$ oc login -u manager -p ${MANAGER_PASSWD}
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ oc login -u manager -p ${MANAGER_PASSWD}
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc extract secret/localusers -n openshift-config --to ~/DO280/labs/auth-provider/ --confirm
/home/student/DO280/labs/auth-provider/htpasswd
[student@workstation ~]$ htpasswd -D ~/DO280/labs/auth-provider/htpasswd manager
Deleting password for user manager
[student@workstation ~]$ oc set data secret/localusers --from-file htpasswd=/home/student/DO280/labs/auth-provider/htpasswd -n openshift-config
secret/localusers data updated
[student@workstation ~]$ oc delete identity "myusers:manager"
identity.user.openshift.io "myusers:manager" deleted
[student@workstation ~]$ oc delete user manager
user.user.openshift.io "manager" deleted
[student@workstation ~]$ oc login -u manager -p ${MANAGER_PASSWD}
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ oc get users
NAME        UID                                    FULL NAME   IDENTITIES
admin       17914b9a-4a74-4e68-989d-2a988c289136               myusers:admin
developer   ebb1a8e6-3dba-41b2-b0fc-f8a65b0846d7               myusers:developer
[student@workstation ~]$ oc get identity
NAME                IDP NAME   IDP USER NAME   USER NAME   USER UID
myusers:admin       myusers    admin           admin       17914b9a-4a74-4e68-989d-2a988c289136
myusers:developer   myusers    developer       developer   ebb1a8e6-3dba-41b2-b0fc-f8a65b0846d7
[student@workstation ~]$ oc extract secret/localusers -n openshift-config --to -
# htpasswd
admin:$2y$05$a2aKYuv8R96uoQ2xxxTEIeGnEI8xZasIQ5/FGYHO7MXpxzU4MT3He
developer:$apr1$nPaPLoFJ$GYsV0obfH5cznZj0erWtW0
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD}
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc delete project auth-provider
project.project.openshift.io "auth-provider" deleted
[student@workstation ~]$ oc edit oauth
oauth.config.openshift.io/cluster edited
[student@workstation ~]$ oc delete secret localusers -n openshift-config
secret "localusers" deleted
[student@workstation ~]$ oc delete user --all
user.user.openshift.io "admin" deleted
user.user.openshift.io "developer" deleted
[student@workstation ~]$ oc delete identity --all
identity.user.openshift.io "myusers:admin" deleted
identity.user.openshift.io "myusers:developer" deleted
[student@workstation ~]$ lab auth-provider finish

Completing Guided Exercise: Configuring Identity Providers

 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

--------------------------------------------------------------------------------------------------------------------
Guided Exercise
Defining and Applying Permissions using RBAC page: 74

-------------------------------------------------------------------------------------------------------------------
In this exercise, you will define role-based access controls and apply permissions to users.
Outcomes
You should be able to:
• Remove project creation privileges from users who are not OpenShift cluster
administrators.
• Create OpenShift groups and add members to these groups.
• Create a project and assign project administration privileges to the project.
• As a project administrator, assign read and write privileges to different groups of users.

[student@workstation ~]$ lab auth-rbac start

Checking prerequisites for Guided Exercise: Defining and Applying permissions using RBAC

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'auth-rbac' project is absent...........................  SUCCESS

Setting up the classroom for Guided Exercise: Defining and Applying permissions using RBAC

 · Create HTPasswd entry for 'admin'...........................  SUCCESS
 · Create HTPasswd entry for 'leader'..........................  SUCCESS
 · Create HTPasswd entry for 'developer'.......................  SUCCESS
 · Create HTPasswd entry for 'qa-engineer'.....................  SUCCESS
 · Create HTPasswd secret: 'localusers'........................  SUCCESS
 · Add HTPasswd IdP............................................  SUCCESS
 · Pause for creation of first oauth pod.......................  SUCCESS
 · Pause for creation of second oauth pod......................  SUCCESS
 · Wait up to 1 minute for 'pod/oauth-openshift-869cff5b95-srnd
   k'..........................................................  SUCCESS
 · Wait up to 1 minute for oauth pod containers to be ready....  SUCCESS
 · Delete all previous users...................................  SUCCESS
 · Delete all previous identities..............................  SUCCESS
 · Pause 5 seconds before validating authentication............  SUCCESS
 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS
 · Validate 'qa-engineer' can log in with password 'redhat'....  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login failed (401 Unauthorized)
Verify you have provided correct credentials.
[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get clusterrolebinding -o wide | grep -E 'NAME|self-provisioner'
NAME                                                                             ROLE                                                                               AGE     USERS                                   GROUPS                                         SERVICEACCOUNTS
self-provisioners                                                                ClusterRole/self-provisioner                                                       366d                                            system:authenticated:oauth                     
[student@workstation ~]$ oc describe clusterrolebindings self-provisioners
Name:         self-provisioners
Labels:       <none>
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  self-provisioner
Subjects:
  Kind   Name                        Namespace
  ----   ----                        ---------
  Group  system:authenticated:oauth  
[student@workstation ~]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
Warning: Your changes may get lost whenever a master is restarted, unless you prevent reconciliation of this rolebinding using the following command: oc annotate clusterrolebinding.rbac self-provisioners 'rbac.authorization.kubernetes.io/autoupdate=false' --overwrite
clusterrole.rbac.authorization.k8s.io/self-provisioner removed: "system:authenticated:oauth"
[student@workstation ~]$ oc describe clusterrolebindings self-provisioners
Error from server (NotFound): clusterrolebindings.rbac.authorization.k8s.io "self-provisioners" not found
[student@workstation ~]$ oc get clusterrolebinding -o wide | grep -E 'NAME|self-provisioner'
NAME                                                                             ROLE                                                                               AGE    USERS                                   GROUPS                                         SERVICEACCOUNTS
[student@workstation ~]$ oc login -u leader -p redhat
Login successful.

You don't have any projects. Contact your system administrator to request a project.
[student@workstation ~]$ oc new-project test
Error from server (Forbidden): You may not request a new project via this API.
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc new-project auth-rbac
Now using project "auth-rbac" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc policy add-role-to-user admin leader
clusterrole.rbac.authorization.k8s.io/admin added: "leader"
[student@workstation ~]$ oc adm groups new dev-group
group.user.openshift.io/dev-group created
[student@workstation ~]$ oc adm groups new dev-group
Error from server (AlreadyExists): groups.user.openshift.io "dev-group" already exists
[student@workstation ~]$ oc adm groups add-users dev-group developer
group.user.openshift.io/dev-group added: "developer"
[student@workstation ~]$ oc adm groups new qa-group
group.user.openshift.io/qa-group created
[student@workstation ~]$ oc adm groups add-users qa-group qa-engineer
group.user.openshift.io/qa-group added: "qa-engineer"
[student@workstation ~]$ oc get groups
NAME        USERS
dev-group   developer
qa-group    qa-engineer
[student@workstation ~]$ oc login -u leader -p redhat
Login successful.

You have one project on this server: "auth-rbac"

Using project "auth-rbac".
[student@workstation ~]$ oc policy add-role-to-group edit dev-group
clusterrole.rbac.authorization.k8s.io/edit added: "dev-group"
[student@workstation ~]$ oc policy add-role-to-group view qa-group
clusterrole.rbac.authorization.k8s.io/view added: "qa-group"
[student@workstation ~]$ oc get rolebindings -o wide
NAME                    ROLE                               AGE     USERS    GROUPS                             SERVICEACCOUNTS
admin                   ClusterRole/admin                  4m9s    admin                                       
admin-0                 ClusterRole/admin                  3m55s   leader                                      
edit                    ClusterRole/edit                   54s              dev-group                          
system:deployers        ClusterRole/system:deployer        4m9s                                                auth-rbac/deployer
system:image-builders   ClusterRole/system:image-builder   4m9s                                                auth-rbac/builder
system:image-pullers    ClusterRole/system:image-puller    4m9s             system:serviceaccounts:auth-rbac   
view                    ClusterRole/view                   41s              qa-group                           
[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You have one project on this server: "auth-rbac"

Using project "auth-rbac".
[student@workstation ~]$ oc new-app --name httpd httpd:2.4
--> Found image d9163e7 (12 months old) in image stream "openshift/httpd" under tag "2.4" for "httpd:2.4"

    Apache httpd 2.4 
    ---------------- 
    Apache httpd 2.4 available as container, is a powerful, efficient, and extensible web server. Apache supports a variety of features, many implemented as compiled modules which extend the core functionality. These can range from server-side programming language support to authentication schemes. Virtual hosting allows one Apache installation to serve many different Web sites.

    Tags: builder, httpd, httpd24


--> Creating resources ...
    imagestreamtag.image.openshift.io "httpd:2.4" created
    deployment.apps "httpd" created
    service "httpd" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/httpd' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc policy add-role-to-user edit qa-engineer
Error from server (Forbidden): rolebindings.rbac.authorization.k8s.io is forbidden: User "developer" cannot list resource "rolebindings" in API group "rbac.authorization.k8s.io" in the namespace "auth-rbac"
[student@workstation ~]$ oc login -u qa-engineer -p redhat
Login successful.

You have one project on this server: "auth-rbac"

Using project "auth-rbac".
[student@workstation ~]$ oc scale deployment httpd --replicas 3
Error from server (Forbidden): deployments.apps "httpd" is forbidden: User "qa-engineer" cannot patch resource "deployments/scale" in API group "apps" in the namespace "auth-rbac"
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "auth-rbac".
[student@workstation ~]$ oc adm policy add-cluster-role-to-group --rolebinding-name self-provisioners self-provisioner system:authenticated:oauth
Warning: Group 'system:authenticated:oauth' not found
clusterrole.rbac.authorization.k8s.io/self-provisioner added: "system:authenticated:oauth"
[student@workstation ~]$ lab auth-rbac finish

Completing Guided Exercise: Defining and Applying permissions using RBAC

 · Delete OpenShift project 'auth-rbac'........................  SUCCESS
 · Wait for project 'auth-rbac' to be gone.....................  SUCCESS
 · Remove group 'dev-group'....................................  SUCCESS
 · Remove group 'qa-group'.....................................  SUCCESS
 · Delete HTPasswd entry for 'qa-engineer'.....................  SUCCESS
 · Update the 'localusers' secret data.........................  SUCCESS
 · Remove user 'qa-engineer'...................................  SUCCESS
 · Remove identity 'localusers:qa-engineer'....................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

---------------------------------------------------------------------------------------------------------------------
LAB
Configuring Authentication and Authorization Page 81
-----------------------------------------------------------------------------------------------------------------------
Outcomes
You should be able to:
• Create users and passwords for HTPasswd authentication.
• Configure the Identity Provider for HTPasswd authentication.
• Assign cluster administration rights to users.
• Remove the ability to create projects at the cluster level.
• Create groups and add users to groups.
• Manage user privileges in projects by granting privileges to groups.
[student@workstation ~]$ lab auth-review start

Checking prerequisites for Lab: Configuring Authentication and Authorization

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'auth-review' project is absent.........................  SUCCESS

Setting up the classroom for Lab: Configuring Authentication and Authorization

 Preparing the student's workstation:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS
 Restoring authentication settings to installation defaults:
 · No need to perform any change...............................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ htpasswd -D ~/DO280/labs/auth-review/tmp_users analyst
Deleting password for user analyst
[student@workstation ~]$ for NAME in tester leader admin developer; do htpasswd -b ~/DO280/labs/auth-review/tmp_users ${NAME} 'L@bR3v!ew';done
Updating password for user tester
Updating password for user leader
Adding password for user admin
Adding password for user developer
[student@workstation ~]$ cat ~/DO280/labs/auth-review/tmp_users
tester:$apr1$Q6ws3BHj$Wwy76CGsevo.DRzo0qvDX0
leader:$apr1$r355xPDT$X3LledBcIheIIcp4PWr21/
admin:$apr1$D/68bwT1$aFhSdvDUWqsU9kXBXMDZE/
developer:$apr1$uBVFBpcF$J9S.lWUs6l0Q0aAclGTq0/
[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc create secret generic auth-review --from-file htpasswd=/home/student/DO280/labs/auth-review/tmp_users -n openshift-config
secret/auth-review created
[student@workstation ~]$ oc get oauth cluster -o yaml > ~/DO280/labs/auth-review/oauth.yaml
[student@workstation ~]$ vi !$
vi ~/DO280/labs/auth-review/oauth.yaml
[student@workstation ~]$ cat !$
cat ~/DO280/labs/auth-review/oauth.yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  creationTimestamp: "2020-08-05T18:23:41Z"
  generation: 8
  managedFields:
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec: {}
    manager: cluster-version-operator
    operation: Update
    time: "2020-08-05T18:23:41Z"
  name: cluster
  resourceVersion: "106407"
  selfLink: /apis/config.openshift.io/v1/oauths/cluster
  uid: c96a18f6-b8de-4722-b5bf-ca6c39cdb78b
spec: 
  identityProviders:
  - htpasswd:
      fileData:
        name: auth-review
    mappingMethod: claim
    name: htpasswd
    type: HTPasswd
[student@workstation ~]$ oc replace -f ~/DO280/labs/auth-review/oauth.yaml
oauth.config.openshift.io/cluster replaced
[student@workstation ~]$ oc get pods -n openshift-authentication
NAME                               READY   STATUS    RESTARTS   AGE
oauth-openshift-57d5b9656b-qjqkb   1/1     Running   0          8m17s
oauth-openshift-57d5b9656b-tr7tx   1/1     Running   0          8m27s
oauth-openshift-5db697b59f-wcsds   0/1     Running   0          9s
[student@workstation ~]$ oc get pods -n openshift-authentication
NAME                               READY   STATUS        RESTARTS   AGE
oauth-openshift-57d5b9656b-qjqkb   1/1     Terminating   0          8m26s
oauth-openshift-57d5b9656b-tr7tx   1/1     Running       0          8m36s
oauth-openshift-5db697b59f-psd8c   0/1     Running       0          7s
oauth-openshift-5db697b59f-wcsds   1/1     Running       0          18s
[student@workstation ~]$ oc adm policy add-cluster-role-to-user cluster-admin admin
Warning: User 'admin' not found
clusterrole.rbac.authorization.k8s.io/cluster-admin added: "admin"
[student@workstation ~]$ oc login -u admin -p 'L@bR3v!ew'
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc get nodes
NAME       STATUS   ROLES           AGE    VERSION
master01   Ready    master,worker   367d   v1.18.3+012b3ec
master02   Ready    master,worker   367d   v1.18.3+012b3ec
master03   Ready    master,worker   367d   v1.18.3+012b3ec
[student@workstation ~]$ oc login -u developer -p 'L@bR3v!ew'
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc get nodes
Error from server (Forbidden): nodes is forbidden: User "developer" cannot list resource "nodes" in API group "" at the cluster scope
[student@workstation ~]$ oc login -u admin -p 'L@bR3v!ew'
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
Warning: Your changes may get lost whenever a master is restarted, unless you prevent reconciliation of this rolebinding using the following command: oc annotate clusterrolebinding.rbac self-provisioners 'rbac.authorization.kubernetes.io/autoupdate=false' --overwrite
clusterrole.rbac.authorization.k8s.io/self-provisioner removed: "system:authenticated:oauth"
[student@workstation ~]$ oc adm groups new managers
group.user.openshift.io/managers created
[student@workstation ~]$ oc adm groups add-users managers leader
group.user.openshift.io/managers added: "leader"
[student@workstation ~]$ oc adm policy add-cluster-role-to-group self-provisioner managers
clusterrole.rbac.authorization.k8s.io/self-provisioner added: "managers"
[student@workstation ~]$ oc login -u leader -p 'L@bR3v!ew'
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc new-project auth-review
Now using project "auth-review" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc login -u admin -p 'L@bR3v!ew'
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "auth-review".
[student@workstation ~]$ oc adm groups new developers
group.user.openshift.io/developers created
[student@workstation ~]$ oc adm groups add-users developers developer
group.user.openshift.io/developers added: "developer"
[student@workstation ~]$ oc policy add-role-to-group edit developers
clusterrole.rbac.authorization.k8s.io/edit added: "developers"
[student@workstation ~]$ oc adm groups new qa
group.user.openshift.io/qa created
[student@workstation ~]$ oc adm groups add-users qa tester
group.user.openshift.io/qa added: "tester"
[student@workstation ~]$ oc policy add-role-to-group view qa
clusterrole.rbac.authorization.k8s.io/view added: "qa"
[student@workstation ~]$ lab auth-review grade

Grading the student's work for Lab: Configuring Authentication and Authorization

 · Cluster uses the HTPasswd identity provider.................  PASS
 · User 'analyst' does not exist in the HTPasswd secret........  PASS
 · The 'admin' user can log in with a password of 'L@bR3v!ew'..  PASS
 · The 'admin' user has the 'cluster-admin' cluster role.......  PASS
 · The 'self-provisioner' cluster role has been removed from th
   e 'system:authenticated:oauth' group........................  PASS
 · The 'managers' group exists.................................  PASS
 · The 'managers' group has the 'self-provisioner' cluster role
   ............................................................  PASS
 · The 'managers' group contains the 'leader' user.............  PASS
 · The 'leader' user can log in with a password of 'L@bR3v!ew'.  PASS
 · The 'auth-review' project exists............................  PASS
 · The 'leader' user has the 'admin' role on the 'auth-review' 
   project.....................................................  PASS
 · The 'developers' group exists...............................  PASS
 · The 'developers' group has the 'edit' role on the 'auth-revi
   ew' project.................................................  PASS
 · The 'dvelopers' group contains the 'developer' user.........  PASS
 · The 'developer' user can log in with a password of 'L@bR3v!e
   w'..........................................................  PASS
 · The 'qa' group exists.......................................  PASS
 · The 'qa' group has the 'view' role on the 'auth-review' proj
   ect.........................................................  PASS
 · The 'qa' group contains the 'tester' user...................  PASS
 · The 'tester' user can log in with a password of 'L@bR3v!ew'.  PASS

Overall exercise grade.........................................  PASS

[student@workstation ~]$ lab auth-review finish

Completing Lab: Configuring Authentication and Authorization

 · Delete OpenShift project 'auth-review'......................  SUCCESS
 · Wait for project 'auth-review' to be gone...................  SUCCESS
 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS
 · Restore project creation privileges.........................  SUCCESS
 · Removing the cluster role binding 'clusterrolebinding.rbac.a
   uthorization.k8s.io/self-provisioner'.......................  SUCCESS
 Restoring authentication settings to installation defaults:
 · Removing 'cluster-admin' role from the 'admin' user.........  SUCCESS
 · Remove HTPasswd secret: 'auth-review'.......................  SUCCESS
 · Remove all configured Identity Providers....................  SUCCESS
 · Remove all existing users...................................  SUCCESS
 · Remove all existing groups..................................  SUCCESS
 · Remove all existing identities..............................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

Summary
-------
In this chapter, you learned:
• A newly-installed OpenShift cluster provides two authentication methods that grant
administrative access: the kubeconfig file and the kubeadmin virtual user.
• The HTPasswd identity provider authenticates users against credentials stored in a secret. The
name of the secret, and other settings for the identity provider, are stored inside the OAuth
custom resource.
• To manage user credentials using the HTPasswd identity provider, you must extract data from
the secret, change that data using the htpasswd command, and then apply the data back to
the secret.
• Creating OpenShift users requires valid credentials, managed by an identity provider, plus user
and identity resources.
• Deleting OpenShift users requires deleting their credentials from the identity provider, and also
deleting their user and identity resources.
• OpenShift uses role-based access control (RBAC) to control user actions. A role is a collection
of rules that govern interaction with OpenShift resources. Default roles exist for cluster
administrators, developers, and auditors.
• To control user interaction, assign a user to one or more roles. A role binding contains all of the
associations of a role to users and groups.
• To grant a user cluster administrator privileges, assign the cluster-admin role to that user.

-------------------------------------------------------------------------------------------------------
Chapter 4 | Configuring Application Security
Guided Exercise : Managing Sensitive Information With Secrets Page: 95
--------------------------------------------------------------------------------------------------------
In this exercise, you will manage information using secrets.
Outcomes
You should be able to:
• Manage secrets and use them to initialize environment variables in applications.
• Use secrets for a MySQL database application.
• Assign secrets to an application that connects to a MySQL database.

[student@workstation ~]$ lab authorization-secrets start

Checking prerequisites for Guided Exercise: Managing Sensitive Information With Secrets

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'authorization-secrets' project is absent...............  SUCCESS

Setting up the classroom for Guided Exercise: Managing Sensitive Information With Secrets

 · Create HTPasswd entry for 'admin'...........................  SUCCESS
 · Create HTPasswd entry for 'leader'..........................  SUCCESS
 · Create HTPasswd entry for 'developer'.......................  SUCCESS
 · Create HTPasswd secret: 'localusers'........................  SUCCESS
 · Add HTPasswd IdP............................................  SUCCESS
 · Assigning the 'cluster-admin' role to the 'admin' user......  SUCCESS
 · Pause for creation of first oauth pod.......................  SUCCESS
 · Wait up to 1 minute for 'pod/oauth-openshift-5dcfcc96d4-5j97
   v'..........................................................  SUCCESS
 · Wait up to 1 minute for oauth pod containers to be ready....  SUCCESS
 · Pause for creation of second oauth pod......................  SUCCESS
 · Wait up to 1 minute for 'pod/oauth-openshift-5dcfcc96d4-cvp8
   l'..........................................................  SUCCESS
 · Wait up to 1 minute for oauth pod containers to be ready....  SUCCESS
 · Delete all previous users...................................  SUCCESS
 · Delete all previous identities..............................  SUCCESS
 · Pause 5 seconds before validating authentication............  SUCCESS
 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u kubeadmin -p ${RHT_OCP4_KUBEADM_PASSWD} https://api.ocp4.example.com:6443
Login successful.

You have access to 58 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
[student@workstation ~]$ oc new-project authorization-secrets
Now using project "authorization-secrets" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc create secret generic mysql --from-literal user=myuser --from-literal password=redhat123 --from-literal database=test_secrets --from-literal hostname=mysql
secret/mysql created
[student@workstation ~]$ oc new-app --name mysql --docker-image registry.access.redhat.com/rhscl/mysql-57-rhel7:5.7-47
--> Found container image 77d20f2 (2 years old) from registry.access.redhat.com for "registry.access.redhat.com/rhscl/mysql-57-rhel7:5.7-47"

    MySQL 5.7 
    --------- 
    MySQL is a multi-user, multi-threaded SQL database server. The container image provides a containerized packaging of the MySQL mysqld daemon and client application. The mysqld server daemon accepts connections from clients and provides access to content from MySQL databases on behalf of the clients.

    Tags: database, mysql, mysql57, rh-mysql57

    * An image stream tag will be created as "mysql:5.7-47" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "mysql" created
    deployment.apps "mysql" created
    service "mysql" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/mysql' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc get pods
NAME                     READY   STATUS             RESTARTS   AGE
mysql-786bb947f9-dqpv6   0/1     CrashLoopBackOff   1          26s
[student@workstation ~]$ oc set env deployment/mysql --from secret/mysql --prefix MYSQL_
deployment.apps/mysql updated
[student@workstation ~]$ oc set volume deployment/mysql --add --type secret --mount-path /run/secrets/mysql --secret-name mysql
info: Generated volume name: volume-kz29g
deployment.apps/mysql volume updated
[student@workstation ~]$ oc get pods
NAME                     READY   STATUS        RESTARTS   AGE
mysql-7b9f6bcd5-ct6gj    1/1     Running       0          12s
mysql-7cd7499d66-rfwlt   0/1     Terminating   0          59s
[student@workstation ~]$ oc rsh mysql-7b9f6bcd5-ct6gj
sh-4.2$ mysql -u myuser --password=redhat123 test_secrets -e 'show databases;'
mysql: [Warning] Using a password on the command line interface can be insecure.
+--------------------+
| Database           |
+--------------------+
| information_schema |
| test_secrets       |
+--------------------+
sh-4.2$ df -h | grep mysql
tmpfs                                 7.9G   16K  7.9G   1% /run/secrets/mysql
sh-4.2$ cd /run/secrets/mysql
sh-4.2$ ls
database  hostname  password  user
sh-4.2$ for FILE in $(ls /run/secrets/mysql); do echo "${FILE}: $(cat /run/secrets/mysql/${FILE})";  done
database: test_secrets
hostname: mysql
password: redhat123
user: myuser
sh-4.2$ exit
exit
[student@workstation ~]$ oc new-app --name quotes --docker-image quay.io/redhattraining/famous-quotes:2.1
--> Found container image 7ff1a7b (10 months old) from quay.io for "quay.io/redhattraining/famous-quotes:2.1"

    Quotes 2.1 
    ---------- 
    Famous Quotes is a PoC application for Go and MySQL

    Tags: poc, mysql, golang

    * An image stream tag will be created as "quotes:2.1" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "quotes" created
    deployment.apps "quotes" created
    service "quotes" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/quotes' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc get pods -l deployment=quotes -w
NAME                      READY   STATUS              RESTARTS   AGE
quotes-6b8b7d87ff-qmm4v   0/1     ContainerCreating   0          12s
quotes-6b8b7d87ff-qmm4v   0/1     Error               0          26s
quotes-6b8b7d87ff-qmm4v   0/1     Error               1          27s
quotes-6b8b7d87ff-qmm4v   0/1     CrashLoopBackOff    1          28s
^C[student@workstation ~]$ oc set env deployment/quotes --from secret/mysql --prefix QUOTES_
deployment.apps/quotes updated
[student@workstation ~]$ oc get pods -l deployment=quotes
NAME                      READY   STATUS              RESTARTS   AGE
quotes-6b8b7d87ff-qmm4v   0/1     CrashLoopBackOff    3          105s
quotes-c4bcc49f8-42kp5    0/1     ContainerCreating   0          11s
[student@workstation ~]$ oc get pods -l deployment=quotes
NAME                      READY   STATUS              RESTARTS   AGE
quotes-6b8b7d87ff-qmm4v   0/1     Error               4          119s
quotes-c4bcc49f8-42kp5    0/1     ContainerCreating   0          25s
[student@workstation ~]$ oc logs quotes-c4bcc49f8-42kp5 | head -n2
2021/08/08 02:49:08 Connecting to the database: myuser:redhat123@tcp(mysql:3306)/test_secrets
2021/08/08 02:49:08 Database connection OK
[student@workstation ~]$ oc expose service quotes --hostname quotes.apps.ocp4.example.com
route.route.openshift.io/quotes exposed
[student@workstation ~]$ oc get route quotes
NAME     HOST/PORT                      PATH   SERVICES   PORT       TERMINATION   WILDCARD
quotes   quotes.apps.ocp4.example.com          quotes     8000-tcp                 None
[student@workstation ~]$ curl -s http://quotes.apps.ocp4.example.com/env | grep QUOTES_
                <li>QUOTES_USER: myuser </li>
                <li>QUOTES_PASSWORD: redhat123 </li>
                <li>QUOTES_DATABASE: test_secrets</li>
                <li>QUOTES_HOST: mysql</li>
[student@workstation ~]$ curl -s http://quotes.apps.ocp4.example.com/status
Database connection OK
[student@workstation ~]$ curl -s http://quotes.apps.ocp4.example.com/random
3: The secret of change is to focus all your energy not on fighting the old but on building the new.
- Socrates
[student@workstation ~]$ oc delete project authorization-secrets
project.project.openshift.io "authorization-secrets" deleted
[student@workstation ~]$ lab authorization-secrets finish

Completing Guided Exercise: Managing Sensitive Information With Secrets


Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

--------------------------------------------------------------------------------------------------------------
Chapter 4 | Configuring Application Security
Guided Exercise
Controlling Application Permissions with Security Context Constraints Page 104
--------------------------------------------------------------------------------------------------------------
In this exercise, you will deploy applications that require pods with extended permissions.
Outcomes
You should be able to:
• Create service accounts and assign security context constraints (SCCs) to them.
• Assign a service account to a deployment configuration.
• Run applications that need root privileges.
[student@workstation ~]$ lab authorization-scc start

Checking prerequisites for Guided Exercise: Controlling Application Permissions with Security Context Constraints

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'authorization-scc' project is absent...................  SUCCESS

Setting up the classroom for Guided Exercise: Controlling Application Permissions with Security Context Constraints

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc new-project authorization-scc
Now using project "authorization-scc" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc new-app --name gitlab --docker-image quay.io/redhattraining/gitlab-ce:8.4.3-ce.0
--> Found container image a26371b (5 years old) from quay.io for "quay.io/redhattraining/gitlab-ce:8.4.3-ce.0"

    * An image stream tag will be created as "gitlab:8.4.3-ce.0" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "gitlab" created
    deployment.apps "gitlab" created
    service "gitlab" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/gitlab' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc get pods
NAME                      READY   STATUS   RESTARTS   AGE
gitlab-76fb75db89-wd4vh   0/1     Error    2          40s
[student@workstation ~]$ oc logs pods/gitlab-76fb75db89-wd4vh |tail
 37:  node.consume_attributes(Gitlab.generate_config(node['fqdn']))
 38:  
 39:  if File.exists?("/var/opt/gitlab/bootstrapped")
 40:  	node.set['gitlab']['bootstrap']['enable'] = false

[2021-08-08T06:07:34+00:00] ERROR: Running exception handlers
[2021-08-08T06:07:34+00:00] ERROR: Exception handlers complete
[2021-08-08T06:07:34+00:00] FATAL: Stacktrace dumped to /opt/gitlab/embedded/cookbooks/cache/chef-stacktrace.out
[2021-08-08T06:07:34+00:00] ERROR: directory[/etc/gitlab] (gitlab::default line 26) had an error: Chef::Exceptions::InsufficientPermissions: Cannot create directory[/etc/gitlab] at /etc/gitlab due to insufficient permissions
[2021-08-08T06:07:34+00:00] FATAL: Chef::Exceptions::ChildConvergeError: Chef run process exited unsuccessfully (exit code 1)
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "authorization-scc".
[student@workstation ~]$ oc get pods/gitlab-76fb75db89-wd4vh -o yaml |oc adm policy scc-subject-review -f -
RESOURCE                      ALLOWED BY   
Pod/gitlab-76fb75db89-wd4vh   anyuid       
[student@workstation ~]$ oc create sa gitlab-sa
serviceaccount/gitlab-sa created
[student@workstation ~]$ oc adm policy add-scc-to-user anyuid -z gitlab-sa
clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: "gitlab-sa"
[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You have one project on this server: "authorization-scc"

Using project "authorization-scc".
[student@workstation ~]$ oc set serviceaccount deployment/gitlab gitlab-sa
deployment.apps/gitlab serviceaccount updated
[student@workstation ~]$ oc get pods
NAME                      READY   STATUS              RESTARTS   AGE
gitlab-6dd7bff495-n7tph   0/1     ContainerCreating   0          8s
gitlab-76fb75db89-wd4vh   0/1     CrashLoopBackOff    4          3m13s
[student@workstation ~]$ oc expose service/gitlab --port 80 --hostname gitlab.apps.ocp4.example.com
route.route.openshift.io/gitlab exposed
[student@workstation ~]$ oc get routes
NAME     HOST/PORT                      PATH   SERVICES   PORT   TERMINATION   WILDCARD
gitlab   gitlab.apps.ocp4.example.com          gitlab     80                   None
[student@workstation ~]$ curl -s http://gitlab.apps.ocp4.example.com/users/sign_in | grep '<title>'
<title>Sign in · GitLab</title>
[student@workstation ~]$ oc delete project authorization-scc
project.project.openshift.io "authorization-scc" deleted
[student@workstation ~]$ lab authorization-scc finish

Completing Guided Exercise: Controlling Application Permissions with Security Context Constraints


Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

--------------------------------------------------------------------------------------------------------------------------
Chapter 4 | Configuring Application Security
Lab
Configuring Application Security Page:110
---------------------------------------------------------------------------------------------------------------------------
[student@workstation ~]$ lab authorization-review start

Checking prerequisites for Lab: Configuring Application Security

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'authorization-review' project is absent................  SUCCESS

Setting up the classroom for Lab: Configuring Application Security

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS
 · Removing the 'anyuid' SCC cluster role binding..............  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>

[student@workstation ~]$ oc new-project authorization-review
Now using project "authorization-review" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc create secret generic review-secret --from-literal user=wpuser --from-literal password=redhat123 --from-literal database=wordpress
secret/review-secret created
[student@workstation ~]$ oc new-app --name mysql --docker-image registry.access.redhat.com/rhscl/mysql-57-rhel7:5.7-47
--> Found container image 77d20f2 (2 years old) from registry.access.redhat.com for "registry.access.redhat.com/rhscl/mysql-57-rhel7:5.7-47"

    MySQL 5.7 
    --------- 
    MySQL is a multi-user, multi-threaded SQL database server. The container image provides a containerized packaging of the MySQL mysqld daemon and client application. The mysqld server daemon accepts connections from clients and provides access to content from MySQL databases on behalf of the clients.

    Tags: database, mysql, mysql57, rh-mysql57

    * An image stream tag will be created as "mysql:5.7-47" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "mysql" created
    deployment.apps "mysql" created
    service "mysql" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/mysql' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc set env deployment/mysql  --from secret/review-secret --prefix MYSQL_
deployment.apps/mysql updated
[student@workstation ~]$ oc get pods
NAME                     READY   STATUS        RESTARTS   AGE
mysql-59db547cdb-hx48f   1/1     Running       0          12s
mysql-786bb947f9-j97hg   0/1     Terminating   3          63s
[student@workstation ~]$ 

[student@workstation ~]$ oc new-app --name wordpress --docker-image quay.io/redhattraining/wordpress:5.3.0 -e WORDPRESS_DB_HOST=mysql -e WORDPRESS_DB_NAME=wordpress
--> Found container image ee025cb (20 months old) from quay.io for "quay.io/redhattraining/wordpress:5.3.0"

    * An image stream tag will be created as "wordpress:5.3.0" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "wordpress" created
    deployment.apps "wordpress" created
    service "wordpress" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/wordpress' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc set env deployment/wordpress --prefix WORDPRESS_DB_ --from secret/review-secret
deployment.apps/wordpress updated
[student@workstation ~]$ watch oc get pods -l deployment=wordpress
[student@workstation ~]$  oc get pods -l deployment=wordpress
NAME                         READY   STATUS    RESTARTS   AGE
wordpress-77d47b957c-bpww7   0/1     Error     0          23s
wordpress-fb55dcc77-gqshq    1/1     Running   1          55s
[student@workstation ~]$  oc get pods -l deployment=wordpress
NAME                         READY   STATUS        RESTARTS   AGE
wordpress-77d47b957c-bpww7   0/1     Error         2          46s
wordpress-fb55dcc77-gqshq    1/1     Terminating   1          78s
[student@workstation ~]$  oc get pods -l deployment=wordpress
NAME                         READY   STATUS    RESTARTS   AGE
wordpress-77d47b957c-bpww7   1/1     Running   3          70s
[student@workstation ~]$  oc get pods -l deployment=wordpress
NAME                         READY   STATUS   RESTARTS   AGE
wordpress-77d47b957c-bpww7   0/1     Error    4          2m
[student@workstation ~]$ oc logs wordpress-77d47b957c-bpww7 | tail
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 10.10.0.37. Set the 'ServerName' directive globally to suppress this message
(13)Permission denied: AH00072: make_sock: could not bind to address [::]:80
(13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:80
no listening sockets available, shutting down
AH00015: Unable to open logs
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "authorization-review".
[student@workstation ~]$ oc get pods/wordpress-77d47b957c-bpww7 -o yaml | oc adm policy scc-subject-review -f -
RESOURCE                         ALLOWED BY   
Pod/wordpress-77d47b957c-bpww7   anyuid       
[student@workstation ~]$ oc create serviceaccount wordpress-sa
serviceaccount/wordpress-sa created
[student@workstation ~]$ oc adm policy add-scc-to-user anyuid -z wordpress-sa
clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: "wordpress-sa"
[student@workstation ~]$ oc set serviceaccount deployment/wordpress wordpress-sa
deployment.apps/wordpress serviceaccount updated
[student@workstation ~]$ oc get pods -l deployment=wordpress
NAME                         READY   STATUS        RESTARTS   AGE
wordpress-77d47b957c-bpww7   0/1     Terminating   5          5m15s
wordpress-b4869674f-bcxwq    1/1     Running       0          13s
[student@workstation ~]$ oc expose service/wordpress --hostname wordpress-review.apps.ocp4.example.com
route.route.openshift.io/wordpress exposed
[student@workstation ~]$ curl -s http://wordpress-review.apps.ocp4.example.com/wp-admin/install.php | grep Installation
	<title>WordPress &rsaquo; Installation</title>
[student@workstation ~]$ lab authorization-review grade

Grading the student's work for Lab: Configuring Application Security

 Verifying the information collected about the cluster:
 · The 'authorization-review' project exists...................  PASS
 · The 'review-secret' secret exists...........................  PASS
 · The 'mysql' application exists..............................  PASS
 · The 'MYSQL_USER' environment variable is set from the 'revie
   w-secret' secret............................................  PASS
 · The 'MYSQL_PASSWORD' environment variable is set from the 'r
   eview-secret' secret........................................  PASS
 · The 'MYSQL_DATABASE' environment variable is set from the 'r
   eview-secret' secret........................................  PASS
 · The 'wordpress' application exists..........................  PASS
 · The 'WORDPRESS_DB_USER' environment variable is set from the
   'review-secret' secret......................................  PASS
 · The 'WORDPRESS_DB_PASSWORD' environment variable is set from
   the 'review-secret' secret..................................  PASS
 · The 'WORDPRESS_DB_DATABASE' environment variable is set from
   the 'review-secret' secret..................................  PASS
 · The 'wordpress-sa' service account exists...................  PASS
 · The 'wordpress-sa' service account uses the 'anyuid' SCC....  PASS
 · The 'wordpress' application uses the 'wordpress-sa' service 
   account.....................................................  PASS
 · The 'wordpress' application uses the 'anyuid' SCC...........  PASS
 · The 'wordpress' deployment is exposed at 'wordpress-review.a
   pps.ocp4.example.com'.......................................  PASS
 · The 'wordpress-review.apps.ocp4.example.com' URL responds to
   external requests...........................................  PASS

Overall exercise grade.........................................  PASS

[student@workstation ~]$ 

Summary
In this chapter, you learned:
• Secret resources allow you to separate sensitive information from application pods. You expose
secrets to an application pod either as environment variables or as ordinary files.
• OpenShift uses security context constraints (SCCs) to define allowed pod interactions with
system resources. By default, pods operate under the restricted context which limits access
to node resources.

------------------------------------------------------------------------------------------------------------------------------------
Chapter 5 | Configuring OpenShift Networking for Applications
Guided Exercise
Troubleshooting OpenShift Software-defined Networking Page 125
--------------------------------------------------------------------------------------------------------------------
In this exercise, you will diagnose and fix connectivity issues with a Kubernetes-style
application deployment.
Outcomes
You should be able to:
• Deploy the To Do Node.js application.
• Create a route to expose an application service.
• Troubleshoot communication between pods in your application using oc debug.
• Update an OpenShift service.


[student@workstation ~]$ lab network-sdn start

Checking prerequisites for Guided Exercise: Troubleshooting OpenShift Software-Defined Networking

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'network-sdn' project is absent.........................  SUCCESS

Setting up the classroom for Guided Exercise: Troubleshooting OpenShift Software-Defined Networking

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS
 Preparing the student's workstation:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You have one project on this server: "authorization-review"

Using project "authorization-review".
[student@workstation ~]$ oc new-project network-sdn
Now using project "network-sdn" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git
to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ cd ~/DO280/labs/network-sdn
[student@workstation network-sdn]$ cat todo-db.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  labels:
    app: todonodejs
    name: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: todonodejs
      name: mysql
  template:
    metadata:
      labels:
        app: todonodejs
        name: mysql
    spec:
      containers:
      - image: registry.access.redhat.com/rhscl/mysql-57-rhel7:5.7-47
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: r00tpa55
        - name: MYSQL_USER
          value: user1
        - name: MYSQL_PASSWORD
          value: mypa55
        - name: MYSQL_DATABASE
	  value: items
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - mountPath: "/var/lib/mysql"
          name: db-volume
      volumes:
      - name: db-volume
        emptyDir: {}
      - name: db-init
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: todonodejs
    name: mysql
  name: mysql
spec:
  ports:
  - port: 3306
  selector:
    name: mysql
[student@workstation network-sdn]$ 
[student@workstation ~]$ cd ~/DO280/labs/network-sdn
[student@workstation network-sdn]$ oc create -f todo-db.yaml
deployment.apps/mysql created
service/mysql created
[student@workstation network-sdn]$ oc status
In project network-sdn on server https://api.ocp4.example.com:6443

svc/mysql - 172.30.235.235:3306
  deployment/mysql deploys registry.access.redhat.com/rhscl/mysql-57-rhel7:5.7-47
    deployment #1 running for 6 seconds - 1 pod


1 info identified, use 'oc status --suggest' to see details.
[student@workstation network-sdn]$ oc get pods
NAME                     READY   STATUS    RESTARTS   AGE
mysql-6dd69fbbf4-ptw6n   1/1     Running   0          18s
[student@workstation network-sdn]$ oc cp db-data.sql mysql-6dd69fbbf4-ptw6n:/tmp
[student@workstation network-sdn]$ oc rsh mysql-6dd69fbbf4-ptw6n
sh-4.2$ mysql -u root  items < /tmp/db-data.sql
sh-4.2$ mysql -u root  items -e "show tables;"
+-----------------+
| Tables_in_items |
+-----------------+
| Item            |
+-----------------+
sh-4.2$ exit
[student@workstation ~]$ source /usr/local/etc/ocp4.config
[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    authorization-review
  * network-sdn

Using project "network-sdn".
[student@workstation ~]$ cd ~/DO280/labs/network-sdn
[student@workstation network-sdn]$ oc status
In project network-sdn on server https://api.ocp4.example.com:6443

svc/mysql - 172.30.235.235:3306
  deployment/mysql deploys registry.access.redhat.com/rhscl/mysql-57-rhel7:5.7-47
    deployment #1 running for 11 minutes - 1 pod


1 info identified, use 'oc status --suggest' to see details.
[student@workstation network-sdn]$ oc create -f todo-db.yaml
Error from server (AlreadyExists): error when creating "todo-db.yaml": deployments.apps "mysql" already exists
Error from server (AlreadyExists): error when creating "todo-db.yaml": services "mysql" already exists
[student@workstation network-sdn]$ oc create -f todo-frontend.yaml
deployment.apps/frontend created
service/frontend created
[student@workstation network-sdn]$ oc get pods
NAME                        READY   STATUS              RESTARTS   AGE
frontend-5598cfc446-dslcq   0/1     ContainerCreating   0          9s
mysql-6dd69fbbf4-ptw6n      1/1     Running             0          11m
[student@workstation network-sdn]$ oc expose service frontend --hostname todo.apps.ocp4.example.com
route.route.openshift.io/frontend exposed
[student@workstation network-sdn]$ oc get routes
NAME       HOST/PORT                    PATH   SERVICES   PORT   TERMINATION   WILDCARD
frontend   todo.apps.ocp4.example.com          frontend   8080                 None
[student@workstation network-sdn]$ oc logs frontend-5598cfc446-dslcq
App is ready at : 8080
[student@workstation network-sdn]$ oc get service/mysql -o jsonpath="{.spec.clusterIP}{'\n'}"
172.30.235.235
[student@workstation network-sdn]$ oc debug -t deployment/frontend
Starting pod/frontend-debug ...
Pod IP: 10.10.0.35
If you don't see a command prompt, try pressing enter.
sh-4.2$ curl -v telnet://172.30.235.235:3306
* About to connect() to 172.30.235.235 port 3306 (#0)
*   Trying 172.30.235.235...
* Connected to 172.30.235.235 (172.30.235.235) port 3306 (#0)
J
5.7.24j .cC'
* RCVD IAC 193
^C
sh-4.2$ exit
exit

Removing debug pod ...
[student@workstation network-sdn]$ oc get service/frontend -o jsonpath="{.spec.clusterIP}{'\n'}"
172.30.227.97
[student@workstation network-sdn]$ oc debug -t deployment/mysql --image registry.access.redhat.com/ubi8/ubi:8.0
Starting pod/mysql-debug ...
Pod IP: 10.10.0.36
If you don't see a command prompt, try pressing enter.
sh-4.4$ curl -m 10 -v http://172.30.227.97:8080
* Rebuilt URL to: http://172.30.227.97:8080/
*   Trying 172.30.227.97...
* TCP_NODELAY set
* Connection timed out after 10000 milliseconds
* Closing connection 0
curl: (28) Connection timed out after 10000 milliseconds
sh-4.4$ exit
exit

Removing debug pod ...
[student@workstation network-sdn]$ oc get pods -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES
frontend-5598cfc446-dslcq   1/1     Running   0          6m47s   10.9.0.17   master03   <none>           <none>
mysql-6dd69fbbf4-ptw6n      1/1     Running   0          18m     10.9.0.16   master03   <none>           <none>
[student@workstation network-sdn]$ oc debug -t deployment/mysql --image registry.access.redhat.com/ubi8/ubi:8.0
Starting pod/mysql-debug ...
Pod IP: 10.10.0.37
If you don't see a command prompt, try pressing enter.
sh-4.4$ curl -m 10 -v http://10.9.0.17:8080
* Rebuilt URL to: http://10.9.0.17:8080/
*   Trying 10.9.0.17...
* TCP_NODELAY set
* Connected to 10.9.0.17 (10.9.0.17) port 8080 (#0)
> GET / HTTP/1.1
> Host: 10.9.0.17:8080
> User-Agent: curl/7.61.1
> Accept: */*
> 
< HTTP/1.1 404 Not Found
< Server: restify
< Content-Type: application/json
< Content-Length: 56
< Date: Sun, 08 Aug 2021 10:54:09 GMT
< Connection: keep-alive
< 
* Connection #0 to host 10.9.0.17 left intact
{"code":"ResourceNotFound","message":"/ does not exist"}sh-4.4$ exit
exit

Removing debug pod ...
[student@workstation network-sdn]$ oc get svc
NAME       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
frontend   ClusterIP   172.30.227.97    <none>        8080/TCP   8m23s
mysql      ClusterIP   172.30.235.235   <none>        3306/TCP   19m
[student@workstation network-sdn]$ oc describe svc/frontend
Name:              frontend
Namespace:         network-sdn
Labels:            app=todonodejs
                   name=frontend
Annotations:       <none>
Selector:          name=api
Type:              ClusterIP
IP:                172.30.227.97
Port:              <unset>  8080/TCP
TargetPort:        8080/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>
[student@workstation network-sdn]$ 
[student@workstation network-sdn]$ oc describe deployment/frontend | grep Labels -A1
Labels:                 app=todonodejs
                        name=frontend
--
  Labels:  app=todonodejs
           name=frontend
[student@workstation network-sdn]$ oc edit svc/frontend
service/frontend edited
[student@workstation network-sdn]$ oc describe svc/frontend
Name:              frontend
Namespace:         network-sdn
Labels:            app=todonodejs
                   name=frontend
Annotations:       <none>
Selector:          name=frontend
Type:              ClusterIP
IP:                172.30.227.97
Port:              <unset>  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.9.0.17:8080
Session Affinity:  None
Events:            <none>
[student@workstation network-sdn]$ oc delete project network-sdn
project.project.openshift.io "network-sdn" deleted
[student@workstation ~]$ cd
[student@workstation ~]$ lab network-sdn finish

Completing Guided Exercise: Troubleshooting OpenShift Software-Defined Networking

 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 


--------------------------------------------------------------------------------------------------------------------------
Chapter 5 | Configuring OpenShift NNNNNetworking for Applications
Guided Exercise
Exposing Applications for External Access Page: 140

---------------------------------------------------------------------------------------------------------------------
[student@workstation ~]$ lab network-ingress start

Checking prerequisites for Guided Exercise: Controlling Cluster Network Ingress

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'network-ingress' project is absent.....................  SUCCESS

Setting up the classroom for Guided Exercise: Controlling Cluster Network Ingress

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS
 Preparing Workstation:
 · Download exercise files.....................................  SUCCESS
 Configuring Certificates:
 · Generating unique CA key password...........................  SUCCESS
 · Setting environment variable in cert. configuration file....  SUCCESS
 · Generating the CA key.......................................  SUCCESS
 · Generating CA certificate...................................  SUCCESS
 · Updating privileges on certs directory......................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You have one project on this server: "authorization-review"

Using project "authorization-review".
[student@workstation ~]$ oc new-project network-ingress
Now using project "network-ingress" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc create -f ~/DO280/labs/network-ingress/todo-app-v1.yaml
deployment.apps/todo-http created
service/todo-http created
[student@workstation ~]$ oc status
In project network-ingress on server https://api.ocp4.example.com:6443

svc/todo-http - 172.30.79.162:80 -> 8080
  deployment/todo-http deploys quay.io/redhattraining/todo-angular:v1.1
    deployment #1 running for 11 seconds - 0/1 pods


1 info identified, use 'oc status --suggest' to see details.
[student@workstation ~]$ cat ~/DO280/labs/network-ingress/todo-app-v1.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: todo-http
  labels:
    app: todo-http
    name: todo-http
  namespace: network-ingress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: todo-http
      name: todo-http
  template:
    metadata:
      labels:
        app: todo-http
        name: todo-http
    spec:
      containers:
      - resources:
          limits:
            cpu: '0.5'
        image: quay.io/redhattraining/todo-angular:v1.1
        name: todo-http
        ports:
        - containerPort: 8080
          name: todo-http
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: todo-http
    name: todo-http
  name: todo-http
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    name: todo-http[student@workstation ~]$ oc expose svc todo-http --hostname todo-http.apps.ocp4.example.com
route.route.openshift.io/todo-http exposed
[student@workstation ~]$ oc get routes
NAME        HOST/PORT                         PATH   SERVICES    PORT   TERMINATION   WILDCARD
todo-http   todo-http.apps.ocp4.example.com          todo-http   8080                 None
[student@workstation ~]$ cd ~/DO280/labs/network-ingress
[student@workstation network-ingress]$ oc create route edge todo-https --service todo-http --hostname todo-https.apps.ocp4.example.com
route.route.openshift.io/todo-https created
[student@workstation network-ingress]$ curl https://todo-https.apps.ocp4.example.com
curl: (60) SSL certificate problem: self signed certificate in certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.
[student@workstation network-ingress]$ oc login -u admin -p redhat
Login successful.

You have access to 60 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "network-ingress".
[student@workstation network-ingress]$ oc extract secrets/router-ca --keys tls.crt -n openshift-ingress-operator
tls.crt
[student@workstation network-ingress]$ curl -I -v --cacert tls.crt https://todo-https.apps.ocp4.example.com
* Rebuilt URL to: https://todo-https.apps.ocp4.example.com/
*   Trying 192.168.50.254...
* TCP_NODELAY set
* Connected to todo-https.apps.ocp4.example.com (192.168.50.254) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: tls.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server did not agree to a protocol
* Server certificate:
*  subject: CN=*.apps.ocp4.example.com
*  start date: Aug  5 18:28:25 2020 GMT
*  expire date: Aug  5 18:28:26 2022 GMT
*  subjectAltName: host "todo-https.apps.ocp4.example.com" matched cert's "*.apps.ocp4.example.com"
*  issuer: CN=ingress-operator@1596652102
*  SSL certificate verify ok.
> HEAD / HTTP/1.1
> Host: todo-https.apps.ocp4.example.com
> User-Agent: curl/7.61.1
> Accept: */*
> 
< HTTP/1.1 200 OK
HTTP/1.1 200 OK
< server: nginx/1.14.1
server: nginx/1.14.1
< date: Sun, 08 Aug 2021 13:49:01 GMT
date: Sun, 08 Aug 2021 13:49:01 GMT
< content-type: text/html
content-type: text/html
< set-cookie: 947acc174fec7aa4880f048abd1ce952=512d245a7bf0445117dcc93d0124a5b7; path=/; HttpOnly; Secure
set-cookie: 947acc174fec7aa4880f048abd1ce952=512d245a7bf0445117dcc93d0124a5b7; path=/; HttpOnly; Secure
< cache-control: private
cache-control: private

< 
* Connection #0 to host todo-https.apps.ocp4.example.com left intact
[student@workstation network-ingress]$ oc login -u developer -p developer
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    authorization-review
  * network-ingress

Using project "network-ingress".
[student@workstation network-ingress]$ oc get svc todo-http -o jsonpath="{.spec.clusterIP}{'\n'}"
172.30.79.162
[student@workstation network-ingress]$ oc debug -t deployment/todo-http --image registry.access.redhat.com/ubi8/ubi:8.0
Starting pod/todo-http-debug ...
Pod IP: 10.10.0.39
If you don't see a command prompt, try pressing enter.
sh-4.4$ curl -v 172.30.79.162
* Rebuilt URL to: 172.30.79.162/
*   Trying 172.30.79.162...
* TCP_NODELAY set
* Connected to 172.30.79.162 (172.30.79.162) port 80 (#0)
> GET / HTTP/1.1
> Host: 172.30.79.162
> User-Agent: curl/7.61.1
> Accept: */*
> 
< HTTP/1.1 200 OK
< Server: nginx/1.14.1
< Date: Sun, 08 Aug 2021 13:50:42 GMT
< Content-Type: text/html
< Transfer-Encoding: chunked
< Connection: keep-alive
< 
<!DOCTYPE html>
<html lang="en" ng-app="todoItemsApp" ng-controller="appCtl">
<head>
    <meta charset="utf-8">
    <title>ToDo app</title>

    <link rel="stylesheet" href="assets/css/libs/bootstrap/bootstrap.css">
    <link rel="stylesheet" href="assets/css/libs/angular-motion/angular-motion.css">
    <link rel="stylesheet" href="assets/css/libs/angular-xeditable/xeditable.css">

    <link rel="stylesheet" href="assets/css/app.css">

    <script type="text/javascript" src="assets/js/libs/jquery/jquery-1.11.1.min.js"></script>
    <script type="text/javascript" src="assets/js/libs/bootstrap/bootstrap.min.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular/angular.min.js"></script>

    <script type="text/javascript" src="assets/js/libs/angular/angular-route.min.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular/angular-animate.min.js"></script>

    <script type="text/javascript" src="assets/js/libs/angular-ui-router/angular-ui-router.js"></script>

    <script type="text/javascript" src="assets/js/libs/angular-strap/angular-strap.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular-strap/angular-strap.tpl.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular-xeditable/xeditable.js"></script>

    <script type="text/javascript" src="assets/js/libs/angular/angular-sanitize.js"></script>
    <script type="text/javascript" src="assets/js/app/utils/md5.js"></script>

    <script type="text/javascript" src="assets/js/app/module.js"></script>
    <script type="text/javascript" src="assets/js/app/states/states.js"></script>

    <script type="text/javascript" src="assets/js/app/domain/todoitems.js"></script>
    <script type="text/javascript" src="assets/js/app/ui/filters.js"></script>

    <script type="text/javascript" src="assets/js/app/ui/focus.js"></script>
    <script type="text/javascript" src="assets/js/app/ui/gravatar.js"></script>
    <script type="text/javascript" src="assets/js/app/ui/editable.js"></script>
</head>
<body>

<nav class="navbar navbar-default" role="navigation">
    <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#nav-toggle">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="/">ToDo App</a>
    </div>
    <div class="collapse navbar-collapse" id="nav-toggle">
        <ul class="nav navbar-nav">
            <li ui-sref-active="active"><a ui-sref="list">Browse</a></li>
            <li ui-sref-active="active"><a ui-sref="create">Create</a></li>
        </ul>
        <form class="navbar-form navbar-right" role="search">
            <input type="text" class="form-control" placeholder="Search" ng-model="search.description" ng-keyup="startSearch()">
        </form>
    </div>

</nav>
<div class="container">
    <div ui-view></div>
</div>

</body>
* Connection #0 to host 172.30.79.162 left intact
</html>sh-4.4$ exit
exit

Removing debug pod ...
[student@workstation network-ingress]$ oc delete route todo-https
route.route.openshift.io "todo-https" deleted
[student@workstation network-ingress]$ cd ~/DO280/labs/network-ingress/certs
[student@workstation certs]$ ls
openssl-commands.txt  passphrase.txt  training-CA.key  training-CA.pem  training.ext
[student@workstation certs]$ cat openssl-commands.txt 
## Run the following command to create the private key

openssl genrsa -out training.key 2048

## Run the following command to generate a certificate signing request

openssl req -new \
  -subj "/C=US/ST=North Carolina/L=Raleigh/O=Red Hat/CN=todo-https.apps.ocp4.example.com" \
  -key training.key -out training.csr

## Run the following command to generate a certificate

openssl x509 -req -in training.csr \
  -passin file:passphrase.txt \
  -CA training-CA.pem -CAkey training-CA.key -CAcreateserial \
  -out training.crt -days 1825 -sha256 -extfile training.ext
[student@workstation certs]$ openssl genrsa -out training.key 2048
Generating RSA private key, 2048 bit long modulus (2 primes)
..........................+++++
..............................................................+++++
e is 65537 (0x010001)
[student@workstation certs]$ openssl req -new \
>   -subj "/C=US/ST=North Carolina/L=Raleigh/O=Red Hat/CN=todo-https.apps.ocp4.example.com" \
>   -key training.key -out training.csr
[student@workstation certs]$ openssl x509 -req -in training.csr \
>   -passin file:passphrase.txt \
>   -CA training-CA.pem -CAkey training-CA.key -CAcreateserial \
>   -out training.crt -days 1825 -sha256 -extfile training.ext
Signature ok
subject=C = US, ST = North Carolina, L = Raleigh, O = Red Hat, CN = todo-https.apps.ocp4.example.com
Getting CA Private Key
[student@workstation certs]$ ls -lhrt
total 36K
-rw-rw-r--. 1 student student  566 Jul 15 06:59 openssl-commands.txt
-rw-rw-r--. 1 student student   33 Aug  8 09:27 passphrase.txt
-rw-r--r--. 1 student student  352 Aug  8 09:27 training.ext
-rw-------. 1 student student 1.8K Aug  8 09:27 training-CA.key
-rw-r--r--. 1 student student 1.4K Aug  8 09:27 training-CA.pem
-rw-------. 1 student student 1.7K Aug  8 09:52 training.key
-rw-rw-r--. 1 student student 1021 Aug  8 09:52 training.csr
-rw-rw-r--. 1 student student   41 Aug  8 09:53 training-CA.srl
-rw-rw-r--. 1 student student 1.4K Aug  8 09:53 training.crt
[student@workstation certs]$ cd ~/DO280/labs/network-ingress
[student@workstation network-ingress]$ oc create secret tls todo-certs --cert certs/training.crt --key certs/training.key
secret/todo-certs created
[student@workstation network-ingress]$ oc create -f todo-app-v2.yaml
deployment.apps/todo-https created
service/todo-https created
[student@workstation network-ingress]$ oc get pods
NAME                          READY   STATUS              RESTARTS   AGE
todo-http-59446d7885-bw8fx    1/1     Running             0          25m
todo-https-85bff5b996-5vxvq   0/1     ContainerCreating   0          9s
[student@workstation network-ingress]$ oc get pods
NAME                          READY   STATUS    RESTARTS   AGE
todo-http-59446d7885-bw8fx    1/1     Running   0          25m
todo-https-85bff5b996-5vxvq   1/1     Running   0          33s
[student@workstation network-ingress]$ oc describe pod todo-https-85bff5b996-5vxvq| grep Mounts -A2
    Mounts:
      /usr/local/etc/ssl/certs from tls-certs (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kvzsg (ro)
[student@workstation network-ingress]$ cat todo-app-v2.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: todo-https
  labels:
    app: todo-https
    name: todo-https
  namespace: network-ingress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: todo-https
      name: todo-https
  template:
    metadata:
      labels:
        app: todo-https
        name: todo-https
    spec:
      containers:
      - resources:
          limits:
            cpu: '0.5'
        image: quay.io/redhattraining/todo-angular:v1.2
        name: todo-https
        ports:
        - containerPort: 8080
          name: todo-http
        - containerPort: 8443
          name: todo-https
        volumeMounts:
        - name: tls-certs
          readOnly: true
          mountPath: /usr/local/etc/ssl/certs
      resources:
        limits:
          memory: 64Mi
      volumes:
      - name: tls-certs
        secret:
          secretName: todo-certs
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: todo-https
    name: todo-https
  name: todo-https
spec:
  ports:
  - name: https
    port: 8443
    protocol: TCP
    targetPort: 8443
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    name: todo-https
[student@workstation network-ingress]$ oc create route passthrough todo-https --service todo-https --port 8443 --hostname todo-https.apps.ocp4.example.com
route.route.openshift.io/todo-https created
[student@workstation network-ingress]$ curl -vvI --cacert certs/training-CA.pem https://todo-https.apps.ocp4.example.com
* Rebuilt URL to: https://todo-https.apps.ocp4.example.com/
*   Trying 192.168.50.254...
* TCP_NODELAY set
* Connected to todo-https.apps.ocp4.example.com (192.168.50.254) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: certs/training-CA.pem
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: C=US; ST=North Carolina; L=Raleigh; O=Red Hat; CN=todo-https.apps.ocp4.example.com
*  start date: Aug  8 13:53:08 2021 GMT
*  expire date: Aug  7 13:53:08 2026 GMT
*  subjectAltName: host "todo-https.apps.ocp4.example.com" matched cert's "*.apps.ocp4.example.com"
*  issuer: C=US; ST=North Carolina; L=Raleigh; O=Red Hat; CN=ocp4.example.com
*  SSL certificate verify ok.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x5563426e7740)
> HEAD / HTTP/2
> Host: todo-https.apps.ocp4.example.com
> User-Agent: curl/7.61.1
> Accept: */*
> 
* Connection state changed (MAX_CONCURRENT_STREAMS == 128)!
< HTTP/2 200 
HTTP/2 200 
< server: nginx/1.14.1
server: nginx/1.14.1
< date: Sun, 08 Aug 2021 13:58:22 GMT
date: Sun, 08 Aug 2021 13:58:22 GMT
< content-type: text/html
content-type: text/html
< content-length: 3017
content-length: 3017
< last-modified: Thu, 28 Nov 2019 19:53:20 GMT
last-modified: Thu, 28 Nov 2019 19:53:20 GMT
< etag: "5de025b0-bc9"
etag: "5de025b0-bc9"
< strict-transport-security: max-age=63072000; includeSubdomains
strict-transport-security: max-age=63072000; includeSubdomains
< x-frame-options: DENY
x-frame-options: DENY
< x-content-type-options: nosniff
x-content-type-options: nosniff
< accept-ranges: bytes
accept-ranges: bytes

< 
* Connection #0 to host todo-https.apps.ocp4.example.com left intact
[student@workstation network-ingress]$ oc get svc todo-https \
> >
bash: syntax error near unexpected token `newline'
[student@workstation network-ingress]$ oc get svc todo-https -o jsonpath="{.spec.clusterIP}{'\n'}"
172.30.221.57
[student@workstation network-ingress]$ oc debug -t deployment/todo-https --image registry.access.redhat.com/ubi8/ubi:8.0
Starting pod/todo-https-debug ...
Pod IP: 10.8.0.37
If you don't see a command prompt, try pressing enter.
sh-4.4$ curl -I http://172.30.121.154
curl: (7) Failed to connect to 172.30.121.154 port 80: No route to host
sh-4.4$ curl -I http://172.30.221.57 
HTTP/1.1 301 Moved Permanently
Server: nginx/1.14.1
Date: Sun, 08 Aug 2021 14:00:04 GMT
Content-Type: text/html
Connection: keep-alive
Location: https://172.30.221.57:8443/

sh-4.4$ curl -s -k https://172.30.221.57:8443 | head -n5
<!DOCTYPE html>
<html lang="en" ng-app="todoItemsApp" ng-controller="appCtl">
<head>
    <meta charset="utf-8">
    <title>ToDo app</title>
sh-4.4$ exit
exit

Removing debug pod ...
[student@workstation network-ingress]$ cd
[student@workstation ~]$ oc delete project network-ingress
project.project.openshift.io "network-ingress" deleted
[student@workstation ~]$ lab network-ingress finish

Completing Guided Exercise: Controlling Cluster Network Ingress

 · Remove exercise files.......................................  FAIL

Cannot continue due to the previous errors.....................  FAIL
One or more terminal prompts is at /home/student/DO280/labs/network-ingress (or a subdirectory). Change to the /home/student/ directory and run 'lab network-ingress finish' again.

[student@workstation ~]$ lab network-ingress finish

Completing Guided Exercise: Controlling Cluster Network Ingress

 · Remove exercise files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 
---------------------------------------------------------------------------------------------------------------



[student@workstation network-ingress]$ cat tcpdump-command.txt 
## Run the following commands to intercept the traffic on your workstation.
* Refresh the web page to capture some data

* To retrieve the name of the main interface, run the following command:
ip a | grep 172.25.250.9

## Tcpdump command
sudo tcpdump -i eth0 -A -n port 80 | grep js
[student@workstation network-ingress]$ pwd
/home/student/DO280/labs/network-ingress
[student@workstation network-ingress]$ sudo tcpdump -i eth0 -A -n port 80 | grep js
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
Accept: application/json, text/plain, */*
    <script type="text/javascript" src="assets/js/libs/jquery/jquery-1.11.1.min.js"></script>
    <script type="text/javascript" src="assets/js/libs/bootstrap/bootstrap.min.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular/angular.min.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular/angular-route.min.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular/angular-animate.min.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular-ui-router/angular-ui-router.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular-strap/angular-strap.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular-strap/angular-strap.tpl.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular-xeditable/xeditable.js"></script>
    <script type="text/javascript" src="assets/js/libs/angular/angular-sanitize.js"></script>
    <script type="text/javascript" src="assets/js/app/utils/md5.js"></script>
    <script type="text/javascript" src="assets/js/app/module.js"></script>
    <script type="text/javascript" src="assets/js/app/states/states.js"></script>
    <script type="text/javascript" src="assets/js/app/domain/to
    
    ----------------------------------------------------------------------------------------------------------------------------
    Chapter 5 | Configuring OpenShift Networking for Applications
    Guided Exercise
    Configuring Network Policies Page: 154    
    ---------------------------------------------------------------------------------------------------------------------------
    
    In this exercise, you will create network policies and review pod isolation created by these
    network policies.
    Outcomes
    You should be able to:
    • Create network policies to control communication between pods.
    • Verify ingress traffic is limited to pods.
    
    [student@workstation ~]$ lab network-policy start

Checking prerequisites for Guided Exercise: Configuring Network Policies

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'network-policy' project is absent......................  SUCCESS
 · The 'network-test' project is absent........................  SUCCESS

Setting up the classroom for Guided Exercise: Configuring Network Policies

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS
 Preparing Workstation:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You have one project on this server: "authorization-review"

Using project "authorization-review".
[student@workstation ~]$ oc new-project network-policy
Now using project "network-policy" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc new-app --name hello --docker-image quay.io/redhattraining/hello-world-nginx:v1.0
--> Found container image 44eaa13 (2 years old) from quay.io for "quay.io/redhattraining/hello-world-nginx:v1.0"

    Red Hat Universal Base Image 8 
    ------------------------------ 
    The Universal Base Image is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.

    Tags: base rhel8

    * An image stream tag will be created as "hello:v1.0" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "hello" created
    deployment.apps "hello" created
    service "hello" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc expose service hello
route.route.openshift.io/hello exposed
[student@workstation ~]$ ~/DO280/labs/network-policy/display-project-info.sh
===================================================================
PROJECT: network-policy

POD NAME                 IP ADDRESS
hello-6c4984d949-4bzwh   10.9.0.16

SERVICE NAME   CLUSTER-IP
hello          172.30.51.242

ROUTE NAME   HOSTNAME                                     PORT
hello        hello-network-policy.apps.ocp4.example.com   8080-tcp

===================================================================
[student@workstation ~]$ cat ~/DO280/labs/network-policy/display-project-info.sh
#!/usr/bin/bash

if oc get project -o jsonpath='{.items[*].metadata.name}' | grep -q network-policy
then
  echo "==================================================================="
  echo "PROJECT: network-policy"
  echo
  oc get pods -o custom-columns="POD NAME:.metadata.name,IP ADDRESS:.status.podIP" -n network-policy
  echo
  oc get svc -o custom-columns="SERVICE NAME:.metadata.name,CLUSTER-IP:.spec.clusterIP" -n network-policy
  echo
  oc get route -o custom-columns="ROUTE NAME:.metadata.name,HOSTNAME:.spec.host,PORT:.spec.port.targetPort" -n network-policy
  echo
  echo "==================================================================="
fi

if oc get project -o jsonpath='{.items[*].metadata.name}' | grep -q network-test
then
  echo "PROJECT: network-test"
  echo
  oc get pods -o custom-columns="POD NAME:.metadata.name" -n network-test
  echo
  echo "==================================================================="
fi
[student@workstation ~]$ oc new-app --name test --docker-image quay.io/redhattraining/hello-world-nginx:v1.0
--> Found container image 44eaa13 (2 years old) from quay.io for "quay.io/redhattraining/hello-world-nginx:v1.0"

    Red Hat Universal Base Image 8 
    ------------------------------ 
    The Universal Base Image is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.

    Tags: base rhel8

    * An image stream tag will be created as "test:v1.0" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "test" created
    deployment.apps "test" created
    service "test" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/test' 
    Run 'oc status' to view your app.
[student@workstation ~]$ ~/DO280/labs/network-policy/display-project-info.sh
===================================================================
PROJECT: network-policy

POD NAME                 IP ADDRESS
hello-6c4984d949-4bzwh   10.9.0.16
test-c4d74c9d5-sbtvb     10.9.0.17

SERVICE NAME   CLUSTER-IP
hello          172.30.51.242
test           172.30.84.161

ROUTE NAME   HOSTNAME                                     PORT
hello        hello-network-policy.apps.ocp4.example.com   8080-tcp

===================================================================
[student@workstation ~]$ oc rsh test-c4d74c9d5-sbtvb curl 10.9.0.16:8080
<html>
  <body>
    <h1>Hello, world from nginx!</h1>
  </body>
</html>
[student@workstation ~]$ oc rsh test-c4d74c9d5-sbtvb curl 172.30.51.242:8080
<html>
  <body>
    <h1>Hello, world from nginx!</h1>
  </body>
</html>
[student@workstation ~]$ curl -s hello-network-policy.apps.ocp4.example.com | grep Hello
    <h1>Hello, world from nginx!</h1>
[student@workstation ~]$ oc new-project network-test
Now using project "network-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc new-app --name sample-app --docker-image quay.io/redhattraining/hello-world-nginx:v1.0
--> Found container image 44eaa13 (2 years old) from quay.io for "quay.io/redhattraining/hello-world-nginx:v1.0"

    Red Hat Universal Base Image 8 
    ------------------------------ 
    The Universal Base Image is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.

    Tags: base rhel8

    * An image stream tag will be created as "sample-app:v1.0" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "sample-app" created
    deployment.apps "sample-app" created
    service "sample-app" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/sample-app' 
    Run 'oc status' to view your app.
[student@workstation ~]$ ~/DO280/labs/network-policy/display-project-info.sh
===================================================================
PROJECT: network-policy

POD NAME                 IP ADDRESS
hello-6c4984d949-4bzwh   10.9.0.16
test-c4d74c9d5-sbtvb     10.9.0.17

SERVICE NAME   CLUSTER-IP
hello          172.30.51.242
test           172.30.84.161

ROUTE NAME   HOSTNAME                                     PORT
hello        hello-network-policy.apps.ocp4.example.com   8080-tcp

===================================================================
PROJECT: network-test

POD NAME
sample-app-d5f945-kn6hx

===================================================================
[student@workstation ~]$ oc rsh sample-app-d5f945-kn6hx curl 172.30.51.242:8080
<html>
  <body>
    <h1>Hello, world from nginx!</h1>
  </body>
</html>
[student@workstation ~]$ oc rsh sample-app-d5f945-kn6hx curl 10.9.0.16:8080
<html>
  <body>
    <h1>Hello, world from nginx!</h1>
  </body>
</html>
[student@workstation ~]$ oc rsh sample-app-d5f945-kn6hx curl 10.9.0.17:8080
<html>
  <body>
    <h1>Hello, world from nginx!</h1>
  </body>
</html>
[student@workstation ~]$ oc project network-policy
Now using project "network-policy" on server "https://api.ocp4.example.com:6443".
[student@workstation ~]$ cd ~/DO280/labs/network-policy/
[student@workstation network-policy]$ cat deny-all.yaml 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-all
spec:
[student@workstation network-policy]$ create -f deny-all.yaml
bash: create: command not found...
[student@workstation network-policy]$ ocreate -f deny-all.yaml
bash: ocreate: command not found...
[student@workstation network-policy]$ oc create -f deny-all.yaml
networkpolicy.networking.k8s.io/deny-all created
[student@workstation network-policy]$ curl -s hello-network-policy.apps.ocp4.example.com | grep Hello
^C
[student@workstation network-policy]$ oc rsh sample-app-d5f945-kn6hx curl 10.9.0.16:8080
Error from server (NotFound): pods "sample-app-d5f945-kn6hx" not found
[student@workstation network-policy]$ oc rsh test-c4d74c9d5-sbtvb curl 10.9.0.16:8080
^Ccommand terminated with exit code 130
[student@workstation network-policy]$ oc project network-test
Now using project "network-test" on server "https://api.ocp4.example.com:6443".
[student@workstation network-policy]$ oc rsh sample-app-d5f945-kn6hx curl 10.9.0.16:8080
^Ccommand terminated with exit code 130
[student@workstation network-policy]$ cat allow-specific.yaml 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-specific
spec:
  podSelector:
    matchLabels:
      deployment: CHANGE_ME
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: CHANGE_ME
        podSelector:
          matchLabels:
            deployment: CHANGE_ME
      ports:
      - port: CHANGE_ME
        protocol: CHANGE_ME
[student@workstation network-policy]$ vi allow-specific.yaml 
[student@workstation network-policy]$ cat allow-specific.yaml 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-specific
spec:
  podSelector:
    matchLabels:
      deployment: hello
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: network-test
        podSelector:
          matchLabels:
            deployment: sample-app
      ports:
      - port: 8080
        protocol: TCP
[student@workstation network-policy]$ oc create -n network-policy -f allow-specific.yaml
networkpolicy.networking.k8s.io/allow-specific created
[student@workstation network-policy]$ oc get networkpolicies -n network-policy
NAME             POD-SELECTOR       AGE
allow-specific   deployment=hello   16s
deny-all         <none>             6m10s
[student@workstation network-policy]$ oc login -u admin -p redhat
Login successful.

You have access to 61 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "network-test".
[student@workstation network-policy]$ oc label namespace network-test name=network-test
namespace/network-test labeled
[student@workstation network-policy]$ oc describe namespace network-test
Name:         network-test
Labels:       name=network-test
Annotations:  openshift.io/description: 
              openshift.io/display-name: 
              openshift.io/requester: developer
              openshift.io/sa.scc.mcs: s0:c25,c5
              openshift.io/sa.scc.supplemental-groups: 1000610000/10000
              openshift.io/sa.scc.uid-range: 1000610000/10000
Status:       Active

No resource quota.

No LimitRange resource.
[student@workstation network-policy]$ oc rsh sample-app-d5f945-kn6hx curl 10.9.0.16:8080
<html>
  <body>
    <h1>Hello, world from nginx!</h1>
  </body>
</html>
[student@workstation network-policy]$ oc rsh sample-app-d5f945-kn6hx curl 10.9.0.16:8081
^Ccommand terminated with exit code 130
[student@workstation network-policy]$ oc rsh sample-app-d5f945-kn6hx curl 10.9.0.17:8080
^Ccommand terminated with exit code 130
[student@workstation network-policy]$ cat allow-from-openshift-ingress.yaml 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-from-openshift-ingress
spec:
  podSelector: CHANGE_ME
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          CHANGE_ME
[student@workstation network-policy]$ vi allow-from-openshift-ingress.yaml 
[student@workstation network-policy]$ cat allow-from-openshift-ingress.yaml 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-from-openshift-ingress
spec:
  podSelector: CHANGE_ME
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
[student@workstation network-policy]$ oc create -n network-policy -f allow-from-openshift-ingress.yaml
Error from server (BadRequest): error when creating "allow-from-openshift-ingress.yaml": NetworkPolicy in version "v1" cannot be handled as a NetworkPolicy: v1.NetworkPolicy.Spec: v1.NetworkPolicySpec.PodSelector: readObjectStart: expect { or n, but found ", error found in #10 byte of ...|elector":"CHANGE_ME"|..., bigger context ...|t.io/policy-group":"ingress"}}}]}],"podSelector":"CHANGE_ME"}}
|...
[student@workstation network-policy]$ vi allow-from-openshift-ingress.yaml 
[student@workstation network-policy]$ cat allow-from-openshift-ingress.yaml 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-from-openshift-ingress
spec:
  podSelector: {} 
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
[student@workstation network-policy]$ oc create -n network-policy -f allow-from-openshift-ingress.yaml
networkpolicy.networking.k8s.io/allow-from-openshift-ingress created
[student@workstation network-policy]$ oc get networkpolicies -n network-policy
NAME                           POD-SELECTOR       AGE
allow-from-openshift-ingress   <none>             25s
allow-specific                 deployment=hello   5m55s
deny-all                       <none>             11m
[student@workstation network-policy]$ oc login -u admin -p redhat
Login successful.

You have access to 61 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "network-test".
[student@workstation network-policy]$ oc label namespace default network.openshift.io/policy-group=ingress
namespace/default labeled
[student@workstation network-policy]$ curl -s hello-network-policy.apps.ocp4.example.com | grep Hello
    <h1>Hello, world from nginx!</h1>
[student@workstation network-policy]$ oc projects
You have access to the following projects and can switch between them with 'oc project <projectname>':

    authorization-review
    default
    kube-node-lease
    kube-public
    kube-system
    network-policy
  * network-test
    nfs-client-provisioner - NFS Provisioner
    openshift
    openshift-apiserver
    openshift-apiserver-operator
    openshift-authentication
    openshift-authentication-operator
    openshift-cloud-credential-operator
    openshift-cluster-machine-approver
    openshift-cluster-node-tuning-operator
    openshift-cluster-samples-operator
    openshift-cluster-storage-operator
    openshift-cluster-version
    openshift-config
    openshift-config-managed
    openshift-config-operator
    openshift-console
    openshift-console-operator
    openshift-controller-manager
    openshift-controller-manager-operator
    openshift-dns
    openshift-dns-operator
    openshift-etcd
    openshift-etcd-operator
    openshift-image-registry
    openshift-infra
    openshift-ingress
    openshift-ingress-operator
    openshift-insights
    openshift-kni-infra
    openshift-kube-apiserver
    openshift-kube-apiserver-operator
    openshift-kube-controller-manager
    openshift-kube-controller-manager-operator
    openshift-kube-scheduler
    openshift-kube-scheduler-operator
    openshift-kube-storage-version-migrator
    openshift-kube-storage-version-migrator-operator
    openshift-machine-api
    openshift-machine-config-operator
    openshift-marketplace
    openshift-monitoring
    openshift-multus
    openshift-network-operator
    openshift-node
    openshift-openstack-infra
    openshift-operator-lifecycle-manager
    openshift-operators
    openshift-ovirt-infra
    openshift-sdn
    openshift-service-ca
    openshift-service-ca-operator
    openshift-service-catalog-removed
    openshift-user-workload-monitoring
    openshift-vsphere-infra

Using project "network-test" on server "https://api.ocp4.example.com:6443".
[student@workstation network-policy]$ cd
[student@workstation ~]$ lab network-policy finish

Completing Guided Exercise: Configuring Network Policies

 · Delete OpenShift project 'network-policy'...................  SUCCESS
 · Wait for project 'network-policy' to be gone................  SUCCESS
 · Delete OpenShift project 'network-test'.....................  SUCCESS
 · Wait for project 'network-test' to be gone..................  SUCCESS
 · Remove network.openshift.io/policy-group=ingress label from 
   the default project.........................................  SUCCESS
 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 


------------------------------------------------------------------------------------------------------------------
Chapter 5 | Configuring OpenShift Networking for Applications
Lab Solution
Chapter 5 | Configuring OpenShift Networking for Applications Page: 165

-----------------------------------------------------------------------------------------------------------------------
In this lab, you will configure a TLS passthrough route for your application.
Outcomes
You should be able to:
• Deploy an application and configure an insecure route.
• Restrict traffic to the applications.
• Generate a TLS certificate for an application.
• Configure a passthrough route for an application with a TLS certificate.


[student@workstation ~]$ lab network-review start

Checking prerequisites for Lab: Configuring OpenShift Networking for Applications

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'network-review' project is absent......................  SUCCESS

Setting up the classroom for Lab: Configuring OpenShift Networking for Applications

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS
 Preparing Workstation:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS
 Configuring Certificates:
 · Generating unique CA key password...........................  SUCCESS
 · Setting environment variable in cert. configuration file....  SUCCESS
 · Generating the CA key.......................................  SUCCESS
 · Generating CA certificate...................................  SUCCESS
 · Creating private key........................................  SUCCESS
 · Updating privileges on certs directory......................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You have one project on this server: "authorization-review"

Using project "authorization-review".
[student@workstation ~]$ oc new-project network-review
Now using project "network-review" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ cd ~/DO280/labs/network-review/
[student@workstation network-review]$ vi php-http.yaml 
[student@workstation network-review]$ cat php-http.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-http
  labels:
    app: php-http
    name: php-http
  namespace: network-review
spec:
  replicas: 1
  selector:
    matchLabels:
      app: php-http
      name: php-http
  template:
    metadata:
      labels:
        app: php-http
        name: php-http
    spec:
      containers:
      - resources:
          limits:
            memory: "128Mi"
            cpu: '0.5'
        # Set the container image name
        image: 'quay.io/redhattraaaaining/php-ssl:v1.0'
        name: php-http
        ports:
          # Define the container port
        - containerPort: 8080
          name: php-http
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: php-http
    name: php-http
  name: php-http
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    name: php-http
[student@workstation network-review]$ oc create -f php-http.yaml
deployment.apps/php-http created
service/php-http created
[student@workstation network-review]$ oc get pods
NAME                       READY   STATUS             RESTARTS   AGE
php-http-8945f7d44-7cjc5   0/1     ImagePullBackOff   0          5m8s
[student@workstation network-review]$ oc edit deployment/php-http
deployment.apps/php-http edited
[student@workstation network-review]$ oc status
In project network-review on server https://api.ocp4.example.com:6443

svc/php-http - 172.30.3.119:80 -> 8080
  deployment/php-http deploys quay.io/redhattraining/php-ssl:v1.0
    deployment #2 running for 14 seconds - 1 pod
    deployment #1 deployed 6 minutes ago


1 info identified, use 'oc status --suggest' to see details.
[student@workstation network-review]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
php-http-7585bf95c9-gshpj   1/1     Running   0          21s
[student@workstation network-review]$ oc expose svc php-http --hostname php-http.apps.ocp4.example.com
route.route.openshift.io/php-http exposed
[student@workstation network-review]$ oc get routes
NAME       HOST/PORT                        PATH   SERVICES   PORT   TERMINATION   WILDCARD
php-http   php-http.apps.ocp4.example.com          php-http   8080                 None
[student@workstation network-review]$ ls
allow-from-openshift-ingress.yaml  certs  deny-all.yaml  php-https.yaml  php-http.yaml
[student@workstation network-review]$ vi deny-all.yaml 
[student@workstation network-review]$ cat deny-all.yaml 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-all
spec:
  podSelector: {}      
[student@workstation network-review]$ oc create -f deny-all.yaml
networkpolicy.networking.k8s.io/deny-all created
[student@workstation network-review]$ curl http://php-http.apps.ocp4.example.com

<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <title>PHP Application</title>
  </head>
  <body>
    <h2><strong>About this application</strong></h2>
            <i class="fa fa-exclamation-triangle"/><span style="color: #993300;"><strong>
          The application is currently server over HTTP        </span></strong>
          <ul>
      <li>
        <strong>Current system load:</strong> 28      </li>
      <li>
        <strong>Number of connections:</strong> 1      </li>
      <li>
        <strong>Memory usage:</strong> 27 Mb      </li>
    </ul>
  </body>
</html>[student@workstation network-review]$ oc get networkpolicy
NAME       POD-SELECTOR   AGE
deny-all   <none>         3m53s
[student@workstation network-review]$ oc get networkpolicy
NAME       POD-SELECTOR   AGE
deny-all   <none>         5m
[student@workstation network-review]$ curl http://php-http.apps.ocp4.example.com
^C
[student@workstation network-review]$ vi allow-from-openshift-ingress.yaml 
[student@workstation network-review]$ cat allow-from-openshift-ingress.yaml 
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
[student@workstation network-review]$ oc create -f allow-from-openshift-ingress.yaml 
networkpolicy.networking.k8s.io/allow-from-openshift-ingress created
[student@workstation network-review]$ oc login -u admin -p redhat
Login successful.

You have access to 60 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "network-review".
[student@workstation network-review]$ oc label namespace default network.openshift.io/policy-group=ingress
namespace/default labeled
[student@workstation network-review]$ for X in {1..4};do curl -s http://php-http.apps.ocp4.example.com | grep "PHP";done
    <title>PHP Application</title>
    <title>PHP Application</title>
    <title>PHP Application</title>
    <title>PHP Application</title>
[student@workstation network-review]$ oc login -u developer -p developer
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    authorization-review
  * network-review

Using project "network-review".
[student@workstation network-review]$ cd ~/DO280/labs/network-review/certs
[student@workstation certs]$ openssl req -new -key training.key -subj "/C=US/ST=North Carolina/L=Raleigh/O=Red Hat/CN=php-https.apps.ocp4.example.com" -out training.csr
[student@workstation certs]$ openssl x509 -req -in training.csr -CA training-CA.pem -CAkey training-CA.key -CAcreateserial -passin file:passphrase.txt -out training.crt -days 3650 -sha256 -extfile training.ext
Signature ok
subject=C = US, ST = North Carolina, L = Raleigh, O = Red Hat, CN = php-https.apps.ocp4.example.com
Getting CA Private Key
[student@workstation certs]$ ls
openssl-commands.txt  training-CA.key  training-CA.srl  training.csr  training.key
passphrase.txt        training-CA.pem  training.crt     training.ext
[student@workstation certs]$ ls -lhrt
total 36K
-rw-rw-r--. 1 student student  566 Jul 15 06:59 openssl-commands.txt
-rw-rw-r--. 1 student student   33 Aug  9 06:14 passphrase.txt
-rw-r--r--. 1 student student  352 Aug  9 06:14 training.ext
-rw-------. 1 student student 1.8K Aug  9 06:14 training-CA.key
-rw-r--r--. 1 student student 1.4K Aug  9 06:14 training-CA.pem
-rw-------. 1 student student 1.7K Aug  9 06:14 training.key
-rw-rw-r--. 1 student student 1021 Aug  9 06:39 training.csr
-rw-rw-r--. 1 student student   41 Aug  9 06:39 training-CA.srl
-rw-rw-r--. 1 student student 1.4K Aug  9 06:39 training.crt
[student@workstation certs]$ cd ..
[student@workstation network-review]$ oc create secret tls php-certs --cert certs/training.crt --key certs/training.key
secret/php-certs created
[student@workstation network-review]$ oc get secrets
NAME                       TYPE                                  DATA   AGE
builder-dockercfg-sk77f    kubernetes.io/dockercfg               1      25m
builder-token-4h6b9        kubernetes.io/service-account-token   4      25m
builder-token-8rwjl        kubernetes.io/service-account-token   4      25m
default-dockercfg-hg6qm    kubernetes.io/dockercfg               1      25m
default-token-6kbpn        kubernetes.io/service-account-token   4      25m
default-token-hmgwp        kubernetes.io/service-account-token   4      25m
deployer-dockercfg-nw94l   kubernetes.io/dockercfg               1      25m
deployer-token-gtjmq       kubernetes.io/service-account-token   4      25m
deployer-token-mrxpl       kubernetes.io/service-account-token   4      25m
php-certs                  kubernetes.io/tls                     2      10s
[student@workstation network-review]$ vi php-https.yaml 
[student@workstation network-review]$ cat php-https.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-https
  labels:
    app: php-https
    name: php-https
  namespace: network-review
spec:
  replicas: 1
  selector:
    matchLabels:
      app: php-https
      name: php-https
  template:
    metadata:
      labels:
        app: php-https
        name: php-https
    spec:
      containers:
      - resources:
          limits:
            memory: "128Mi"
            cpu: '0.5'
        # Set the container image name
        image: 'quay.io/redhattraining/php-ssl:v1.1'
        name: php-https
        ports:
          # Define the container port
        - containerPort: 8443
          name: php-https
        volumeMounts:
        - name: tls-certs
          readOnly: true
          mountPath: /usr/local/etc/ssl/certs
      volumes:
      - name: tls-certs
        secret:
          # Define the name the secret
          secretName: php-certs
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: php-https
    name: php-https
  name: php-https
spec:
  ports:
  - port: 443
    protocol: TCP
    targetPort: 8443
  selector:
    name: php-https
[student@workstation network-review]$ oc create -f php-https.yaml
deployment.apps/php-https created
service/php-https created
[student@workstation network-review]$ oc get pods
NAME                         READY   STATUS              RESTARTS   AGE
php-http-7585bf95c9-gshpj    1/1     Running             0          17m
php-https-5df75876c5-m8glk   0/1     ContainerCreating   0          10s
[student@workstation network-review]$ oc get pods
NAME                         READY   STATUS    RESTARTS   AGE
php-http-7585bf95c9-gshpj    1/1     Running   0          19m
php-https-5df75876c5-m8glk   1/1     Running   0          2m8s
[student@workstation network-review]$ oc create route passthrough php-https --service php-https --port 8443 --hostname php-https.apps.ocp4.example.com
route.route.openshift.io/php-https created
[student@workstation network-review]$ oc get routes
NAME        HOST/PORT                         PATH   SERVICES    PORT   TERMINATION   WILDCARD
php-http    php-http.apps.ocp4.example.com           php-http    8080                 None
php-https   php-https.apps.ocp4.example.com          php-https   8443   passthrough   None
[student@workstation network-review]$ curl -v --cacert certs/training-CA.pem https://php-https.apps.ocp4.example.com
* Rebuilt URL to: https://php-https.apps.ocp4.example.com/
*   Trying 192.168.50.254...
* TCP_NODELAY set
* Connected to php-https.apps.ocp4.example.com (192.168.50.254) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: certs/training-CA.pem
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, [no content] (0):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, [no content] (0):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, [no content] (0):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, [no content] (0):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, [no content] (0):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
* ALPN, server accepted to use http/1.1
* Server certificate:
*  subject: C=US; ST=North Carolina; L=Raleigh; O=Red Hat; CN=php-https.apps.ocp4.example.com
*  start date: Aug  9 10:39:31 2021 GMT
*  expire date: Aug  7 10:39:31 2031 GMT
*  subjectAltName: host "php-https.apps.ocp4.example.com" matched cert's "*.apps.ocp4.example.com"
*  issuer: C=US; ST=North Carolina; L=Raleigh; O=Red Hat; CN=ocp4.example.com
*  SSL certificate verify ok.
* TLSv1.3 (OUT), TLS app data, [no content] (0):
> GET / HTTP/1.1
> Host: php-https.apps.ocp4.example.com
> User-Agent: curl/7.61.1
> Accept: */*
> 
* TLSv1.3 (IN), TLS handshake, [no content] (0):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, [no content] (0):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS app data, [no content] (0):
< HTTP/1.1 200 OK
< Date: Mon, 09 Aug 2021 10:47:51 GMT
< Server: Apache/2.4.37 (Red Hat Enterprise Linux) OpenSSL/1.1.1
< X-Powered-By: PHP/7.2.11
< Transfer-Encoding: chunked
< Content-Type: text/html; charset=UTF-8
< 

<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <title>PHP Application</title>
  </head>
  <body>
    <h2><strong>About this application</strong></h2>
            <i class="fa fa-lock"/><span style="color: #339966;"><strong>
          The application is currently served over TLS        </span></strong>
          <ul>
      <li>
        <strong>Current system load:</strong> 169.5      </li>
      <li>
        <strong>Number of connections:</strong> 0      </li>
      <li>
        <strong>Memory usage:</strong> 57 Mb      </li>
    </ul>
  </body>
* Connection #0 to host php-https.apps.ocp4.example.com left intact
</html>[student@workstation network-review]$ cd
[student@workstation ~]$ lab network-review grade

Grading the student's work for Lab: Configuring OpenShift Networking for Applications

 · network-review is present...................................  PASS
 · php-http deployment is present..............................  PASS
 · php-http deployment points to v1.0 of the application.......  PASS
 · php-http deployment uses port 8080..........................  PASS
 · php-http container is running...............................  PASS
 · Route for php-http is present...............................  PASS
 · Route for php-http is accessible............................  PASS
 · SSL certificate is present..................................  PASS
 · SSL certificate matches required CN.........................  PASS
 · TLS secret is present.......................................  PASS
 · php-https deployment is present.............................  PASS
 · php-https deployment points to v1.1 of the application......  PASS
 · php-https deployment uses port 8443.........................  PASS
 · php-https container is running..............................  PASS
 · Route for php-https is present..............................  PASS
 · Route for php-https is accessible using TLS CA..............  PASS
 · Network policy denies pod to pod traffic....................  PASS
 · Default namespace is labeled with network.openshift.io/polic
   y-group=ingress.............................................  PASS

Overall exercise grade.........................................  PASS

[student@workstation ~]$ lab network-review finish

Completing Lab: Configuring OpenShift Networking for Applications

 · Delete OpenShift project 'network-review'...................  SUCCESS
 · Wait for project 'network-review' to be gone................  SUCCESS
 · Remove network.openshift.io/policy-group=ingress label from 
   the default project.........................................  SUCCESS
 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 


Summary
In this chapter, you learned:
• OpenShift implements a software-defined networking (SDN) to manage the network
infrastructure of the cluster. SDN decouples the software that handles the traffic from the
underlying mechanisms that route the traffic.
• Kubernetes provides services that allow the logical grouping of pods under a common access
route. Services act as load balancers in front of one or more pods.
• Services use selectors (labels) that indicate which pods available to the service.
• There are two kind of routes: secure, and insecure. Secure routes encrypt the traffic using TLS
certificates, and insecure routes forward traffic over an unencrypted connection.
Secure routes support three modes: edge, passthrough, and re-encryption.
• Network policies control network traffic to pods. Logical zones can be created in the SDN to
segregate traffic among pods in any namespace.

---------------------------------------------------------------------------------------------------------------
Chapter 6 | Controlling Pod Scheduling
Guided Exercise
Controlling Pod Scheduling Behavior Page: 185
---------------------------------------------------------------------------------------------------------------------
In this exercise, you will configure an application to run on a subset of the cluster worker
nodes.
Outcomes
You should be able to use the OpenShift command-line interface to:
• Add a new label to a node.
• Deploy pods to nodes that match a specified label.
• Remove a label from a node.
• Troubleshoot when pods fail to deploy to a node.


[student@workstation ~]$ lab schedule-pods start

Checking prerequisites for Guided Exercise: Controlling Pod Scheduling Behavior

 Verify the OpenShift cluster is running:
 · Waiting up to 5 minutes for router pods to be available.....  SUCCESS
 · Waiting up to 5 minutes for OAuth to be available...........  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS

 Checking for conflicts with existing OpenShift projects:
 · The 'schedule-pods' project is absent.......................  SUCCESS
 · The 'schedule-pods-ts' project is absent....................  SUCCESS

Setting up the classroom for Guided Exercise: Controlling Pod Scheduling Behavior

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS

 Preparing the student's cluster:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS
 · Label the first worker node with 'client=ACME'..............  SUCCESS
 · Create project 'schedule-pods-ts' for troubleshooting.......  SUCCESS
 · Assign 'edit' role to 'developer' on 'schedule-pods-ts'.....  SUCCESS
 · Deploy 'hello-ts' application to 'schedule-pods-ts'.........  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * authorization-review
    schedule-pods-ts

Using project "authorization-review".
[student@workstation ~]$ oc new-project schedule-pods
Now using project "schedule-pods" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc new-app --name hello --docker-image quay.io/redhattraining/hello-world-nginx:v1.0
--> Found container image 44eaa13 (2 years old) from quay.io for "quay.io/redhattraining/hello-world-nginx:v1.0"

    Red Hat Universal Base Image 8 
    ------------------------------ 
    The Universal Base Image is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.

    Tags: base rhel8

    * An image stream tag will be created as "hello:v1.0" that will track this image

--> Creating resources ...
    imagestream.image.openshift.io "hello" created
    deployment.apps "hello" created
    service "hello" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/hello' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc expose svc/hello
route.route.openshift.io/hello exposed
[student@workstation ~]$ oc scale --replicas 4 deployment/hello
deployment.apps/hello scaled
[student@workstation ~]$ oc get pods -o wide
NAME                     READY   STATUS              RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
hello-6c4984d949-6rjd4   1/1     Running             0          13s   10.10.0.40   master01   <none>           <none>
hello-6c4984d949-7fc87   1/1     Running             0          13s   10.10.0.39   master01   <none>           <none>
hello-6c4984d949-bk25n   1/1     Running             0          48s   10.9.0.13    master03   <none>           <none>
hello-6c4984d949-rmfcw   0/1     ContainerCreating   0          13s   <none>       master02   <none>           <none>
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 61 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "schedule-pods".
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 61 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "schedule-pods".
[student@workstation ~]$ oc get nodes -L env
NAME       STATUS   ROLES           AGE    VERSION           ENV
master01   Ready    master,worker   368d   v1.18.3+012b3ec   
master02   Ready    master,worker   368d   v1.18.3+012b3ec   
master03   Ready    master,worker   368d   v1.18.3+012b3ec   
[student@workstation ~]$ oc label node master01 env=dev
node/master01 labeled
[student@workstation ~]$ oc label node master02 env=prod
node/master02 labeled
[student@workstation ~]$ oc get nodes -L env
NAME       STATUS   ROLES           AGE    VERSION           ENV
master01   Ready    master,worker   368d   v1.18.3+012b3ec   dev
master02   Ready    master,worker   368d   v1.18.3+012b3ec   prod
master03   Ready    master,worker   368d   v1.18.3+012b3ec   
[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    authorization-review
  * schedule-pods
    schedule-pods-ts

Using project "schedule-pods".
[student@workstation ~]$ oc edit deployment/hello
deployment.apps/hello edited
[student@workstation ~]$ oc get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
hello-b556ccf8b-6mx4c   1/1     Running   0          19s   10.10.0.42   master01   <none>           <none>
hello-b556ccf8b-8vh4k   1/1     Running   0          17s   10.10.0.43   master01   <none>           <none>
hello-b556ccf8b-crz9f   1/1     Running   0          16s   10.10.0.44   master01   <none>           <none>
hello-b556ccf8b-mtvtf   1/1     Running   0          20s   10.10.0.41   master01   <none>           <none>
[student@workstation ~]$ oc delete project schedule-pods
project.project.openshift.io "schedule-pods" deleted
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 61 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "schedule-pods".
[student@workstation ~]$ oc label node -l env env-
node/master01 labeled
node/master02 labeled
[student@workstation ~]$ oc label node -l env env-
[student@workstation ~]$ oc project schedule-pods-ts
Now using project "schedule-pods-ts" on server "https://api.ocp4.example.com:6443".
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
hello-ts-6bc96f8664-gx8dm   0/1     Pending   0          7m12s
[student@workstation ~]$ oc describe pod hello-ts-6bc96f8664-gx8dm
Name:           hello-ts-6bc96f8664-gx8dm
Namespace:      schedule-pods-ts
Priority:       0
Node:           <none>
Labels:         app=hello-ts
                pod-template-hash=6bc96f8664
Annotations:    openshift.io/scc: restricted
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/hello-ts-6bc96f8664
Containers:
  hello-world-nginx:
    Image:        quay.io/redhattraining/hello-world-nginx:v1.0
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sxl7f (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  default-token-sxl7f:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-sxl7f
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  client=acme
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age                   From               Message
  ----     ------            ----                  ----               -------
  Warning  FailedScheduling  62s (x16 over 7m32s)  default-scheduler  0/3 nodes are available: 3 node(s) didn't match node selector.
[student@workstation ~]$ oc edit deployment/hello-ts
deployment.apps/hello-ts edited
[student@workstation ~]$ oc status
In project schedule-pods-ts on server https://api.ocp4.example.com:6443

deployment/hello-ts deploys quay.io/redhattraining/hello-world-nginx:v1.0
  deployment #2 running for 16 seconds - 1 pod
  deployment #1 deployed 9 minutes ago


1 info identified, use 'oc status --suggest' to see details.
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
hello-ts-7b66d48c8d-mphm8   1/1     Running   0          22s
[student@workstation ~]$ oc describe pod hello-ts-7b66d48c8d-mphm8 | grep -i client
Node-Selectors:  client=ACME
[student@workstation ~]$ lab schedule-pods finish

Completing Guided Exercise: Controlling Pod Scheduling Behavior

 · Delete OpenShift project 'schedule-pods-ts'.................  SUCCESS
 · Wait for project 'schedule-pods-ts' to be gone..............  SUCCESS
 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS
 · Remove 'client' label from worker nodes.....................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

--------------------------------------------------------------------------------------------------------------------------
Chapter 6 | Controlling Pod Scheduling
Guided Exercise
Limiting Resource Usage by an Application Page: 201
---------------------------------------------------------------------------------------------------------------------------
In this exercise, you will configure an application so that it does not consume all computing
resources from the cluster and its worker nodes.
Outcomes
You should be able to use the OpenShift command-line interface to:
• Configure an application to specify resource requests for CPU and memory usage.
• Modify an application to work within existing cluster restrictions.
• Create a quota to limit the total amount of CPU, memory, and configuration maps
available to a project.


[student@workstation ~]$ lab schedule-limit start

Checking prerequisites for Guided Exercise: Limiting Resource Usage by an Application

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS

 Checking for conflicts with existing OpenShift projects:
 · The 'schedule-limit' project is absent......................  SUCCESS
 Checking for conflicts with existing OpenShift projects:
 · The 'template-test' project is absent.......................  SUCCESS

Setting up the classroom for Guided Exercise: Limiting Resource Usage by an Application

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS

 Preparing the student's cluster:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You have one project on this server: "authorization-review"

Using project "authorization-review".
[student@workstation ~]$ oc new-project schedule-limit
Now using project "schedule-limit" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc create deployment hello-limit --image quay.io/redhattraining/hello-world-nginx:v1.0 --dry-run=client -o yaml > ~/DO280/labs/schedule-limit/hello-limit.yaml
[student@workstation ~]$ ^C
[student@workstation ~]$ vi ~/DO280/labs/schedule-limit/hello-limit.yaml
[student@workstation ~]$ oc create --save-config -f ~/DO280/labs/schedule-limit/hello-limit.yaml
deployment.apps/hello-limit created
[student@workstation ~]$ oc get pods
NAME                          READY   STATUS    RESTARTS   AGE
hello-limit-6dcc899b8-9tncr   1/1     Running   0          24s
[student@workstation ~]$ oc get events --field-selector type=Warning
No resources found in schedule-limit namespace.
[student@workstation ~]$ oc get pods
NAME                          READY   STATUS    RESTARTS   AGE
hello-limit-6dcc899b8-9tncr   1/1     Running   0          67s
[student@workstation ~]$ oc describe pod hello-limit-6dcc899b8-9tncr | less
[student@workstation ~]$ vi ~/DO280/labs/schedule-limit/hello-limit.yaml
[student@workstation ~]$ oc describe deployment/hello-limit | less
[student@workstation ~]$ vi ~/DO280/labs/schedule-limit/hello-limit.yaml
[student@workstation ~]$ oc delete deployment/hello-limit
deployment.apps "hello-limit" deleted
[student@workstation ~]$ oc create --save-config -f ~/DO280/labs/schedule-limit/hello-limit.yaml
deployment.apps/hello-limit created
[student@workstation ~]$ oc get pods
NAME                           READY   STATUS    RESTARTS   AGE
hello-limit-5cc86ff6b8-42txd   0/1     Pending   0          9s
[student@workstation ~]$ cat ~/DO280/labs/schedule-limit/hello-limit.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: hello-limit
  name: hello-limit
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-limit
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hello-limit
    spec:
      containers:
      - image: quay.io/redhattraining/hello-world-nginx:v1.0
        name: hello-world-nginx
        resources: 
          requests:
            cpu: "3"
            memory: 20Mi
status: {}
[student@workstation ~]$ oc describe deployment/hello-limit 
Name:                   hello-limit
Namespace:              schedule-limit
CreationTimestamp:      Mon, 09 Aug 2021 22:05:11 -0400
Labels:                 app=hello-limit
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=hello-limit
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=hello-limit
  Containers:
   hello-world-nginx:
    Image:      quay.io/redhattraining/hello-world-nginx:v1.0
    Port:       <none>
    Host Port:  <none>
    Requests:
      cpu:        3
      memory:     20Mi
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   hello-limit-5cc86ff6b8 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  37s   deployment-controller  Scaled up replica set hello-limit-5cc86ff6b8 to 1
[student@workstation ~]$ oc get events --field-selector type=Warning
LAST SEEN   TYPE      REASON             OBJECT                             MESSAGE
<unknown>   Warning   FailedScheduling   pod/hello-limit-5cc86ff6b8-42txd   0/3 nodes are available: 3 Insufficient cpu.
<unknown>   Warning   FailedScheduling   pod/hello-limit-5cc86ff6b8-42txd   0/3 nodes are available: 3 Insufficient cpu.
[student@workstation ~]$ cat ~/DO280/labs/schedule-limit/hello-limit.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: hello-limit
  name: hello-limit
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-limit
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: hello-limit
    spec:
      containers:
      - image: quay.io/redhattraining/hello-world-nginx:v1.0
        name: hello-world-nginx
        resources: 
          requests:
            cpu: "3"
            memory: 20Mi
status: {}
[student@workstation ~]$ vi ~/DO280/labs/schedule-limit/hello-limit.yaml
[student@workstation ~]$ grep -i cpu !$
grep -i cpu ~/DO280/labs/schedule-limit/hello-limit.yaml
            cpu: "1500m"
[student@workstation ~]$ oc apply -f ~/DO280/labs/schedule-limit/hello-limit.yaml
deployment.apps/hello-limit configured
[student@workstation ~]$ oc get pods
NAME                         READY   STATUS    RESTARTS   AGE
hello-limit-d55cd65c-mlthf   1/1     Running   0          8s
[student@workstation ~]$ oc scale --replicas 4 deployment/hello-limit
deployment.apps/hello-limit scaled
[student@workstation ~]$ oc get pods
NAME                         READY   STATUS    RESTARTS   AGE
hello-limit-d55cd65c-jlh9m   0/1     Pending   0          22s
hello-limit-d55cd65c-m5s8c   0/1     Pending   0          22s
hello-limit-d55cd65c-mlthf   1/1     Running   0          52s
hello-limit-d55cd65c-vwmmk   0/1     Pending   0          22s
[student@workstation ~]$ oc get events --field-selector type=Warning
LAST SEEN   TYPE      REASON             OBJECT                             MESSAGE
<unknown>   Warning   FailedScheduling   pod/hello-limit-5cc86ff6b8-42txd   0/3 nodes are available: 3 Insufficient cpu.
<unknown>   Warning   FailedScheduling   pod/hello-limit-5cc86ff6b8-42txd   0/3 nodes are available: 3 Insufficient cpu.
<unknown>   Warning   FailedScheduling   pod/hello-limit-5cc86ff6b8-42txd   skip schedule deleting pod: schedule-limit/hello-limit-5cc86ff6b8-42txd
<unknown>   Warning   FailedScheduling   pod/hello-limit-d55cd65c-jlh9m     0/3 nodes are available: 3 Insufficient cpu.
<unknown>   Warning   FailedScheduling   pod/hello-limit-d55cd65c-jlh9m     0/3 nodes are available: 3 Insufficient cpu.
<unknown>   Warning   FailedScheduling   pod/hello-limit-d55cd65c-m5s8c     0/3 nodes are available: 3 Insufficient cpu.
<unknown>   Warning   FailedScheduling   pod/hello-limit-d55cd65c-m5s8c     0/3 nodes are available: 3 Insufficient cpu.
<unknown>   Warning   FailedScheduling   pod/hello-limit-d55cd65c-vwmmk     0/3 nodes are available: 3 Insufficient cpu.
<unknown>   Warning   FailedScheduling   pod/hello-limit-d55cd65c-vwmmk     0/3 nodes are available: 3 Insufficient cpu.
[student@workstation ~]$ oc delete all -l app=hello-limit
pod "hello-limit-d55cd65c-jlh9m" deleted
pod "hello-limit-d55cd65c-m5s8c" deleted
pod "hello-limit-d55cd65c-mlthf" deleted
pod "hello-limit-d55cd65c-vwmmk" deleted
deployment.apps "hello-limit" deleted
replicaset.apps "hello-limit-5cc86ff6b8" deleted
[student@workstation ~]$ oc create --save-config -f ~/DO280/labs/schedule-limit/loadtest.yaml
deployment.apps/loadtest created
service/loadtest created
route.route.openshift.io/loadtest created
[student@workstation ~]$ oc get routes
NAME       HOST/PORT                        PATH   SERVICES   PORT   TERMINATION   WILDCARD
loadtest   loadtest.apps.ocp4.example.com          loadtest   8080                 None
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
loadtest-58d967ffbb-xkrqt   1/1     Running   0          28s
[student@workstation ~]$ oc adm top pod
error: metrics not available yet
[student@workstation ~]$ oc adm top pod
NAME                        CPU(cores)   MEMORY(bytes)   
loadtest-58d967ffbb-xkrqt   0m           21Mi            
[student@workstation ~]$ curl -X GET http://loadtest.apps.ocp4.example.com/api/loadtest/v1/mem/150/60
curl: (52) Empty reply from server
[student@workstation ~]$ oc adm top pod
NAME                        CPU(cores)   MEMORY(bytes)   
loadtest-58d967ffbb-xkrqt   0m           173Mi           
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
loadtest-58d967ffbb-xkrqt   1/1     Running   0          3m49s
[student@workstation ~]$ oc adm top pod
NAME                        CPU(cores)   MEMORY(bytes)   
loadtest-58d967ffbb-xkrqt   0m           22Mi            
[student@workstation ~]$ curl -X GET http://loadtest.apps.ocp4.example.com/api/loadtest/v1/mem/200/60
<html><body><h1>502 Bad Gateway</h1>
The server returned an invalid or incomplete response.
</body></html>
[student@workstation ~]$ curl -X GET http://loadtest.apps.ocp4.example.com/api/loadtest/v1/mem/200/60
<html><body><h1>502 Bad Gateway</h1>
The server returned an invalid or incomplete response.
</body></html>
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS      RESTARTS   AGE
loadtest-58d967ffbb-xkrqt   0/1     OOMKilled   1          20m
[student@workstation ~]$ oc adm top pod
NAME                        CPU(cores)   MEMORY(bytes)   
loadtest-58d967ffbb-xkrqt   9m           21Mi            
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
loadtest-58d967ffbb-xkrqt   1/1     Running   2          21m
[student@workstation ~]$ oc delete all -l app=loadtest
pod "loadtest-58d967ffbb-xkrqt" deleted
service "loadtest" deleted
deployment.apps "loadtest" deleted
replicaset.apps "loadtest-58d967ffbb" deleted
route.route.openshift.io "loadtest" deleted
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 60 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "schedule-limit".
[student@workstation ~]$ oc create quota project-quota --hard cpu="3",memory="1G",configmaps="3" -n schedule-limit
resourcequota/project-quota created
[student@workstation ~]$ oc login -u developer -p developer
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    authorization-review
  * schedule-limit

Using project "schedule-limit".
[student@workstation ~]$ for X in {1..4};do oc create configmap my-config${X} --from-literal key${X}=value${X};done
configmap/my-config1 created
configmap/my-config2 created
configmap/my-config3 created
Error from server (Forbidden): configmaps "my-config4" is forbidden: exceeded quota: project-quota, requested: configmaps=1, used: configmaps=3, limited: configmaps=3
[student@workstation ~]$ oc login -u admin -p redhat
Login successful.

You have access to 60 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "schedule-limit".
[student@workstation ~]$ oc adm create-bootstrap-project-template -o yaml > /tmp/project-template.yaml
[student@workstation ~]$ vi /home/student/DO280/labs/schedule-limit/quota-limits.yaml
[student@workstation ~]$ vi /tmp/project-template.yaml 
[student@workstation ~]$ cat !$
cat /tmp/project-template.yaml
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  creationTimestamp: null
  name: project-request
objects:
- apiVersion: project.openshift.io/v1
  kind: Project
  metadata:
    annotations:
      openshift.io/description: ${PROJECT_DESCRIPTION}
      openshift.io/display-name: ${PROJECT_DISPLAYNAME}
      openshift.io/requester: ${PROJECT_REQUESTING_USER}
    creationTimestamp: null
    name: ${PROJECT_NAME}
  spec: {}
  status: {}
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    creationTimestamp: null
    name: admin
    namespace: ${PROJECT_NAME}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: admin
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: ${PROJECT_ADMIN_USER}
parameters:
- name: PROJECT_NAME
- name: PROJECT_DISPLAYNAME
- name: PROJECT_DESCRIPTION
- name: PROJECT_ADMIN_USER
- name: PROJECT_REQUESTING_USER
[student@workstation ~]$ cat /home/student/DO280/solutions/schedule-limit/project-template.yaml
apiVersion: template.openshift.io/v1
kind: Template
metadata:
  creationTimestamp: null
  name: project-request
objects:
- apiVersion: project.openshift.io/v1
  kind: Project
  metadata:
    annotations:
      openshift.io/description: ${PROJECT_DESCRIPTION}
      openshift.io/display-name: ${PROJECT_DISPLAYNAME}
      openshift.io/requester: ${PROJECT_REQUESTING_USER}
    creationTimestamp: null
    name: ${PROJECT_NAME}
  spec: {}
  status: {}
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    creationTimestamp: null
    name: admin
    namespace: ${PROJECT_NAME}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: admin
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: ${PROJECT_ADMIN_USER}
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: ${PROJECT_NAME}-quota
  spec:
    hard:
      cpu: 3
      memory: 10G
- apiVersion: v1
  kind: LimitRange
  metadata:
    name: ${PROJECT_NAME}-limits
  spec:
    limits:
      - type: Container
        defaultRequest:
          cpu: 30m
          memory: 30M
parameters:
- name: PROJECT_NAME
- name: PROJECT_DISPLAYNAME
- name: PROJECT_DESCRIPTION
- name: PROJECT_ADMIN_USER
- name: PROJECT_REQUESTING_USER
[student@workstation ~]$ cp /home/student/DO280/solutions/schedule-limit/project-template.yaml /tmp/project-template.yaml 
[student@workstation ~]$ oc create -f /tmp/project-template.yaml -n openshift-config
template.template.openshift.io/project-request created
[student@workstation ~]$ oc edit projects.config.openshift.io/cluster
project.config.openshift.io/cluster edited
[student@workstation ~]$ oc get pods -n openshift-apiserver
NAME                         READY   STATUS     RESTARTS   AGE
apiserver-59b6b7968c-6hvj5   1/1     Running    0          2d18h
apiserver-59b6b7968c-gsxqg   1/1     Running    0          2d18h
apiserver-75cfdcf877-r9hwj   0/1     Init:0/1   0          9s
[student@workstation ~]$ oc get pods -n openshift-apiserver
NAME                         READY   STATUS        RESTARTS   AGE
apiserver-59b6b7968c-6hvj5   1/1     Running       0          2d18h
apiserver-59b6b7968c-gsxqg   1/1     Terminating   0          2d18h
apiserver-75cfdcf877-7n2cq   0/1     Pending       0          0s
apiserver-75cfdcf877-r9hwj   1/1     Running       0          16s
[student@workstation ~]$ oc get pods -n openshift-apiserver
NAME                         READY   STATUS        RESTARTS   AGE
apiserver-59b6b7968c-6hvj5   1/1     Running       0          2d18h
apiserver-59b6b7968c-gsxqg   0/1     Terminating   0          2d18h
apiserver-75cfdcf877-7n2cq   0/1     Pending       0          6s
apiserver-75cfdcf877-r9hwj   1/1     Running       0          22s
[student@workstation ~]$ oc get pods -n openshift-apiserver
NAME                         READY   STATUS    RESTARTS   AGE
apiserver-75cfdcf877-4v4z2   0/1     Running   0          10s
apiserver-75cfdcf877-7n2cq   1/1     Running   0          24s
apiserver-75cfdcf877-r9hwj   1/1     Running   0          40s
[student@workstation ~]$ oc new-project template-test
Now using project "template-test" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ oc get resourcequotas,limitranges
NAME                                AGE   REQUEST                   LIMIT
resourcequota/template-test-quota   10s   cpu: 0/3, memory: 0/10G   

NAME                              CREATED AT
limitrange/template-test-limits   2021-08-10T02:41:27Z
[student@workstation ~]$ oc delete project schedule-limit
project.project.openshift.io "schedule-limit" deleted
[student@workstation ~]$ oc delete project template-test
project.project.openshift.io "template-test" deleted
[student@workstation ~]$ lab schedule-limit finish

Completing Guided Exercise: Limiting Resource Usage by an Application

 · Removing project template 'template.template.openshift.io/pr
   oject-request'..............................................  SUCCESS
 · Reverting the cluster to use the default project template...  SUCCESS
 · Removing /tmp/project-template.yaml.........................  SUCCESS
 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

----------------------------------------------------------------------------------------------------------------------
Chapter 6 | Controlling Pod Scheduling
Guided Exercise
Scaling an Application Page: 215
------------------------------------------------------------------------------------------------------------------------
In this exercise, you will scale an application manually and automatically.
Outcomes
You should be able to use the OpenShift command-line interface to:
• Manually scale an application.
• Configure an application to automatically scale based on usage.


[student@workstation ~]$ lab schedule-scale start

Checking prerequisites for Guided Exercise: Scaling an Application

 Verify the OpenShift cluster is running:
 · Router pods are available...................................  SUCCESS
 · OAuth pods are available....................................  SUCCESS
 · API pods are available......................................  SUCCESS
 · Control plane node 'master01' is ready......................  SUCCESS
 · Control plane node 'master02' is ready......................  SUCCESS
 · Control plane node 'master03' is ready......................  SUCCESS

 Checking for conflicts with existing OpenShift projects:
 · The 'schedule-scale' project is absent......................  SUCCESS

Setting up the classroom for Guided Exercise: Scaling an Application

 · Validate 'admin' can log in with password 'redhat'..........  SUCCESS
 · Validate 'leader' can log in with password 'redhat'.........  SUCCESS
 · Validate 'developer' can log in with password 'developer'...  SUCCESS

 Preparing the student's cluster:
 · Download exercise files.....................................  SUCCESS
 · Download solution files.....................................  SUCCESS

Overall start status...........................................  SUCCESS

[student@workstation ~]$ oc login -u developer -p developer https://api.ocp4.example.com:6443
Login successful.

You have one project on this server: "authorization-review"

Using project "authorization-review".
[student@workstation ~]$ oc new-project schedule-scale
Now using project "schedule-scale" on server "https://api.ocp4.example.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[student@workstation ~]$ ls
Desktop  DO280  Documents  Downloads  loadtest.yaml  Music  Pictures  Public  Templates  venv  Videos
[student@workstation ~]$ cp loadtest.yaml ~/DO280/labs/schedule-scale/loadtest.yaml
[student@workstation ~]$ oc create --save-config -f ~/DO280/labs/schedule-scale/loadtest.yaml
deployment.apps/loadtest created
service/loadtest created
route.route.openshift.io/loadtest created
[student@workstation ~]$ cat !$
cat ~/DO280/labs/schedule-scale/loadtest.yaml
apiVersion: v1
items:
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    creationTimestamp: null
    labels:
      app: loadtest
    name: loadtest
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: loadtest
    strategy: {}
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: loadtest
      spec:
        containers:
        - image: quay.io/redhattraining/loadtest:v1.0
          name: loadtest
          resources:
            requests:
              cpu: "25m"
              memory: 25Mi
            limits:
              cpu: "100m"
              memory: 100Mi
  status: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: null
    labels:
      app: loadtest
    name: loadtest
  spec:
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: loadtest
  status:
    loadBalancer: {}
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    annotations:
      haproxy.router.openshift.io/timeout: 60s
    creationTimestamp: null
    labels:
      app: loadtest
    name: loadtest
  spec:
    host: ""
    port:
      targetPort: 8080
    subdomain: ""
    to:
      kind: ""
      name: loadtest
      weight: null
  status:
    ingress: null
kind: List
metadata: {}
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
loadtest-7c88f9884b-5rgh8   1/1     Running   0          24s
[student@workstation ~]$ oc describe pod/loadtest-7c88f9884b-5rgh8| grep -A2 -E "Limits|Requests"
    Limits:
      cpu:     100m
      memory:  100Mi
    Requests:
      cpu:        25m
      memory:     25Mi
[student@workstation ~]$ oc scale --replicas 5 deployment/loadtest
deployment.apps/loadtest scaled
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
loadtest-7c88f9884b-5mkgp   1/1     Running   0          10s
loadtest-7c88f9884b-5rgh8   1/1     Running   0          76s
loadtest-7c88f9884b-cwjnp   1/1     Running   0          10s
loadtest-7c88f9884b-jkzfm   1/1     Running   0          10s
loadtest-7c88f9884b-s2t77   1/1     Running   0          10s
[student@workstation ~]$ oc scale --replicas 1 deployment/loadtest
deployment.apps/loadtest scaled
[student@workstation ~]$ oc get pods
NAME                        READY   STATUS        RESTARTS   AGE
loadtest-7c88f9884b-5mkgp   1/1     Running       0          30s
loadtest-7c88f9884b-5rgh8   1/1     Terminating   0          96s
loadtest-7c88f9884b-cwjnp   1/1     Terminating   0          30s
loadtest-7c88f9884b-jkzfm   1/1     Terminating   0          30s
loadtest-7c88f9884b-s2t77   1/1     Terminating   0          30s
[student@workstation ~]$ oc autoscale deployment/loadtest --min 2 --max 10 --cpu-percent 50
horizontalpodautoscaler.autoscaling/loadtest autoscaled
[student@workstation ~]$ oc get hpa/loadtest
NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
loadtest   Deployment/loadtest   <unknown>/50%   2         10        1          16s
[student@workstation ~]$ oc get hpa/loadtest
NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
loadtest   Deployment/loadtest   <unknown>/50%   2         10        1          18s
[student@workstation ~]$ oc get route/loadtes
Error from server (NotFound): routes.route.openshift.io "loadtes" not found
[student@workstation ~]$ oc get route/loadtest
NAME       HOST/PORT                                       PATH   SERVICES   PORT   TERMINATION   WILDCARD
loadtest   loadtest-schedule-scale.apps.ocp4.example.com          loadtest   8080                 None
[student@workstation ~]$ curl -X GET http://loadtest-schedule-scale.apps.ocp4.example.com/api/loadtest/v1/cpu/1
curl: (52) Empty reply from server
[student@workstation ~]$ oc get hpa/loadtest
NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
loadtest   Deployment/loadtest   <unknown>/50%   2         10        2          3m12s
[student@workstation ~]$ oc new-app --name scaling --as-deployment-config --docker-image quay.io/redhattraining/scaling:v1.0
--> Found container image 4e17b8d (21 months old) from quay.io for "quay.io/redhattraining/scaling:v1.0"

    Apache 2.4 with PHP 7.2 
    ----------------------- 
    PHP 7.2 available as container is a base platform for building and running various PHP 7.2 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php72, rh-php72

    * An image stream tag will be created as "scaling:v1.0" that will track this image
    * This image will be deployed in deployment config "scaling"
    * Ports 8080/tcp, 8443/tcp will be load balanced by service "scaling"
      * Other containers can access this service through the hostname "scaling"

--> Creating resources ...
    imagestream.image.openshift.io "scaling" created
    deploymentconfig.apps.openshift.io "scaling" created
    service "scaling" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/scaling' 
    Run 'oc status' to view your app.
[student@workstation ~]$ oc expose svc/scaling
route.route.openshift.io/scaling exposed
[student@workstation ~]$ oc scale --replicas 3 dc/scaling
deploymentconfig.apps.openshift.io/scaling scaled
[student@workstation ~]$ oc get pods -o wide -l deploymentconfig=scaling
NAME              READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
scaling-1-lccbb   1/1     Running   0          17s   10.9.0.25    master03   <none>           <none>
scaling-1-m6pgb   1/1     Running   0          51s   10.10.0.40   master01   <none>           <none>
scaling-1-rwctf   1/1     Running   0          17s   10.8.0.38    master02   <none>           <none>
[student@workstation ~]$ oc get route/scaling
NAME      HOST/PORT                                      PATH   SERVICES   PORT       TERMINATION   WILDCARD
scaling   scaling-schedule-scale.apps.ocp4.example.com          scaling    8080-tcp                 None
[student@workstation ~]$ ~/DO280/labs/schedule-scale/curl-route.sh
     34 Server IP: 10.10.0.40 
     34 Server IP: 10.8.0.38 
     32 Server IP: 10.9.0.25 
[student@workstation ~]$ ~/DO280/labs/schedule-scale/curl-route.sh
     34 Server IP: 10.10.0.40 
     32 Server IP: 10.8.0.38 
     34 Server IP: 10.9.0.25 
[student@workstation ~]$ oc get hpa/loadtest
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
loadtest   Deployment/loadtest   0%/50%    2         10        8          6m51s
[student@workstation ~]$ ~/DO280/labs/schedule-scale/curl-route.sh
     32 Server IP: 10.10.0.40 
     34 Server IP: 10.8.0.38 
     34 Server IP: 10.9.0.25 
[student@workstation ~]$ oc get hpa/loadtest
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
loadtest   Deployment/loadtest   0%/50%    2         10        8          7m2s
[student@workstation ~]$ cat ~/DO280/labs/schedule-scale/curl-route.sh
#!/usr/bin/bash

if $(oc get route/scaling -n schedule-scale -o jsonpath='{.spec.host}' &> /dev/null); then
  ROUTE=$(oc get route/scaling -n schedule-scale -o jsonpath='{.spec.host}')
  (for x in {1..100}; do curl -s http://${ROUTE}; done) | sort | uniq -c
else
  echo
  echo "Error: Ensure that the 'scaling' route exists in the 'schedule-scale'"
  echo "       namespace and then try running this script again."
  echo
fi
[student@workstation ~]$ oc delete project schedule-scale
project.project.openshift.io "schedule-scale" deleted
[student@workstation ~]$ lab schedule-scale finish

Completing Guided Exercise: Scaling an Application

 · Remove exercise files.......................................  SUCCESS
 · Remove solution files.......................................  SUCCESS

Please use start if you wish to do the exercise again.

[student@workstation ~]$ 

